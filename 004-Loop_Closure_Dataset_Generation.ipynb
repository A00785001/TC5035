{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A00785001/TC5035/blob/main/004-Loop_Closure_Dataset_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_title"
      },
      "source": [
        "# Loop Closure Dataset Generation: Time Alignment, Pairing & Labeling\n",
        "## Phase 1.5: Supervised Learning Dataset Preparation for Jetson Nano Training Pipeline\n",
        "\n",
        "**Pipeline Phase:** Feature Extraction â†’ **[THIS NOTEBOOK]** â†’ Fusion MLP Training â†’ Deployment  \n",
        "**Target Hardware:** Waveshare Jetbot AI Pro Kit (Jetson Nano)  \n",
        "**SLAM System:** Google Cartographer (2D)  \n",
        "**Training Platform:** Vertex AI\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "documentation_section"
      },
      "source": [
        "## ğŸ“‹ NOTEBOOK DOCUMENTATION\n",
        "\n",
        "### Purpose\n",
        "\n",
        "This notebook transforms independently extracted multi-modal features (camera + LiDAR) into a supervised learning dataset for training a loop closure detection classifier. It bridges the gap between feature extraction and neural network training by performing temporal alignment, intelligent pairing, and ground truth labeling.\n",
        "\n",
        "---\n",
        "\n",
        "### Required Inputs\n",
        "\n",
        "#### 1. Extracted Features (HDF5 Format)\n",
        "\n",
        "**File:** `features.h5` (generated from feature extraction pipeline)\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "features.h5\n",
        "â”œâ”€â”€ camera/\n",
        "â”‚   â”œâ”€â”€ features [N_cam, 1280]    # MobileNetV2 embeddings (L2 normalized)\n",
        "â”‚   â”œâ”€â”€ timestamps [N_cam]        # ROS timestamps (float64, seconds)\n",
        "â”‚   â””â”€â”€ filenames [N_cam]         # Source image filenames (strings)\n",
        "â””â”€â”€ lidar/\n",
        "    â”œâ”€â”€ features [N_lid, 256]     # 1D CNN descriptors (L2 normalized)\n",
        "    â”œâ”€â”€ timestamps [N_lid]        # ROS timestamps (float64, seconds)\n",
        "    â””â”€â”€ filenames [N_lid]         # Source scan filenames (strings)\n",
        "```\n",
        "\n",
        "**Properties:**\n",
        "- All features are L2 normalized (||f|| = 1.0)\n",
        "- Timestamps are in ROS time format (seconds since epoch)\n",
        "- Features were extracted independently without temporal alignment\n",
        "- Typical size: 100-300 frames per modality per session\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. ROS Bag File (ROS1 Format)\n",
        "\n",
        "**File:** `session_data.bag` (from data collection session)\n",
        "\n",
        "**Critical Topics Required:**\n",
        "\n",
        "| Topic | Type | Rate | Purpose |\n",
        "|-------|------|------|----------|\n",
        "| `/trajectory_node_list` | visualization_msgs/MarkerArray | ~0.9 Hz | SLAM trajectory nodes with poses |\n",
        "| `/constraint_list` | visualization_msgs/MarkerArray | ~0.36 Hz | SLAM constraints (loop closures) |\n",
        "| `/scan` | sensor_msgs/LaserScan | ~0.7 Hz | LiDAR validation (optional) |\n",
        "| `/csi_cam_0/image_raw/compressed` | sensor_msgs/CompressedImage | ~0.22 Hz | Camera validation (optional) |\n",
        "\n",
        "**Ground Truth Source:**  \n",
        "Cartographer SLAM publishes loop closure detections as `INTER_SUBMAP` constraints in `/constraint_list`. These represent confirmed spatial correspondences that the robot has revisited a previous location and serve as ground truth positive labels.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Session Metadata\n",
        "\n",
        "**Required Information:**\n",
        "- Session ID (e.g., `20251016_133216`)\n",
        "- Session duration and environment (for validation)\n",
        "- Map dimensions (for spatial thresholds)\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "#### **Phase 1: Time Alignment & Synchronization**\n",
        "\n",
        "**Challenge:** Multi-rate asynchronous sensors produce features and trajectory nodes at different rates:\n",
        "- Camera: ~0.22 Hz (every 4.5 seconds)\n",
        "- LiDAR: ~0.7 Hz (every 1.4 seconds)  \n",
        "- Trajectory nodes: ~0.9 Hz (every 1.1 seconds)\n",
        "\n",
        "**Solution:** Bidirectional nearest neighbor matching with temporal tolerance\n",
        "\n",
        "**Steps:**\n",
        "1. Parse ROS bag to extract all trajectory nodes (node_id, timestamp, pose)\n",
        "2. Load camera and LiDAR features with their timestamps\n",
        "3. Align features to nodes using KD-tree nearest neighbor search (max offset: 0.5s)\n",
        "4. Create unified database mapping node_id â†’ {camera_feature, lidar_feature, pose}\n",
        "5. Filter to retain only nodes with both modalities present\n",
        "\n",
        "**Output:** Temporally aligned multi-modal feature database\n",
        "\n",
        "---\n",
        "\n",
        "#### **Phase 2: Intelligent Pairing Strategy**\n",
        "\n",
        "**Challenge:** Create balanced training pairs that help the model learn robust loop closure detection\n",
        "\n",
        "**Solution:** Three-tier pairing strategy based on spatial and perceptual characteristics\n",
        "\n",
        "**Pair Types:**\n",
        "\n",
        "1. **Positive Pairs (Target: 30%)**\n",
        "   - Source: Cartographer INTER_SUBMAP constraints\n",
        "   - Criteria: Spatial distance < 2.0m, angular distance < Ï€/2\n",
        "   - Label: 1 (loop closure)\n",
        "   - Purpose: Learn what true loop closures look like\n",
        "\n",
        "2. **Easy Negative Pairs (Target: 35%)**\n",
        "   - Source: Random sampling from trajectory\n",
        "   - Criteria: Spatial distance > 5.0m, temporal distance > 5.0s\n",
        "   - Label: 0 (not loop closure)\n",
        "   - Purpose: Learn basic spatial discrimination\n",
        "\n",
        "3. **Hard Negative Pairs (Target: 35%)**\n",
        "   - **Type A - Perceptual Aliasing:** High feature similarity (cosine > 0.7) but spatially distant (> 3.0m)\n",
        "   - **Type B - Different Viewpoints:** Same location (< 2.0m) but different orientation (> Ï€/2) and temporally distinct (> 5.0s)\n",
        "   - Label: 0 (not loop closure)\n",
        "   - Purpose: Learn robustness against confusing situations (similar-looking places, revisits from different angles)\n",
        "\n",
        "**Output:** Balanced set of (query_node_id, candidate_node_id, label) triplets\n",
        "\n",
        "---\n",
        "\n",
        "#### **Phase 3: Pairwise Feature Computation & Labeling**\n",
        "\n",
        "**Challenge:** Convert pair information into trainable feature vectors\n",
        "\n",
        "**Solution:** Absolute difference encoding with temporal splitting\n",
        "\n",
        "**Steps:**\n",
        "1. For each pair (query, candidate):\n",
        "   - Retrieve concatenated features: f_query = [1280D_cam + 256D_lidar] = 1536D\n",
        "   - Retrieve concatenated features: f_candidate = 1536D\n",
        "   - Compute pairwise feature: abs(f_query - f_candidate) = 1536D\n",
        "2. Create train/validation/test splits using temporal ordering:\n",
        "   - Train: First 60% of trajectory\n",
        "   - Validation: Middle 20% of trajectory  \n",
        "   - Test: Last 20% of trajectory\n",
        "3. Package into final dataset with metadata\n",
        "\n",
        "**Why absolute difference?**\n",
        "- Symmetric: |f_a - f_b| = |f_b - f_a| (order doesn't matter)\n",
        "- Range: [0, âˆš2] for L2-normalized features\n",
        "- Small values â†’ similar features â†’ likely loop closure\n",
        "- Large values â†’ different features â†’ likely not loop closure\n",
        "\n",
        "**Output:** Final training-ready dataset\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Outputs\n",
        "\n",
        "#### Primary Output\n",
        "\n",
        "**File:** `loop_closure_dataset.pkl` (Python pickle format)\n",
        "\n",
        "**Structure:**\n",
        "```python\n",
        "{\n",
        "  'metadata': {\n",
        "    'session_id': str,\n",
        "    'creation_date': str,\n",
        "    'num_valid_nodes': int,\n",
        "    'num_total_pairs': int,\n",
        "    'positive_ratio': float,\n",
        "    'feature_dim': int  # 1536\n",
        "  },\n",
        "  'train': {\n",
        "    'features': np.array [N_train, 1536],  # Pairwise features\n",
        "    'labels': np.array [N_train],          # Binary labels\n",
        "    'pair_info': list of dicts             # (query_id, candidate_id, distance, etc.)\n",
        "  },\n",
        "  'val': { ... },      # Same structure as train\n",
        "  'test': { ... },     # Same structure as train\n",
        "  'node_database': dict  # Full node info for analysis\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### Diagnostic Outputs\n",
        "\n",
        "**Files Generated:**\n",
        "- `alignment_report.txt` - Phase 1 statistics (alignment rates, time offsets)\n",
        "- `pairing_report.txt` - Phase 2 statistics (pair counts, spatial distributions)\n",
        "- `validation_report.txt` - Quality assurance checks\n",
        "- `dataset_diagnostics.png` - Visualization plots:\n",
        "  - Spatial distribution of pairs on map\n",
        "  - Feature similarity distributions\n",
        "  - Label balance per split\n",
        "  - Temporal coverage analysis\n",
        "\n",
        "---\n",
        "\n",
        "### Success Criteria\n",
        "\n",
        "The dataset is ready for Phase 2 (MLP Training) when:\n",
        "\n",
        "âœ… **Alignment Rate:** >80% of features successfully aligned to trajectory nodes  \n",
        "âœ… **Valid Nodes:** >50 nodes with both camera and LiDAR features  \n",
        "âœ… **Positive Pairs:** At least 20 INTER_SUBMAP constraints found  \n",
        "âœ… **Label Balance:** 25-35% positive pairs (target: 30%)  \n",
        "âœ… **Hard Negatives:** At least 30% of total pairs are hard negatives  \n",
        "âœ… **No Data Leakage:** Train/val/test splits are temporally ordered  \n",
        "âœ… **Feature Quality:** Mean pairwise feature in range [0.2, 0.5], std > 0.1\n",
        "\n",
        "---\n",
        "\n",
        "### Pipeline Context\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Phase 0: Data Collection                                   â”‚\n",
        "â”‚  â€¢ Robot navigation with SLAM                               â”‚\n",
        "â”‚  â€¢ Output: session_data.bag                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Phase 1: Feature Extraction                                â”‚\n",
        "â”‚  â€¢ MobileNetV2 (camera) + 1D CNN (LiDAR)                    â”‚\n",
        "â”‚  â€¢ Output: features.h5                                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Phase 1.5: Dataset Generation [THIS NOTEBOOK]              â”‚\n",
        "â”‚  â€¢ Time alignment                                           â”‚\n",
        "â”‚  â€¢ Intelligent pairing                                      â”‚\n",
        "â”‚  â€¢ Ground truth labeling                                    â”‚\n",
        "â”‚  â€¢ Output: loop_closure_dataset.pkl                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Phase 2: Fusion MLP Training                               â”‚\n",
        "â”‚  â€¢ Architecture: 1536â†’512â†’128â†’1                             â”‚\n",
        "â”‚  â€¢ Loss: BCE + Hard Negative Mining                         â”‚\n",
        "â”‚  â€¢ Output: fusion_mlp.pth                                   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Phase 3: Export & Deployment                               â”‚\n",
        "â”‚  â€¢ Convert to ONNX â†’ TensorRT FP16                          â”‚\n",
        "â”‚  â€¢ Deploy to Jetson Nano                                    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "**Time Alignment:**\n",
        "- `max_time_offset`: 0.5 seconds (allows one frame lag for asynchronous sensors)\n",
        "\n",
        "**Spatial Thresholds:**\n",
        "- Positive pairs: distance < 2.0m, angle < 90Â°\n",
        "- Easy negatives: distance > 5.0m\n",
        "- Hard negatives (Type A): distance > 3.0m, cosine similarity > 0.7\n",
        "- Hard negatives (Type B): distance < 2.0m, angle > 90Â°\n",
        "\n",
        "**Dataset Composition:**\n",
        "- Positive: 30% (1:2 ratio per pipeline design)\n",
        "- Easy negative: 35%\n",
        "- Hard negative: 35%\n",
        "\n",
        "**Train/Val/Test Split:**\n",
        "- 60% / 20% / 20% based on temporal trajectory progression\n",
        "\n",
        "---\n",
        "\n",
        "### Notes & Considerations\n",
        "\n",
        "âš ï¸ **Cartographer Message Format:** The `/trajectory_node_list` and `/constraint_list` topics use `visualization_msgs/MarkerArray` format (for RViz visualization) rather than native SLAM messages. This notebook includes specialized parsing logic to extract structured data from marker namespaces and IDs.\n",
        "\n",
        "âš ï¸ **Feature Normalization:** All input features must be L2 normalized. The pairwise absolute difference will have range [0, âˆš2].\n",
        "\n",
        "âš ï¸ **Temporal Ordering:** Train/val/test splits respect temporal ordering to prevent data leakage. The model should generalize to future trajectories, not memorize past ones.\n",
        "\n",
        "âš ï¸ **Hard Negative Importance:** Hard negatives are critical for robustness. Without them, the model may fail on perceptually similar places or revisits from different angles.\n",
        "\n",
        "âš ï¸ **Session Requirements:** For robust training, combine multiple sessions (5-10) to reach 5,000-10,000 total pairs. Single session provides proof-of-concept but may overfit.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_1_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 1. Initialization and Setup</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet rosbags opencv-python pillow numpy matplotlib tqdm seaborn pandas h5py scipy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import h5py\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import io\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Import scientific computing\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.spatial.distance import cosine, euclidean\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import ROS bag handling\n",
        "from rosbags.rosbag1 import Reader\n",
        "from rosbags.typesys import Stores, get_typestore\n",
        "\n",
        "print(\"âœ… All libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_typestore"
      },
      "outputs": [],
      "source": [
        "# Initialize typestore for ROS1 message deserialization\n",
        "typestore = get_typestore(Stores.ROS1_NOETIC)\n",
        "print(\"âœ… Typestore initialized for ROS1 Noetic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_2_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 2. Storage Mounting</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_3_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 3. ROS Bag and Feature File Loading</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "specify_paths"
      },
      "outputs": [],
      "source": [
        "# Specify the path to the sessions folder\n",
        "data_path = \"/content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_session"
      },
      "outputs": [],
      "source": [
        "# Specify the session\n",
        "session_id = \"20251016_133216\"\n",
        "print(f\"Using session: {session_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "construct_paths"
      },
      "outputs": [],
      "source": [
        "# Construct paths\n",
        "session_path = Path(data_path) / session_id\n",
        "bag_path = session_path / \"session_data.bag\"\n",
        "features_path = session_path / \"features.h5\"  # Assumes features.h5 is in same directory\n",
        "\n",
        "# Verify files exist\n",
        "if not bag_path.exists():\n",
        "    raise FileNotFoundError(f\"ROS bag not found: {bag_path}\")\n",
        "if not features_path.exists():\n",
        "    raise FileNotFoundError(f\"Features file not found: {features_path}\")\n",
        "\n",
        "print(f\"âœ… ROS Bag: {bag_path}\")\n",
        "print(f\"âœ… Features: {features_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_bag_info"
      },
      "outputs": [],
      "source": [
        "# Get basic bag information\n",
        "with Reader(bag_path) as reader:\n",
        "    bag_name = bag_path.name\n",
        "    file_size_mb = bag_path.stat().st_size / (1024 * 1024)\n",
        "    duration_sec = (reader.end_time - reader.start_time) * 1e-9\n",
        "    total_messages = reader.message_count\n",
        "    unique_topics = {conn.topic for conn in reader.connections}\n",
        "    avg_message_rate = total_messages / duration_sec if duration_sec > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ROS BAG INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"File: {bag_name}\")\n",
        "print(f\"Size: {file_size_mb:.2f} MB\")\n",
        "print(f\"Duration: {duration_sec:.2f} seconds ({duration_sec/60:.2f} minutes)\")\n",
        "print(f\"Total Messages: {total_messages:,}\")\n",
        "print(f\"Unique Topics: {len(unique_topics)}\")\n",
        "print(f\"Average Message Rate: {avg_message_rate:.2f} msg/sec\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_4_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 4. ROS Bag Basic EDA</font>\n",
        "\n",
        "Quick exploratory data analysis to understand the bag contents before proceeding with dataset generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_4_1"
      },
      "source": [
        "### 4.1 Topic Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "topic_overview"
      },
      "outputs": [],
      "source": [
        "# Extract topic information\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOPIC ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    topic_data = []\n",
        "    for idx, conn in enumerate(reader.connections):\n",
        "        topic_data.append({\n",
        "            'Connection_Index': idx,\n",
        "            'Topic': conn.topic,\n",
        "            'Message Type': conn.msgtype,\n",
        "            'Message Count': conn.msgcount,\n",
        "            'Frequency (Hz)': conn.msgcount / duration_sec if duration_sec > 0 else 0\n",
        "        })\n",
        "\n",
        "    df_topics = pd.DataFrame(topic_data)\n",
        "    df_topics = df_topics.sort_values('Message Count', ascending=False)\n",
        "\n",
        "    print(\"\\nğŸ“Š Topics in ROS Bag:\")\n",
        "    print(df_topics.to_string(index=False))\n",
        "\n",
        "    # Save to CSV\n",
        "    df_topics.to_csv('topic_overview.csv', index=False)\n",
        "    print(\"\\nâœ… Topic overview saved to: topic_overview.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_4_2"
      },
      "source": [
        "### 4.2 Message Rate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "message_rate_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze message timing and rates\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MESSAGE RATE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    # Collect timestamps per connection\n",
        "    connection_timestamps = defaultdict(list)\n",
        "\n",
        "    print(\"\\nCollecting message timestamps...\")\n",
        "    for connection, timestamp, rawdata in tqdm(reader.messages(), total=reader.message_count):\n",
        "        # Find connection index\n",
        "        conn_idx = None\n",
        "        for idx, conn in enumerate(reader.connections):\n",
        "            if (conn.topic == connection.topic and\n",
        "                conn.msgtype == connection.msgtype and\n",
        "                conn.msgcount == connection.msgcount):\n",
        "                conn_idx = idx\n",
        "                break\n",
        "\n",
        "        if conn_idx is not None:\n",
        "            connection_timestamps[conn_idx].append(timestamp * 1e-9)  # Convert to seconds\n",
        "\n",
        "    # Calculate statistics\n",
        "    print(f\"\\n{'Topic':<40} {'Description':<30} {'Mean Î”t (s)':<12} {'Std Î”t (s)':<12}\")\n",
        "    print(\"-\" * 94)\n",
        "\n",
        "    for conn_idx, timestamps in sorted(connection_timestamps.items()):\n",
        "        if len(timestamps) > 1:\n",
        "            intervals = np.diff(timestamps)\n",
        "\n",
        "            # Get topic info\n",
        "            topic_row = df_topics[df_topics['Connection_Index'] == conn_idx].iloc[0]\n",
        "            topic_name = topic_row['Topic']\n",
        "\n",
        "            print(f\"{topic_name:<40} {np.mean(intervals):<12.4f} {np.std(intervals):<12.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_5_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 5. Cartographer Data Validation</font>\n",
        "\n",
        "**Purpose:** Validate that Cartographer SLAM data is consistent, complete, and suitable for generating ground truth labels.\n",
        "\n",
        "**What we validate:**\n",
        "1. `/trajectory_node_list` topic exists and contains valid nodes\n",
        "2. `/constraint_list` topic exists and contains INTER_SUBMAP constraints (loop closures)\n",
        "3. Node IDs are consistent and can be matched between topics\n",
        "4. Spatial data (poses) is reasonable and covers the map\n",
        "5. Temporal data is consistent with sensor timestamps\n",
        "6. Sufficient INTER_SUBMAP constraints exist for positive pairs\n",
        "\n",
        "**Critical for Success:** Without valid Cartographer data, we cannot generate ground truth labels for loop closure detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_5_1"
      },
      "source": [
        "### 5.1 Check Required Topics Existence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_required_topics"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"CARTOGRAPHER DATA VALIDATION - PHASE 1: TOPIC CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "required_topics = {\n",
        "    '/trajectory_node_list': 'visualization_msgs/MarkerArray',\n",
        "    '/constraint_list': 'visualization_msgs/MarkerArray'\n",
        "}\n",
        "\n",
        "found_topics = {}\n",
        "validation_passed = True\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    for topic_name, expected_type in required_topics.items():\n",
        "        found = False\n",
        "        for conn in reader.connections:\n",
        "            if conn.topic == topic_name:\n",
        "                found = True\n",
        "                found_topics[topic_name] = {\n",
        "                    'type': conn.msgtype,\n",
        "                    'count': conn.msgcount,\n",
        "                    'matches_expected': conn.msgtype == expected_type\n",
        "                }\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            print(f\"âŒ FAILED: Topic '{topic_name}' not found in bag file\")\n",
        "            validation_passed = False\n",
        "        else:\n",
        "            info = found_topics[topic_name]\n",
        "            if info['matches_expected']:\n",
        "                print(f\"âœ… PASS: Topic '{topic_name}' found\")\n",
        "                print(f\"   - Type: {info['type']}\")\n",
        "                print(f\"   - Messages: {info['count']}\")\n",
        "            else:\n",
        "                print(f\"âš ï¸  WARNING: Topic '{topic_name}' found but type mismatch\")\n",
        "                print(f\"   - Expected: {expected_type}\")\n",
        "                print(f\"   - Found: {info['type']}\")\n",
        "                validation_passed = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if validation_passed:\n",
        "    print(\"âœ… Phase 1 PASSED: All required topics found\")\n",
        "else:\n",
        "    print(\"âŒ Phase 1 FAILED: Missing or incorrect topics\")\n",
        "    raise ValueError(\"Cannot proceed without required Cartographer topics\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_5_2"
      },
      "source": [
        "### 5.2 Parse and Validate Trajectory Nodes\n",
        "\n",
        "**Cartographer Message Format:** The `/trajectory_node_list` topic uses `visualization_msgs/MarkerArray` for RViz visualization. Each trajectory node is represented as a SPHERE marker.\n",
        "\n",
        "**Extraction Strategy:**\n",
        "- Marker namespace: \"Trajectory <trajectory_id>\" (e.g., \"Trajectory 0\")\n",
        "- Marker ID: Encodes the node ID\n",
        "- Marker pose: Contains (x, y, z) position and orientation quaternion\n",
        "- We need to parse these markers to extract structured trajectory data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parse_trajectory_nodes"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CARTOGRAPHER DATA VALIDATION - PHASE 2: TRAJECTORY NODES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trajectory_nodes = []\n",
        "trajectory_node_timestamps = []  # Store message receipt times\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    print(\"\\nParsing trajectory node messages...\")\n",
        "\n",
        "    for connection, timestamp, rawdata in tqdm(reader.messages()):\n",
        "        if connection.topic == '/trajectory_node_list':\n",
        "            try:\n",
        "                # Deserialize MarkerArray message\n",
        "                msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "\n",
        "                # Extract markers\n",
        "                for marker in msg.markers:\n",
        "                    # Trajectory nodes have namespace like \"Trajectory 0\"\n",
        "                    if marker.ns.startswith(\"Trajectory\"):\n",
        "                        # Extract trajectory ID from namespace\n",
        "                        traj_id = int(marker.ns.split()[-1])\n",
        "\n",
        "                        # Node ID is encoded in marker.id\n",
        "                        node_id = marker.id\n",
        "\n",
        "                        # Extract pose\n",
        "                        pose = {\n",
        "                            'x': marker.pose.position.x,\n",
        "                            'y': marker.pose.position.y,\n",
        "                            'z': marker.pose.position.z,\n",
        "                            'qx': marker.pose.orientation.x,\n",
        "                            'qy': marker.pose.orientation.y,\n",
        "                            'qz': marker.pose.orientation.z,\n",
        "                            'qw': marker.pose.orientation.w\n",
        "                        }\n",
        "\n",
        "                        # Store node information\n",
        "                        # Note: We use message timestamp as proxy for node timestamp\n",
        "                        trajectory_nodes.append({\n",
        "                            'node_id': node_id,\n",
        "                            'trajectory_id': traj_id,\n",
        "                            'timestamp': timestamp * 1e-9,  # Convert to seconds\n",
        "                            'pose': pose\n",
        "                        })\n",
        "\n",
        "                        trajectory_node_timestamps.append(timestamp * 1e-9)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Warning: Failed to parse trajectory node message: {e}\")\n",
        "                continue\n",
        "\n",
        "# Remove duplicates (Cartographer may republish the same node multiple times)\n",
        "# Keep the latest timestamp for each unique node_id\n",
        "unique_nodes = {}\n",
        "for node in trajectory_nodes:\n",
        "    node_id = node['node_id']\n",
        "    if node_id not in unique_nodes or node['timestamp'] > unique_nodes[node_id]['timestamp']:\n",
        "        unique_nodes[node_id] = node\n",
        "\n",
        "trajectory_nodes = list(unique_nodes.values())\n",
        "trajectory_nodes.sort(key=lambda n: n['timestamp'])\n",
        "\n",
        "print(f\"\\nâœ… Extracted {len(trajectory_nodes)} unique trajectory nodes\")\n",
        "\n",
        "# Validation checks\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"TRAJECTORY NODE VALIDATION CHECKS\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "# Check 1: Minimum number of nodes\n",
        "min_nodes = 50\n",
        "check_1 = len(trajectory_nodes) >= min_nodes\n",
        "validation_results.append(('Sufficient nodes (>= 50)', check_1))\n",
        "print(f\"{'âœ…' if check_1 else 'âŒ'} Node count: {len(trajectory_nodes)} (minimum: {min_nodes})\")\n",
        "\n",
        "# Check 2: Temporal consistency\n",
        "if len(trajectory_nodes) > 1:\n",
        "    timestamps = [n['timestamp'] for n in trajectory_nodes]\n",
        "    time_range = max(timestamps) - min(timestamps)\n",
        "    check_2 = time_range > 60  # At least 1 minute of trajectory\n",
        "    validation_results.append(('Temporal span (> 60s)', check_2))\n",
        "    print(f\"{'âœ…' if check_2 else 'âŒ'} Time range: {time_range:.2f} seconds\")\n",
        "else:\n",
        "    check_2 = False\n",
        "    validation_results.append(('Temporal span (> 60s)', check_2))\n",
        "    print(f\"âŒ Insufficient nodes for temporal analysis\")\n",
        "\n",
        "# Check 3: Spatial consistency\n",
        "if len(trajectory_nodes) > 1:\n",
        "    x_coords = [n['pose']['x'] for n in trajectory_nodes]\n",
        "    y_coords = [n['pose']['y'] for n in trajectory_nodes]\n",
        "\n",
        "    map_width = max(x_coords) - min(x_coords)\n",
        "    map_height = max(y_coords) - min(y_coords)\n",
        "\n",
        "    check_3 = map_width > 1.0 and map_height > 1.0  # At least 1m x 1m map\n",
        "    validation_results.append(('Spatial extent (> 1m x 1m)', check_3))\n",
        "    print(f\"{'âœ…' if check_3 else 'âŒ'} Map dimensions: {map_width:.2f}m x {map_height:.2f}m\")\n",
        "else:\n",
        "    check_3 = False\n",
        "    validation_results.append(('Spatial extent (> 1m x 1m)', check_3))\n",
        "    print(f\"âŒ Insufficient nodes for spatial analysis\")\n",
        "\n",
        "# Check 4: Node ID consistency\n",
        "node_ids = [n['node_id'] for n in trajectory_nodes]\n",
        "unique_node_ids = len(set(node_ids))\n",
        "check_4 = unique_node_ids == len(trajectory_nodes)\n",
        "validation_results.append(('Unique node IDs', check_4))\n",
        "print(f\"{'âœ…' if check_4 else 'âš ï¸ '} Unique node IDs: {unique_node_ids}/{len(trajectory_nodes)}\")\n",
        "\n",
        "# Overall phase result\n",
        "phase_2_passed = all(result[1] for result in validation_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if phase_2_passed:\n",
        "    print(\"âœ… Phase 2 PASSED: Trajectory nodes are valid\")\n",
        "else:\n",
        "    print(\"âŒ Phase 2 FAILED: Trajectory node validation failed\")\n",
        "    print(\"âš ï¸  Warning: Proceeding may result in poor dataset quality\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_5_3"
      },
      "source": [
        "### 5.3 Parse and Validate Constraints (Loop Closures)\n",
        "\n",
        "**Cartographer Message Format:** The `/constraint_list` topic uses `visualization_msgs/MarkerArray` with LINE_STRIP markers connecting pairs of nodes.\n",
        "\n",
        "**Constraint Types:**\n",
        "- **INTRA_SUBMAP:** Constraints within the same local submap (odometry-based)\n",
        "- **INTER_SUBMAP:** Constraints between different submaps (loop closures) â† **THIS IS WHAT WE NEED**\n",
        "\n",
        "**Extraction Strategy:**\n",
        "- Namespace identifies constraint type: \"Inter constraints\" or \"Intra constraints\"\n",
        "- LINE_STRIP markers have exactly 2 points (start and end node)\n",
        "- Marker ID can be used to link to node pairs\n",
        "- We extract start/end poses and compute spatial relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parse_constraints"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CARTOGRAPHER DATA VALIDATION - PHASE 3: CONSTRAINTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_constraints = []\n",
        "inter_submap_constraints = []\n",
        "intra_submap_constraints = []\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    print(\"\\nParsing constraint messages...\")\n",
        "\n",
        "    for connection, timestamp, rawdata in tqdm(reader.messages()):\n",
        "        if connection.topic == '/constraint_list':\n",
        "            try:\n",
        "                # Deserialize MarkerArray message\n",
        "                msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "\n",
        "                # Extract markers\n",
        "                for marker in msg.markers:\n",
        "                    # Constraints are LINE_STRIP markers\n",
        "                    if marker.type == 4:  # LINE_STRIP = 4\n",
        "                        # Determine constraint type from namespace\n",
        "                        is_inter_submap = \"Inter\" in marker.ns or \"inter\" in marker.ns\n",
        "                        is_intra_submap = \"Intra\" in marker.ns or \"intra\" in marker.ns\n",
        "\n",
        "                        if not (is_inter_submap or is_intra_submap):\n",
        "                            continue\n",
        "\n",
        "                        # LINE_STRIP should have exactly 2 points (start and end node)\n",
        "                        if len(marker.points) == 2:\n",
        "                            start_point = marker.points[0]\n",
        "                            end_point = marker.points[1]\n",
        "\n",
        "                            # Compute spatial distance\n",
        "                            dx = end_point.x - start_point.x\n",
        "                            dy = end_point.y - start_point.y\n",
        "                            dz = end_point.z - start_point.z\n",
        "                            spatial_distance = np.sqrt(dx**2 + dy**2 + dz**2)\n",
        "\n",
        "                            constraint = {\n",
        "                                'constraint_id': marker.id,\n",
        "                                'type': 'INTER_SUBMAP' if is_inter_submap else 'INTRA_SUBMAP',\n",
        "                                'start_pose': {\n",
        "                                    'x': start_point.x,\n",
        "                                    'y': start_point.y,\n",
        "                                    'z': start_point.z\n",
        "                                },\n",
        "                                'end_pose': {\n",
        "                                    'x': end_point.x,\n",
        "                                    'y': end_point.y,\n",
        "                                    'z': end_point.z\n",
        "                                },\n",
        "                                'spatial_distance': spatial_distance,\n",
        "                                'timestamp': timestamp * 1e-9\n",
        "                            }\n",
        "\n",
        "                            all_constraints.append(constraint)\n",
        "\n",
        "                            if is_inter_submap:\n",
        "                                inter_submap_constraints.append(constraint)\n",
        "                            else:\n",
        "                                intra_submap_constraints.append(constraint)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Warning: Failed to parse constraint message: {e}\")\n",
        "                continue\n",
        "\n",
        "print(f\"\\nâœ… Extracted {len(all_constraints)} total constraints\")\n",
        "print(f\"   - INTER_SUBMAP (loop closures): {len(inter_submap_constraints)}\")\n",
        "print(f\"   - INTRA_SUBMAP (odometry): {len(intra_submap_constraints)}\")\n",
        "\n",
        "# Validation checks\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"CONSTRAINT VALIDATION CHECKS\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "# Check 1: Minimum INTER_SUBMAP constraints\n",
        "min_inter_constraints = 10\n",
        "check_1 = len(inter_submap_constraints) >= min_inter_constraints\n",
        "validation_results.append(('Sufficient INTER_SUBMAP (>= 10)', check_1))\n",
        "print(f\"{'âœ…' if check_1 else 'âŒ'} INTER_SUBMAP count: {len(inter_submap_constraints)} (minimum: {min_inter_constraints})\")\n",
        "\n",
        "if not check_1:\n",
        "    print(\"   âš ï¸  WARNING: Insufficient loop closures for robust training\")\n",
        "    print(\"   â†’ This session may not have enough revisited locations\")\n",
        "    print(\"   â†’ Consider: (1) longer data collection, (2) more loops, (3) combining multiple sessions\")\n",
        "\n",
        "# Check 2: Spatial distribution of INTER_SUBMAP constraints\n",
        "if len(inter_submap_constraints) > 0:\n",
        "    inter_distances = [c['spatial_distance'] for c in inter_submap_constraints]\n",
        "    mean_distance = np.mean(inter_distances)\n",
        "\n",
        "    # Loop closures should be spatially close (< 5m typically)\n",
        "    check_2 = mean_distance < 5.0\n",
        "    validation_results.append(('Reasonable spatial distances (< 5m)', check_2))\n",
        "    print(f\"{'âœ…' if check_2 else 'âš ï¸ '} Mean INTER_SUBMAP distance: {mean_distance:.2f}m\")\n",
        "    print(f\"   Range: [{min(inter_distances):.2f}, {max(inter_distances):.2f}]m\")\n",
        "else:\n",
        "    check_2 = False\n",
        "    validation_results.append(('Reasonable spatial distances (< 5m)', check_2))\n",
        "\n",
        "# Check 3: Ratio of INTER to INTRA constraints\n",
        "if len(intra_submap_constraints) > 0:\n",
        "    inter_intra_ratio = len(inter_submap_constraints) / len(intra_submap_constraints)\n",
        "\n",
        "    # Typical ratio: 5-20% INTER constraints\n",
        "    check_3 = 0.05 <= inter_intra_ratio <= 0.30\n",
        "    validation_results.append(('Reasonable INTER/INTRA ratio (5-30%)', check_3))\n",
        "    print(f\"{'âœ…' if check_3 else 'âš ï¸ '} INTER/INTRA ratio: {inter_intra_ratio:.2%}\")\n",
        "else:\n",
        "    check_3 = True  # No INTRA constraints is unusual but not fatal\n",
        "    validation_results.append(('Reasonable INTER/INTRA ratio (5-30%)', check_3))\n",
        "    print(f\"âš ï¸  No INTRA constraints found (unusual but not critical)\")\n",
        "\n",
        "# Overall phase result\n",
        "phase_3_passed = validation_results[0][1]  # Only first check is critical\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if phase_3_passed:\n",
        "    print(\"âœ… Phase 3 PASSED: Constraints are valid\")\n",
        "else:\n",
        "    print(\"âŒ Phase 3 FAILED: Insufficient INTER_SUBMAP constraints\")\n",
        "    print(\"âš ï¸  Cannot proceed with dataset generation\")\n",
        "    raise ValueError(\"Insufficient loop closure constraints for training\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_5_4"
      },
      "source": [
        "### 5.4 Cross-Validation: Match Constraints to Nodes\n",
        "\n",
        "**Critical Check:** Verify that constraint poses correspond to actual trajectory nodes. This ensures we can later match constraints to feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "match_constraints_to_nodes"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CARTOGRAPHER DATA VALIDATION - PHASE 4: CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Build spatial index for trajectory nodes\n",
        "node_positions = np.array([[n['pose']['x'], n['pose']['y'], n['pose']['z']] for n in trajectory_nodes])\n",
        "node_ids = [n['node_id'] for n in trajectory_nodes]\n",
        "node_tree = KDTree(node_positions)\n",
        "\n",
        "print(\"\\nMatching constraints to trajectory nodes...\")\n",
        "\n",
        "matched_constraints = []\n",
        "unmatched_constraints = []\n",
        "match_threshold = 0.5  # 50cm tolerance for spatial matching\n",
        "\n",
        "for constraint in tqdm(inter_submap_constraints):\n",
        "    # Find nearest node to start pose\n",
        "    start_pos = np.array([constraint['start_pose']['x'],\n",
        "                          constraint['start_pose']['y'],\n",
        "                          constraint['start_pose']['z']])\n",
        "\n",
        "    start_dist, start_idx = node_tree.query(start_pos)\n",
        "\n",
        "    # Find nearest node to end pose\n",
        "    end_pos = np.array([constraint['end_pose']['x'],\n",
        "                        constraint['end_pose']['y'],\n",
        "                        constraint['end_pose']['z']])\n",
        "\n",
        "    end_dist, end_idx = node_tree.query(end_pos)\n",
        "\n",
        "    # Check if both poses match nodes within threshold\n",
        "    if start_dist < match_threshold and end_dist < match_threshold:\n",
        "        # Successfully matched!\n",
        "        matched_constraint = constraint.copy()\n",
        "        matched_constraint['node_id_i'] = node_ids[start_idx]\n",
        "        matched_constraint['node_id_j'] = node_ids[end_idx]\n",
        "        matched_constraint['match_error_i'] = start_dist\n",
        "        matched_constraint['match_error_j'] = end_dist\n",
        "\n",
        "        matched_constraints.append(matched_constraint)\n",
        "    else:\n",
        "        unmatched_constraints.append(constraint)\n",
        "\n",
        "match_rate = len(matched_constraints) / len(inter_submap_constraints) if len(inter_submap_constraints) > 0 else 0\n",
        "\n",
        "print(f\"\\nâœ… Matched {len(matched_constraints)}/{len(inter_submap_constraints)} constraints to nodes ({match_rate:.1%})\")\n",
        "\n",
        "if len(unmatched_constraints) > 0:\n",
        "    print(f\"âš ï¸  {len(unmatched_constraints)} constraints could not be matched (tolerance: {match_threshold}m)\")\n",
        "\n",
        "# Validation check\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"CROSS-VALIDATION CHECKS\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Check: High match rate\n",
        "min_match_rate = 0.70\n",
        "check_match_rate = match_rate >= min_match_rate\n",
        "print(f\"{'âœ…' if check_match_rate else 'âš ï¸ '} Constraint-to-node match rate: {match_rate:.1%} (minimum: {min_match_rate:.0%})\")\n",
        "\n",
        "if not check_match_rate:\n",
        "    print(\"   âš ï¸  Low match rate may indicate:\")\n",
        "    print(\"      - Trajectory nodes are incomplete\")\n",
        "    print(\"      - Constraint poses don't align with node poses\")\n",
        "    print(\"      - Need to adjust match_threshold\")\n",
        "\n",
        "# Check: Sufficient matched constraints for training\n",
        "min_matched_constraints = 10\n",
        "check_sufficient = len(matched_constraints) >= min_matched_constraints\n",
        "print(f\"{'âœ…' if check_sufficient else 'âŒ'} Matched constraint count: {len(matched_constraints)} (minimum: {min_matched_constraints})\")\n",
        "\n",
        "# Overall phase result\n",
        "phase_4_passed = check_sufficient\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if phase_4_passed:\n",
        "    print(\"âœ… Phase 4 PASSED: Constraints successfully matched to nodes\")\n",
        "else:\n",
        "    print(\"âŒ Phase 4 FAILED: Insufficient matched constraints\")\n",
        "    print(\"âš ï¸  Cannot proceed with dataset generation\")\n",
        "    raise ValueError(\"Unable to match constraints to trajectory nodes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store matched constraints for later use\n",
        "inter_submap_constraints = matched_constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_5_5"
      },
      "source": [
        "### 5.5 Cartographer Validation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation_summary"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CARTOGRAPHER DATA VALIDATION - FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "validation_summary = f\"\"\"\n",
        "ğŸ“Š CARTOGRAPHER DATA VALIDATION REPORT\n",
        "{'='*70}\n",
        "\n",
        "SESSION INFORMATION:\n",
        "  â€¢ Session ID: {session_id}\n",
        "  â€¢ Duration: {duration_sec:.2f} seconds ({duration_sec/60:.2f} minutes)\n",
        "  â€¢ Bag size: {file_size_mb:.2f} MB\n",
        "\n",
        "TRAJECTORY NODES:\n",
        "  â€¢ Total unique nodes: {len(trajectory_nodes)}\n",
        "  â€¢ Time range: {trajectory_nodes[-1]['timestamp'] - trajectory_nodes[0]['timestamp']:.2f} seconds\n",
        "  â€¢ Map dimensions: {map_width:.2f}m x {map_height:.2f}m\n",
        "\n",
        "CONSTRAINTS:\n",
        "  â€¢ Total constraints: {len(all_constraints)}\n",
        "  â€¢ INTER_SUBMAP (loop closures): {len(matched_constraints)}\n",
        "  â€¢ INTRA_SUBMAP (odometry): {len(intra_submap_constraints)}\n",
        "  â€¢ Match rate (INTER to nodes): {match_rate:.1%}\n",
        "\n",
        "VALIDATION STATUS:\n",
        "  âœ… Phase 1: Required topics present\n",
        "  {'âœ…' if phase_2_passed else 'âŒ'} Phase 2: Trajectory nodes valid\n",
        "  {'âœ…' if phase_3_passed else 'âŒ'} Phase 3: Constraints valid\n",
        "  {'âœ…' if phase_4_passed else 'âŒ'} Phase 4: Cross-validation passed\n",
        "\n",
        "DATASET GENERATION READINESS:\n",
        "  {'âœ…' if len(matched_constraints) >= 20 else 'âš ï¸ '} Positive pairs available: {len(matched_constraints)} (target: >= 20)\n",
        "  {'âœ…' if len(trajectory_nodes) >= 100 else 'âš ï¸ '} Nodes for negatives: {len(trajectory_nodes)} (target: >= 100)\n",
        "\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "print(validation_summary)\n",
        "\n",
        "# Save validation report\n",
        "with open('cartographer_validation_report.txt', 'w') as f:\n",
        "    f.write(validation_summary)\n",
        "\n",
        "print(\"âœ… Validation report saved to: cartographer_validation_report.txt\")\n",
        "\n",
        "# Overall validation result\n",
        "overall_passed = phase_2_passed and phase_3_passed and phase_4_passed\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if overall_passed:\n",
        "    print(\"ğŸ‰ CARTOGRAPHER DATA VALIDATION: PASSED\")\n",
        "    print(\"   â†’ Ready to proceed with dataset generation\")\n",
        "else:\n",
        "    print(\"âŒ CARTOGRAPHER DATA VALIDATION: FAILED\")\n",
        "    print(\"   â†’ Cannot proceed with dataset generation\")\n",
        "    print(\"   â†’ Review validation report and address issues\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_6_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 6. Load Extracted Features</font>\n",
        "\n",
        "Load the pre-extracted camera and LiDAR features from the HDF5 file. These features were generated independently and need to be aligned to trajectory nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_features"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"LOADING EXTRACTED FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load features from HDF5\n",
        "with h5py.File(features_path, 'r') as f:\n",
        "    # Load camera features\n",
        "    camera_features = f['camera/features'][:]\n",
        "    camera_timestamps = f['camera/timestamps'][:]\n",
        "    camera_filenames = [fn.decode('utf-8') if isinstance(fn, bytes) else fn for fn in f['camera/filenames'][:]]\n",
        "\n",
        "    # Load LiDAR features\n",
        "    lidar_features = f['lidar/features'][:]\n",
        "    lidar_timestamps = f['lidar/timestamps'][:]\n",
        "    lidar_filenames = [fn.decode('utf-8') if isinstance(fn, bytes) else fn for fn in f['lidar/filenames'][:]]\n",
        "\n",
        "print(f\"\\nâœ… Features loaded successfully\")\n",
        "print(f\"\\nCamera features:\")\n",
        "print(f\"  Shape: {camera_features.shape}\")\n",
        "print(f\"  Timestamps: {len(camera_timestamps)}\")\n",
        "print(f\"  Time range: [{camera_timestamps[0]:.2f}, {camera_timestamps[-1]:.2f}]\")\n",
        "\n",
        "print(f\"\\nLiDAR features:\")\n",
        "print(f\"  Shape: {lidar_features.shape}\")\n",
        "print(f\"  Timestamps: {len(lidar_timestamps)}\")\n",
        "print(f\"  Time range: [{lidar_timestamps[0]:.2f}, {lidar_timestamps[-1]:.2f}]\")\n",
        "\n",
        "# Validate feature properties\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"FEATURE VALIDATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Check L2 normalization\n",
        "camera_norms = np.linalg.norm(camera_features, axis=1)\n",
        "lidar_norms = np.linalg.norm(lidar_features, axis=1)\n",
        "\n",
        "camera_normalized = np.allclose(camera_norms, 1.0, atol=1e-5)\n",
        "lidar_normalized = np.allclose(lidar_norms, 1.0, atol=1e-5)\n",
        "\n",
        "print(f\"{'âœ…' if camera_normalized else 'âŒ'} Camera features L2 normalized: {camera_normalized}\")\n",
        "print(f\"   Mean norm: {np.mean(camera_norms):.6f}, Std: {np.std(camera_norms):.6f}\")\n",
        "\n",
        "print(f\"{'âœ…' if lidar_normalized else 'âŒ'} LiDAR features L2 normalized: {lidar_normalized}\")\n",
        "print(f\"   Mean norm: {np.mean(lidar_norms):.6f}, Std: {np.std(lidar_norms):.6f}\")\n",
        "\n",
        "# Check dimensions\n",
        "expected_camera_dim = 1280\n",
        "expected_lidar_dim = 256\n",
        "\n",
        "camera_dim_ok = camera_features.shape[1] == expected_camera_dim\n",
        "lidar_dim_ok = lidar_features.shape[1] == expected_lidar_dim\n",
        "\n",
        "print(f\"{'âœ…' if camera_dim_ok else 'âŒ'} Camera feature dimension: {camera_features.shape[1]} (expected: {expected_camera_dim})\")\n",
        "print(f\"{'âœ…' if lidar_dim_ok else 'âŒ'} LiDAR feature dimension: {lidar_features.shape[1]} (expected: {expected_lidar_dim})\")\n",
        "\n",
        "if not (camera_normalized and lidar_normalized and camera_dim_ok and lidar_dim_ok):\n",
        "    raise ValueError(\"Feature validation failed - check features.h5 file\")\n",
        "\n",
        "print(\"\\nâœ… All feature validations passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_7_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 7. Time Alignment: Features â†” Trajectory Nodes</font>\n",
        "\n",
        "**Challenge:** Features and trajectory nodes have different timestamps due to asynchronous multi-rate sensors.\n",
        "\n",
        "**Solution:** Bidirectional nearest neighbor matching with temporal tolerance (0.5 seconds).\n",
        "\n",
        "**Goal:** Create a unified database where each valid trajectory node has both camera and LiDAR features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_7_1"
      },
      "source": [
        "### 7.1 Align Camera Features to Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "align_camera_features"
      },
      "outputs": [],
      "source": [
        "def align_features_to_nodes(feature_timestamps, trajectory_nodes, max_offset=0.5):\n",
        "    \"\"\"\n",
        "    Align features to trajectory nodes using nearest neighbor matching.\n",
        "\n",
        "    Args:\n",
        "        feature_timestamps: Array of feature timestamps (seconds)\n",
        "        trajectory_nodes: List of trajectory node dictionaries\n",
        "        max_offset: Maximum allowable time difference (seconds)\n",
        "\n",
        "    Returns:\n",
        "        alignment_map: Dict mapping feature_idx -> {'node_id', 'time_offset', 'node_timestamp'}\n",
        "    \"\"\"\n",
        "    # Build KD-tree for fast nearest neighbor search\n",
        "    node_timestamps = np.array([n['timestamp'] for n in trajectory_nodes])\n",
        "    node_ids = [n['node_id'] for n in trajectory_nodes]\n",
        "\n",
        "    tree = KDTree(node_timestamps.reshape(-1, 1))\n",
        "\n",
        "    alignment_map = {}\n",
        "\n",
        "    for feat_idx, feat_time in enumerate(feature_timestamps):\n",
        "        # Find nearest node\n",
        "        dist, idx = tree.query([[feat_time]], k=1)\n",
        "        time_diff = abs(dist[0][0])\n",
        "\n",
        "        if time_diff < max_offset:\n",
        "            # Valid alignment\n",
        "            alignment_map[feat_idx] = {\n",
        "                'node_id': node_ids[idx[0]],\n",
        "                'time_offset': time_diff,\n",
        "                'node_timestamp': node_timestamps[idx[0]]\n",
        "            }\n",
        "\n",
        "    return alignment_map\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TIME ALIGNMENT - CAMERA FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "camera_alignment = align_features_to_nodes(camera_timestamps, trajectory_nodes, max_offset=0.5)\n",
        "\n",
        "camera_alignment_rate = len(camera_alignment) / len(camera_timestamps)\n",
        "time_offsets = [a['time_offset'] for a in camera_alignment.values()]\n",
        "\n",
        "print(f\"\\nâœ… Camera alignment complete\")\n",
        "print(f\"   Aligned: {len(camera_alignment)}/{len(camera_timestamps)} ({camera_alignment_rate:.1%})\")\n",
        "print(f\"   Mean time offset: {np.mean(time_offsets):.3f}s\")\n",
        "print(f\"   Max time offset: {np.max(time_offsets):.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_7_2"
      },
      "source": [
        "### 7.2 Align LiDAR Features to Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "align_lidar_features"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TIME ALIGNMENT - LIDAR FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "lidar_alignment = align_features_to_nodes(lidar_timestamps, trajectory_nodes, max_offset=0.5)\n",
        "\n",
        "lidar_alignment_rate = len(lidar_alignment) / len(lidar_timestamps)\n",
        "time_offsets = [a['time_offset'] for a in lidar_alignment.values()]\n",
        "\n",
        "print(f\"\\nâœ… LiDAR alignment complete\")\n",
        "print(f\"   Aligned: {len(lidar_alignment)}/{len(lidar_timestamps)} ({lidar_alignment_rate:.1%})\")\n",
        "print(f\"   Mean time offset: {np.mean(time_offsets):.3f}s\")\n",
        "print(f\"   Max time offset: {np.max(time_offsets):.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_7_3"
      },
      "source": [
        "### 7.3 Create Unified Node Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_node_database"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CREATING UNIFIED NODE DATABASE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create database indexed by node_id\n",
        "node_database = {}\n",
        "\n",
        "# Initialize with all trajectory nodes\n",
        "for node in trajectory_nodes:\n",
        "    node_database[node['node_id']] = {\n",
        "        'node_id': node['node_id'],\n",
        "        'trajectory_id': node['trajectory_id'],\n",
        "        'timestamp': node['timestamp'],\n",
        "        'pose': node['pose'],\n",
        "        'camera_feature': None,\n",
        "        'lidar_feature': None,\n",
        "        'has_camera': False,\n",
        "        'has_lidar': False,\n",
        "        'has_both': False\n",
        "    }\n",
        "\n",
        "# Add camera features\n",
        "for feat_idx, alignment_info in camera_alignment.items():\n",
        "    node_id = alignment_info['node_id']\n",
        "    if node_id in node_database:\n",
        "        node_database[node_id]['camera_feature'] = camera_features[feat_idx]\n",
        "        node_database[node_id]['has_camera'] = True\n",
        "\n",
        "# Add LiDAR features\n",
        "for feat_idx, alignment_info in lidar_alignment.items():\n",
        "    node_id = alignment_info['node_id']\n",
        "    if node_id in node_database:\n",
        "        node_database[node_id]['lidar_feature'] = lidar_features[feat_idx]\n",
        "        node_database[node_id]['has_lidar'] = True\n",
        "\n",
        "# Mark nodes with both modalities\n",
        "for node_id, node_data in node_database.items():\n",
        "    node_data['has_both'] = node_data['has_camera'] and node_data['has_lidar']\n",
        "\n",
        "    # Concatenate features if both available\n",
        "    if node_data['has_both']:\n",
        "        node_data['combined_feature'] = np.concatenate([\n",
        "            node_data['camera_feature'],\n",
        "            node_data['lidar_feature']\n",
        "        ])\n",
        "\n",
        "# Filter to valid nodes (both modalities)\n",
        "valid_nodes = {k: v for k, v in node_database.items() if v['has_both']}\n",
        "\n",
        "print(f\"\\nâœ… Unified database created\")\n",
        "print(f\"   Total nodes: {len(node_database)}\")\n",
        "print(f\"   Nodes with camera: {sum(1 for n in node_database.values() if n['has_camera'])}\")\n",
        "print(f\"   Nodes with LiDAR: {sum(1 for n in node_database.values() if n['has_lidar'])}\")\n",
        "print(f\"   Nodes with both: {len(valid_nodes)}\")\n",
        "print(f\"   Combined feature dimension: {valid_nodes[list(valid_nodes.keys())[0]]['combined_feature'].shape[0]}D\")\n",
        "\n",
        "# Validation check\n",
        "min_valid_nodes = 50\n",
        "if len(valid_nodes) < min_valid_nodes:\n",
        "    print(f\"\\nâŒ Insufficient valid nodes: {len(valid_nodes)} (minimum: {min_valid_nodes})\")\n",
        "    raise ValueError(\"Not enough nodes with both camera and LiDAR features\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Sufficient valid nodes for dataset generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_8_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 8. Generate Positive Pairs (Loop Closures)</font>\n",
        "\n",
        "**Source:** INTER_SUBMAP constraints from Cartographer (validated in Section 5)\n",
        "\n",
        "**Criteria:**\n",
        "- Both nodes must be in valid_nodes (have both modalities)\n",
        "- Spatial distance < 2.0m\n",
        "- Angular distance < Ï€/2 radians\n",
        "\n",
        "**Label:** 1 (positive loop closure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_positive_pairs"
      },
      "outputs": [],
      "source": [
        "def quaternion_to_yaw(qx, qy, qz, qw):\n",
        "    \"\"\"Convert quaternion to yaw angle (rotation around z-axis).\"\"\"\n",
        "    siny_cosp = 2 * (qw * qz + qx * qy)\n",
        "    cosy_cosp = 1 - 2 * (qy * qy + qz * qz)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "    return yaw\n",
        "\n",
        "def compute_angular_distance(yaw1, yaw2):\n",
        "    \"\"\"Compute angular distance between two yaw angles (0 to Ï€).\"\"\"\n",
        "    diff = abs(yaw1 - yaw2)\n",
        "    # Normalize to [0, Ï€]\n",
        "    if diff > np.pi:\n",
        "        diff = 2 * np.pi - diff\n",
        "    return diff\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GENERATING POSITIVE PAIRS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "positive_pairs = []\n",
        "max_spatial_distance = 2.0  # meters\n",
        "max_angular_distance = np.pi / 2  # 90 degrees\n",
        "\n",
        "print(f\"\\nProcessing {len(inter_submap_constraints)} INTER_SUBMAP constraints...\")\n",
        "\n",
        "for constraint in tqdm(inter_submap_constraints):\n",
        "    node_id_i = constraint['node_id_i']\n",
        "    node_id_j = constraint['node_id_j']\n",
        "\n",
        "    # Check if both nodes are valid\n",
        "    if node_id_i not in valid_nodes or node_id_j not in valid_nodes:\n",
        "        continue\n",
        "\n",
        "    # Get node data\n",
        "    node_i = valid_nodes[node_id_i]\n",
        "    node_j = valid_nodes[node_id_j]\n",
        "\n",
        "    # Compute spatial distance\n",
        "    dx = node_i['pose']['x'] - node_j['pose']['x']\n",
        "    dy = node_i['pose']['y'] - node_j['pose']['y']\n",
        "    spatial_distance = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "    # Compute angular distance\n",
        "    yaw_i = quaternion_to_yaw(node_i['pose']['qx'], node_i['pose']['qy'],\n",
        "                              node_i['pose']['qz'], node_i['pose']['qw'])\n",
        "    yaw_j = quaternion_to_yaw(node_j['pose']['qx'], node_j['pose']['qy'],\n",
        "                              node_j['pose']['qz'], node_j['pose']['qw'])\n",
        "    angular_distance = compute_angular_distance(yaw_i, yaw_j)\n",
        "\n",
        "    # Apply thresholds\n",
        "    if spatial_distance < max_spatial_distance and angular_distance < max_angular_distance:\n",
        "        positive_pairs.append({\n",
        "            'query_node_id': node_id_i,\n",
        "            'candidate_node_id': node_id_j,\n",
        "            'label': 1,\n",
        "            'pair_type': 'positive',\n",
        "            'spatial_distance': spatial_distance,\n",
        "            'angular_distance': angular_distance,\n",
        "            'temporal_distance': abs(node_i['timestamp'] - node_j['timestamp'])\n",
        "        })\n",
        "\n",
        "print(f\"\\nâœ… Generated {len(positive_pairs)} positive pairs\")\n",
        "\n",
        "if len(positive_pairs) > 0:\n",
        "    spatial_dists = [p['spatial_distance'] for p in positive_pairs]\n",
        "    angular_dists = [p['angular_distance'] for p in positive_pairs]\n",
        "    temporal_dists = [p['temporal_distance'] for p in positive_pairs]\n",
        "\n",
        "    print(f\"\\nSpatial distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(spatial_dists):.2f}m\")\n",
        "    print(f\"   Range: [{np.min(spatial_dists):.2f}, {np.max(spatial_dists):.2f}]m\")\n",
        "\n",
        "    print(f\"\\nAngular distance statistics:\")\n",
        "    print(f\"   Mean: {np.rad2deg(np.mean(angular_dists)):.1f}Â°\")\n",
        "    print(f\"   Range: [{np.rad2deg(np.min(angular_dists)):.1f}Â°, {np.rad2deg(np.max(angular_dists)):.1f}Â°]\")\n",
        "\n",
        "    print(f\"\\nTemporal distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(temporal_dists):.2f}s\")\n",
        "    print(f\"   Range: [{np.min(temporal_dists):.2f}, {np.max(temporal_dists):.2f}]s\")\n",
        "else:\n",
        "    print(\"\\nâŒ No positive pairs generated - check thresholds or constraints\")\n",
        "    raise ValueError(\"Cannot proceed without positive pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_9_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 9. Generate Easy Negative Pairs</font>\n",
        "\n",
        "**Strategy:** Random sampling from spatially and temporally distant locations\n",
        "\n",
        "**Criteria:**\n",
        "- Spatial distance > 5.0m (clearly different locations)\n",
        "- Temporal distance > 5.0s (not consecutive frames)\n",
        "\n",
        "**Target:** 35% of total dataset (1.17Ã— positive pairs)\n",
        "\n",
        "**Label:** 0 (not a loop closure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_easy_negatives"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"GENERATING EASY NEGATIVE PAIRS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "min_spatial_distance = 5.0  # meters\n",
        "min_temporal_distance = 5.0  # seconds\n",
        "\n",
        "# Calculate target number (35% of total, so 35/30 * num_positive)\n",
        "num_target = int(len(positive_pairs) * 35 / 30)\n",
        "\n",
        "print(f\"\\nTarget: {num_target} easy negative pairs\")\n",
        "print(f\"Criteria: spatial > {min_spatial_distance}m, temporal > {min_temporal_distance}s\")\n",
        "\n",
        "easy_negative_pairs = []\n",
        "valid_node_ids = list(valid_nodes.keys())\n",
        "max_attempts = num_target * 20  # Prevent infinite loop\n",
        "attempts = 0\n",
        "\n",
        "print(\"\\nSampling pairs...\")\n",
        "with tqdm(total=num_target) as pbar:\n",
        "    while len(easy_negative_pairs) < num_target and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "\n",
        "        # Random sample two different nodes\n",
        "        node_id_i, node_id_j = np.random.choice(valid_node_ids, size=2, replace=False)\n",
        "\n",
        "        node_i = valid_nodes[node_id_i]\n",
        "        node_j = valid_nodes[node_id_j]\n",
        "\n",
        "        # Compute spatial distance\n",
        "        dx = node_i['pose']['x'] - node_j['pose']['x']\n",
        "        dy = node_i['pose']['y'] - node_j['pose']['y']\n",
        "        spatial_distance = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "        # Compute temporal distance\n",
        "        temporal_distance = abs(node_i['timestamp'] - node_j['timestamp'])\n",
        "\n",
        "        # Check criteria\n",
        "        if spatial_distance > min_spatial_distance and temporal_distance > min_temporal_distance:\n",
        "            # Check for duplicates (avoid same pair in reverse order)\n",
        "            pair_key = tuple(sorted([node_id_i, node_id_j]))\n",
        "            existing_keys = [tuple(sorted([p['query_node_id'], p['candidate_node_id']]))\n",
        "                           for p in easy_negative_pairs]\n",
        "\n",
        "            if pair_key not in existing_keys:\n",
        "                easy_negative_pairs.append({\n",
        "                    'query_node_id': node_id_i,\n",
        "                    'candidate_node_id': node_id_j,\n",
        "                    'label': 0,\n",
        "                    'pair_type': 'easy_negative',\n",
        "                    'spatial_distance': spatial_distance,\n",
        "                    'temporal_distance': temporal_distance\n",
        "                })\n",
        "                pbar.update(1)\n",
        "\n",
        "if len(easy_negative_pairs) < num_target:\n",
        "    print(f\"\\nâš ï¸  Warning: Only generated {len(easy_negative_pairs)}/{num_target} easy negatives\")\n",
        "    print(\"   (May need to relax thresholds or collect more data)\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Generated {len(easy_negative_pairs)} easy negative pairs\")\n",
        "\n",
        "if len(easy_negative_pairs) > 0:\n",
        "    spatial_dists = [p['spatial_distance'] for p in easy_negative_pairs]\n",
        "    temporal_dists = [p['temporal_distance'] for p in easy_negative_pairs]\n",
        "\n",
        "    print(f\"\\nSpatial distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(spatial_dists):.2f}m\")\n",
        "    print(f\"   Range: [{np.min(spatial_dists):.2f}, {np.max(spatial_dists):.2f}]m\")\n",
        "\n",
        "    print(f\"\\nTemporal distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(temporal_dists):.2f}s\")\n",
        "    print(f\"   Range: [{np.min(temporal_dists):.2f}, {np.max(temporal_dists):.2f}]s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_10_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 10. Generate Hard Negative Pairs</font>\n",
        "\n",
        "**Critical for robustness:** Hard negatives teach the model to distinguish challenging cases.\n",
        "\n",
        "**Two types:**\n",
        "- **Type A (Perceptual Aliasing):** High feature similarity but spatially distant\n",
        "- **Type B (Different Viewpoints):** Same location but different viewing angle\n",
        "\n",
        "**Target:** 35% of total dataset (1.17Ã— positive pairs)\n",
        "\n",
        "**Label:** 0 (not a loop closure despite similarity/proximity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_10_1"
      },
      "source": [
        "### 10.1 Type A: Perceptual Aliasing (Similar Features, Different Locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_hard_negatives_type_a"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"GENERATING HARD NEGATIVE PAIRS - TYPE A (PERCEPTUAL ALIASING)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "min_similarity = 0.7  # Cosine similarity threshold\n",
        "min_spatial_distance = 3.0  # meters (must be spatially distant)\n",
        "\n",
        "# Calculate target (half of hard negatives budget)\n",
        "num_hard_total = int(len(positive_pairs) * 35 / 30)\n",
        "num_target_type_a = num_hard_total // 2\n",
        "\n",
        "print(f\"\\nTarget: {num_target_type_a} Type A hard negatives\")\n",
        "print(f\"Criteria: cosine similarity > {min_similarity}, spatial > {min_spatial_distance}m\")\n",
        "\n",
        "hard_negative_pairs_type_a = []\n",
        "valid_node_ids = list(valid_nodes.keys())\n",
        "\n",
        "# Compute pairwise cosine similarities (this is expensive, so we'll sample)\n",
        "print(\"\\nComputing feature similarities...\")\n",
        "n_samples = min(len(valid_node_ids), 200)  # Limit computation\n",
        "sampled_node_ids = np.random.choice(valid_node_ids, size=n_samples, replace=False)\n",
        "\n",
        "# Build feature matrix\n",
        "features_matrix = np.array([valid_nodes[nid]['combined_feature'] for nid in sampled_node_ids])\n",
        "\n",
        "# Compute all pairwise cosine similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(features_matrix)\n",
        "\n",
        "print(\"\\nSearching for perceptually similar pairs...\")\n",
        "max_attempts = num_target_type_a * 20\n",
        "attempts = 0\n",
        "\n",
        "with tqdm(total=num_target_type_a) as pbar:\n",
        "    while len(hard_negative_pairs_type_a) < num_target_type_a and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "\n",
        "        # Random sample from high-similarity pairs\n",
        "        i, j = np.random.randint(0, n_samples, size=2)\n",
        "        if i == j:\n",
        "            continue\n",
        "\n",
        "        similarity = similarity_matrix[i, j]\n",
        "\n",
        "        if similarity > min_similarity:\n",
        "            node_id_i = sampled_node_ids[i]\n",
        "            node_id_j = sampled_node_ids[j]\n",
        "\n",
        "            node_i = valid_nodes[node_id_i]\n",
        "            node_j = valid_nodes[node_id_j]\n",
        "\n",
        "            # Compute spatial distance\n",
        "            dx = node_i['pose']['x'] - node_j['pose']['x']\n",
        "            dy = node_i['pose']['y'] - node_j['pose']['y']\n",
        "            spatial_distance = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "            if spatial_distance > min_spatial_distance:\n",
        "                # Check for duplicates\n",
        "                pair_key = tuple(sorted([node_id_i, node_id_j]))\n",
        "                existing_keys = [tuple(sorted([p['query_node_id'], p['candidate_node_id']]))\n",
        "                               for p in hard_negative_pairs_type_a]\n",
        "\n",
        "                if pair_key not in existing_keys:\n",
        "                    hard_negative_pairs_type_a.append({\n",
        "                        'query_node_id': node_id_i,\n",
        "                        'candidate_node_id': node_id_j,\n",
        "                        'label': 0,\n",
        "                        'pair_type': 'hard_negative_type_a',\n",
        "                        'spatial_distance': spatial_distance,\n",
        "                        'feature_similarity': similarity\n",
        "                    })\n",
        "                    pbar.update(1)\n",
        "\n",
        "print(f\"\\nâœ… Generated {len(hard_negative_pairs_type_a)} Type A hard negatives\")\n",
        "\n",
        "if len(hard_negative_pairs_type_a) > 0:\n",
        "    spatial_dists = [p['spatial_distance'] for p in hard_negative_pairs_type_a]\n",
        "    similarities = [p['feature_similarity'] for p in hard_negative_pairs_type_a]\n",
        "\n",
        "    print(f\"\\nSpatial distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(spatial_dists):.2f}m\")\n",
        "    print(f\"   Range: [{np.min(spatial_dists):.2f}, {np.max(spatial_dists):.2f}]m\")\n",
        "\n",
        "    print(f\"\\nFeature similarity statistics:\")\n",
        "    print(f\"   Mean: {np.mean(similarities):.3f}\")\n",
        "    print(f\"   Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_10_2"
      },
      "source": [
        "### 10.2 Type B: Same Location, Different Viewpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_hard_negatives_type_b"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"GENERATING HARD NEGATIVE PAIRS - TYPE B (DIFFERENT VIEWPOINTS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "max_spatial_distance = 2.0  # meters (same location)\n",
        "min_angular_distance = np.pi / 2  # 90 degrees (different viewpoint)\n",
        "min_temporal_distance = 5.0  # seconds (temporally distinct)\n",
        "\n",
        "# Calculate target (remaining half of hard negatives budget)\n",
        "num_target_type_b = num_hard_total - len(hard_negative_pairs_type_a)\n",
        "\n",
        "print(f\"\\nTarget: {num_target_type_b} Type B hard negatives\")\n",
        "print(f\"Criteria: spatial < {max_spatial_distance}m, angular > {np.rad2deg(min_angular_distance):.0f}Â°, temporal > {min_temporal_distance}s\")\n",
        "\n",
        "hard_negative_pairs_type_b = []\n",
        "max_attempts = num_target_type_b * 20\n",
        "attempts = 0\n",
        "\n",
        "print(\"\\nSearching for same-location different-viewpoint pairs...\")\n",
        "with tqdm(total=num_target_type_b) as pbar:\n",
        "    while len(hard_negative_pairs_type_b) < num_target_type_b and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "\n",
        "        # Random sample two different nodes\n",
        "        node_id_i, node_id_j = np.random.choice(valid_node_ids, size=2, replace=False)\n",
        "\n",
        "        node_i = valid_nodes[node_id_i]\n",
        "        node_j = valid_nodes[node_id_j]\n",
        "\n",
        "        # Compute spatial distance\n",
        "        dx = node_i['pose']['x'] - node_j['pose']['x']\n",
        "        dy = node_i['pose']['y'] - node_j['pose']['y']\n",
        "        spatial_distance = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "        # Compute angular distance\n",
        "        yaw_i = quaternion_to_yaw(node_i['pose']['qx'], node_i['pose']['qy'],\n",
        "                                  node_i['pose']['qz'], node_i['pose']['qw'])\n",
        "        yaw_j = quaternion_to_yaw(node_j['pose']['qx'], node_j['pose']['qy'],\n",
        "                                  node_j['pose']['qz'], node_j['pose']['qw'])\n",
        "        angular_distance = compute_angular_distance(yaw_i, yaw_j)\n",
        "\n",
        "        # Compute temporal distance\n",
        "        temporal_distance = abs(node_i['timestamp'] - node_j['timestamp'])\n",
        "\n",
        "        # Check criteria\n",
        "        if (spatial_distance < max_spatial_distance and\n",
        "            angular_distance > min_angular_distance and\n",
        "            temporal_distance > min_temporal_distance):\n",
        "\n",
        "            # Check for duplicates\n",
        "            pair_key = tuple(sorted([node_id_i, node_id_j]))\n",
        "            existing_keys = [tuple(sorted([p['query_node_id'], p['candidate_node_id']]))\n",
        "                           for p in hard_negative_pairs_type_b]\n",
        "\n",
        "            if pair_key not in existing_keys:\n",
        "                hard_negative_pairs_type_b.append({\n",
        "                    'query_node_id': node_id_i,\n",
        "                    'candidate_node_id': node_id_j,\n",
        "                    'label': 0,\n",
        "                    'pair_type': 'hard_negative_type_b',\n",
        "                    'spatial_distance': spatial_distance,\n",
        "                    'angular_distance': angular_distance,\n",
        "                    'temporal_distance': temporal_distance\n",
        "                })\n",
        "                pbar.update(1)\n",
        "\n",
        "print(f\"\\nâœ… Generated {len(hard_negative_pairs_type_b)} Type B hard negatives\")\n",
        "\n",
        "if len(hard_negative_pairs_type_b) > 0:\n",
        "    spatial_dists = [p['spatial_distance'] for p in hard_negative_pairs_type_b]\n",
        "    angular_dists = [p['angular_distance'] for p in hard_negative_pairs_type_b]\n",
        "    temporal_dists = [p['temporal_distance'] for p in hard_negative_pairs_type_b]\n",
        "\n",
        "    print(f\"\\nSpatial distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(spatial_dists):.2f}m\")\n",
        "    print(f\"   Range: [{np.min(spatial_dists):.2f}, {np.max(spatial_dists):.2f}]m\")\n",
        "\n",
        "    print(f\"\\nAngular distance statistics:\")\n",
        "    print(f\"   Mean: {np.rad2deg(np.mean(angular_dists)):.1f}Â°\")\n",
        "    print(f\"   Range: [{np.rad2deg(np.min(angular_dists)):.1f}Â°, {np.rad2deg(np.max(angular_dists)):.1f}Â°]\")\n",
        "\n",
        "    print(f\"\\nTemporal distance statistics:\")\n",
        "    print(f\"   Mean: {np.mean(temporal_dists):.2f}s\")\n",
        "    print(f\"   Range: [{np.min(temporal_dists):.2f}, {np.max(temporal_dists):.2f}]s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsection_10_3"
      },
      "source": [
        "### 10.3 Combine All Hard Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "combine_hard_negatives"
      },
      "outputs": [],
      "source": [
        "# Combine both types of hard negatives\n",
        "hard_negative_pairs = hard_negative_pairs_type_a + hard_negative_pairs_type_b\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HARD NEGATIVE PAIRS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal hard negatives: {len(hard_negative_pairs)}\")\n",
        "print(f\"   Type A (perceptual aliasing): {len(hard_negative_pairs_type_a)}\")\n",
        "print(f\"   Type B (different viewpoint): {len(hard_negative_pairs_type_b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_11_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 11. Compute Pairwise Features & Create Dataset</font>\n",
        "\n",
        "**Pairwise Encoding:** For each pair (query, candidate), compute:\n",
        "```\n",
        "pairwise_feature = abs(f_query - f_candidate)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `f_query` = concatenated [camera_1280D + lidar_256D] = 1536D\n",
        "- `f_candidate` = concatenated [camera_1280D + lidar_256D] = 1536D\n",
        "- Result: 1536D difference vector\n",
        "\n",
        "**Properties:**\n",
        "- Symmetric: |a - b| = |b - a|\n",
        "- Range: [0, âˆš2] for L2-normalized features\n",
        "- Small values â†’ similar â†’ likely loop closure\n",
        "- Large values â†’ different â†’ likely not loop closure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "combine_all_pairs"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"COMBINING ALL PAIRS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Combine all pairs\n",
        "all_pairs = positive_pairs + easy_negative_pairs + hard_negative_pairs\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Composition:\")\n",
        "print(f\"   Total pairs: {len(all_pairs)}\")\n",
        "print(f\"   Positive: {len(positive_pairs)} ({100*len(positive_pairs)/len(all_pairs):.1f}%)\")\n",
        "print(f\"   Easy negative: {len(easy_negative_pairs)} ({100*len(easy_negative_pairs)/len(all_pairs):.1f}%)\")\n",
        "print(f\"   Hard negative: {len(hard_negative_pairs)} ({100*len(hard_negative_pairs)/len(all_pairs):.1f}%)\")\n",
        "print(f\"      â€¢ Type A: {len(hard_negative_pairs_type_a)} ({100*len(hard_negative_pairs_type_a)/len(all_pairs):.1f}%)\")\n",
        "print(f\"      â€¢ Type B: {len(hard_negative_pairs_type_b)} ({100*len(hard_negative_pairs_type_b)/len(all_pairs):.1f}%)\")\n",
        "\n",
        "# Validate label balance\n",
        "positive_ratio = len(positive_pairs) / len(all_pairs)\n",
        "target_positive_ratio = 0.30\n",
        "\n",
        "if 0.25 <= positive_ratio <= 0.35:\n",
        "    print(f\"\\nâœ… Label balance good: {positive_ratio:.1%} positive (target: {target_positive_ratio:.0%})\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  Label balance suboptimal: {positive_ratio:.1%} positive (target: {target_positive_ratio:.0%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compute_pairwise_features"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPUTING PAIRWISE FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "print(f\"\\nProcessing {len(all_pairs)} pairs...\")\n",
        "for pair in tqdm(all_pairs):\n",
        "    query_node_id = pair['query_node_id']\n",
        "    candidate_node_id = pair['candidate_node_id']\n",
        "\n",
        "    # Get combined features (1536D each)\n",
        "    f_query = valid_nodes[query_node_id]['combined_feature']\n",
        "    f_candidate = valid_nodes[candidate_node_id]['combined_feature']\n",
        "\n",
        "    # Compute pairwise feature: absolute difference\n",
        "    pairwise_feature = np.abs(f_query - f_candidate)\n",
        "\n",
        "    # Create dataset entry\n",
        "    dataset.append({\n",
        "        'pairwise_feature': pairwise_feature,\n",
        "        'label': pair['label'],\n",
        "        'query_node_id': query_node_id,\n",
        "        'candidate_node_id': candidate_node_id,\n",
        "        'pair_type': pair['pair_type'],\n",
        "        'query_timestamp': valid_nodes[query_node_id]['timestamp'],\n",
        "        'candidate_timestamp': valid_nodes[candidate_node_id]['timestamp'],\n",
        "        'spatial_distance': pair.get('spatial_distance', None),\n",
        "        'metadata': pair  # Keep all original pair info\n",
        "    })\n",
        "\n",
        "print(f\"\\nâœ… Pairwise features computed for {len(dataset)} pairs\")\n",
        "\n",
        "# Convert to arrays for analysis\n",
        "X = np.array([d['pairwise_feature'] for d in dataset])\n",
        "y = np.array([d['label'] for d in dataset])\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Arrays:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   Positive samples: {np.sum(y == 1)} ({100*np.mean(y):.1f}%)\")\n",
        "print(f\"   Negative samples: {np.sum(y == 0)} ({100*(1-np.mean(y)):.1f}%)\")\n",
        "\n",
        "# Feature statistics\n",
        "print(f\"\\nğŸ“ˆ Pairwise Feature Statistics:\")\n",
        "print(f\"   Mean: {np.mean(X):.4f}\")\n",
        "print(f\"   Std: {np.std(X):.4f}\")\n",
        "print(f\"   Min: {np.min(X):.4f}\")\n",
        "print(f\"   Max: {np.max(X):.4f}\")\n",
        "print(f\"   Theoretical max: {np.sqrt(2):.4f} (âˆš2 for L2-normalized features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_12_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 12. Train/Validation/Test Split</font>\n",
        "\n",
        "**Strategy:** Temporal splitting to prevent data leakage\n",
        "\n",
        "**Rationale:** The model should generalize to future trajectory segments, not memorize past ones. Splitting by time ensures test data represents unseen temporal regions.\n",
        "\n",
        "**Split:**\n",
        "- Train: First 60% of trajectory\n",
        "- Validation: Middle 20% of trajectory\n",
        "- Test: Last 20% of trajectory\n",
        "\n",
        "**Assignment:** Each pair is assigned to a split based on the LATER timestamp (max of query and candidate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temporal_split"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRAIN/VALIDATION/TEST SPLIT (TEMPORAL)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get all timestamps from valid nodes\n",
        "all_timestamps = sorted([node['timestamp'] for node in valid_nodes.values()])\n",
        "min_time = all_timestamps[0]\n",
        "max_time = all_timestamps[-1]\n",
        "time_range = max_time - min_time\n",
        "\n",
        "# Define split boundaries\n",
        "train_end = min_time + 0.60 * time_range\n",
        "val_end = min_time + 0.80 * time_range\n",
        "\n",
        "print(f\"\\nTime range: [{min_time:.2f}, {max_time:.2f}] seconds\")\n",
        "print(f\"Duration: {time_range:.2f} seconds\")\n",
        "print(f\"\\nSplit boundaries:\")\n",
        "print(f\"   Train: [{min_time:.2f}, {train_end:.2f}]\")\n",
        "print(f\"   Val:   [{train_end:.2f}, {val_end:.2f}]\")\n",
        "print(f\"   Test:  [{val_end:.2f}, {max_time:.2f}]\")\n",
        "\n",
        "# Assign each pair to a split based on the LATER timestamp\n",
        "train_dataset = []\n",
        "val_dataset = []\n",
        "test_dataset = []\n",
        "\n",
        "print(\"\\nAssigning pairs to splits...\")\n",
        "for data_point in tqdm(dataset):\n",
        "    # Use the maximum timestamp (later observation)\n",
        "    pair_time = max(data_point['query_timestamp'], data_point['candidate_timestamp'])\n",
        "\n",
        "    if pair_time < train_end:\n",
        "        train_dataset.append(data_point)\n",
        "    elif pair_time < val_end:\n",
        "        val_dataset.append(data_point)\n",
        "    else:\n",
        "        test_dataset.append(data_point)\n",
        "\n",
        "print(f\"\\nâœ… Split complete\")\n",
        "print(f\"\\nğŸ“Š Split Statistics:\")\n",
        "print(f\"   Train: {len(train_dataset)} pairs ({100*len(train_dataset)/len(dataset):.1f}%)\")\n",
        "print(f\"   Val:   {len(val_dataset)} pairs ({100*len(val_dataset)/len(dataset):.1f}%)\")\n",
        "print(f\"   Test:  {len(test_dataset)} pairs ({100*len(test_dataset)/len(dataset):.1f}%)\")\n",
        "\n",
        "# Check label balance in each split\n",
        "def get_label_stats(split_data, split_name):\n",
        "    labels = [d['label'] for d in split_data]\n",
        "    positive_count = sum(labels)\n",
        "    negative_count = len(labels) - positive_count\n",
        "    positive_ratio = positive_count / len(labels) if len(labels) > 0 else 0\n",
        "\n",
        "    print(f\"\\n   {split_name}:\")\n",
        "    print(f\"      Positive: {positive_count} ({100*positive_ratio:.1f}%)\")\n",
        "    print(f\"      Negative: {negative_count} ({100*(1-positive_ratio):.1f}%)\")\n",
        "\n",
        "    return positive_ratio\n",
        "\n",
        "print(\"\\nğŸ“Š Label Balance per Split:\")\n",
        "train_pos_ratio = get_label_stats(train_dataset, \"Train\")\n",
        "val_pos_ratio = get_label_stats(val_dataset, \"Val\")\n",
        "test_pos_ratio = get_label_stats(test_dataset, \"Test\")\n",
        "\n",
        "# Validate minimum samples per split\n",
        "min_samples = 10\n",
        "if len(train_dataset) < min_samples or len(val_dataset) < min_samples or len(test_dataset) < min_samples:\n",
        "    print(f\"\\nâš ï¸  Warning: Some splits have fewer than {min_samples} samples\")\n",
        "    print(\"   Consider collecting more data or adjusting split ratios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_13_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 13. Dataset Quality Validation & Visualization</font>\n",
        "\n",
        "Final validation checks and diagnostic visualizations before saving the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_validation"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATASET QUALITY VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "validation_checks = []\n",
        "\n",
        "# Check 1: Sufficient total samples\n",
        "min_total_samples = 50\n",
        "check_1 = len(dataset) >= min_total_samples\n",
        "validation_checks.append(('Sufficient samples (>= 50)', check_1))\n",
        "print(f\"\\n{'âœ…' if check_1 else 'âŒ'} Total samples: {len(dataset)} (minimum: {min_total_samples})\")\n",
        "\n",
        "# Check 2: Label balance\n",
        "positive_ratio = np.mean(y)\n",
        "check_2 = 0.20 <= positive_ratio <= 0.40\n",
        "validation_checks.append(('Label balance (20-40% positive)', check_2))\n",
        "print(f\"{'âœ…' if check_2 else 'âš ï¸ '} Positive ratio: {positive_ratio:.1%} (target: 25-35%)\")\n",
        "\n",
        "# Check 3: Feature quality\n",
        "mean_feature = np.mean(X)\n",
        "std_feature = np.std(X)\n",
        "check_3 = 0.1 < mean_feature < 1.0 and std_feature > 0.05\n",
        "validation_checks.append(('Feature statistics reasonable', check_3))\n",
        "print(f\"{'âœ…' if check_3 else 'âš ï¸ '} Mean: {mean_feature:.3f}, Std: {std_feature:.3f}\")\n",
        "\n",
        "# Check 4: No NaN or Inf values\n",
        "check_4 = not (np.any(np.isnan(X)) or np.any(np.isinf(X)))\n",
        "validation_checks.append(('No NaN/Inf values', check_4))\n",
        "print(f\"{'âœ…' if check_4 else 'âŒ'} Data integrity: {'OK' if check_4 else 'FAILED'}\")\n",
        "\n",
        "# Check 5: All splits have samples\n",
        "check_5 = len(train_dataset) > 0 and len(val_dataset) > 0 and len(test_dataset) > 0\n",
        "validation_checks.append(('All splits non-empty', check_5))\n",
        "print(f\"{'âœ…' if check_5 else 'âŒ'} Splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "# Check 6: Hard negatives present\n",
        "hard_neg_ratio = len(hard_negative_pairs) / len(all_pairs)\n",
        "check_6 = hard_neg_ratio >= 0.20\n",
        "validation_checks.append(('Sufficient hard negatives (>= 20%)', check_6))\n",
        "print(f\"{'âœ…' if check_6 else 'âš ï¸ '} Hard negative ratio: {hard_neg_ratio:.1%}\")\n",
        "\n",
        "# Overall validation\n",
        "all_passed = all(check[1] for check in validation_checks)\n",
        "critical_passed = validation_checks[0][1] and validation_checks[3][1] and validation_checks[4][1]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if all_passed:\n",
        "    print(\"âœ… ALL VALIDATION CHECKS PASSED\")\n",
        "    print(\"   Dataset is ready for training!\")\n",
        "elif critical_passed:\n",
        "    print(\"âš ï¸  VALIDATION PASSED WITH WARNINGS\")\n",
        "    print(\"   Dataset is usable but may benefit from improvements\")\n",
        "else:\n",
        "    print(\"âŒ VALIDATION FAILED\")\n",
        "    print(\"   Critical issues detected - review before training\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_dataset"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATING DIAGNOSTIC VISUALIZATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle(f'Loop Closure Dataset Diagnostics - Session {session_id}', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Spatial distribution of pairs\n",
        "ax = axes[0, 0]\n",
        "for pair_type, color, label in [\n",
        "    ('positive', 'green', 'Positive'),\n",
        "    ('easy_negative', 'blue', 'Easy Negative'),\n",
        "    ('hard_negative_type_a', 'orange', 'Hard Neg (Type A)'),\n",
        "    ('hard_negative_type_b', 'red', 'Hard Neg (Type B)')\n",
        "]:\n",
        "    pairs_of_type = [d for d in dataset if d['pair_type'] == pair_type]\n",
        "    if pairs_of_type:\n",
        "        distances = [d['spatial_distance'] for d in pairs_of_type if d['spatial_distance'] is not None]\n",
        "        if distances:\n",
        "            ax.hist(distances, bins=20, alpha=0.6, color=color, label=label)\n",
        "\n",
        "ax.set_xlabel('Spatial Distance (m)')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Spatial Distance Distribution by Pair Type')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Feature value distribution\n",
        "ax = axes[0, 1]\n",
        "X_pos = X[y == 1]\n",
        "X_neg = X[y == 0]\n",
        "ax.hist(X_pos.flatten(), bins=50, alpha=0.6, color='green', label='Positive', density=True)\n",
        "ax.hist(X_neg.flatten(), bins=50, alpha=0.6, color='red', label='Negative', density=True)\n",
        "ax.set_xlabel('Pairwise Feature Value')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Pairwise Feature Distribution')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Label balance\n",
        "ax = axes[0, 2]\n",
        "splits = ['Train', 'Val', 'Test', 'Overall']\n",
        "pos_counts = [\n",
        "    sum(d['label'] for d in train_dataset),\n",
        "    sum(d['label'] for d in val_dataset),\n",
        "    sum(d['label'] for d in test_dataset),\n",
        "    int(np.sum(y))\n",
        "]\n",
        "neg_counts = [\n",
        "    len(train_dataset) - pos_counts[0],\n",
        "    len(val_dataset) - pos_counts[1],\n",
        "    len(test_dataset) - pos_counts[2],\n",
        "    len(dataset) - pos_counts[3]\n",
        "]\n",
        "\n",
        "x_pos = np.arange(len(splits))\n",
        "width = 0.35\n",
        "ax.bar(x_pos - width/2, pos_counts, width, label='Positive', color='green', alpha=0.7)\n",
        "ax.bar(x_pos + width/2, neg_counts, width, label='Negative', color='red', alpha=0.7)\n",
        "ax.set_xlabel('Split')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Label Balance per Split')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(splits)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Map visualization (2D trajectory)\n",
        "ax = axes[1, 0]\n",
        "# Plot all nodes\n",
        "node_positions = np.array([[n['pose']['x'], n['pose']['y']] for n in valid_nodes.values()])\n",
        "ax.scatter(node_positions[:, 0], node_positions[:, 1], c='gray', s=10, alpha=0.5, label='Nodes')\n",
        "\n",
        "# Plot positive pairs\n",
        "for pair in positive_pairs[:50]:  # Limit to 50 for clarity\n",
        "    node_i = valid_nodes[pair['query_node_id']]\n",
        "    node_j = valid_nodes[pair['candidate_node_id']]\n",
        "    ax.plot([node_i['pose']['x'], node_j['pose']['x']],\n",
        "           [node_i['pose']['y'], node_j['pose']['y']],\n",
        "           'g-', alpha=0.3, linewidth=1)\n",
        "\n",
        "ax.set_xlabel('X (m)')\n",
        "ax.set_ylabel('Y (m)')\n",
        "ax.set_title('Map with Loop Closures (Green Lines)')\n",
        "ax.axis('equal')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Temporal coverage\n",
        "ax = axes[1, 1]\n",
        "train_times = [d['query_timestamp'] for d in train_dataset] + [d['candidate_timestamp'] for d in train_dataset]\n",
        "val_times = [d['query_timestamp'] for d in val_dataset] + [d['candidate_timestamp'] for d in val_dataset]\n",
        "test_times = [d['query_timestamp'] for d in test_dataset] + [d['candidate_timestamp'] for d in test_dataset]\n",
        "\n",
        "ax.hist([train_times, val_times, test_times], bins=30, label=['Train', 'Val', 'Test'],\n",
        "       color=['blue', 'orange', 'green'], alpha=0.6)\n",
        "ax.axvline(train_end, color='red', linestyle='--', linewidth=2, label='Train/Val boundary')\n",
        "ax.axvline(val_end, color='red', linestyle='--', linewidth=2, label='Val/Test boundary')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Temporal Coverage of Splits')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Pair type distribution\n",
        "ax = axes[1, 2]\n",
        "pair_types = [d['pair_type'] for d in dataset]\n",
        "type_counts = pd.Series(pair_types).value_counts()\n",
        "colors = ['green', 'blue', 'orange', 'red']\n",
        "ax.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "ax.set_title('Dataset Composition by Pair Type')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dataset_diagnostics.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\nâœ… Diagnostic plots saved to: dataset_diagnostics.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_14_title"
      },
      "source": [
        "## <font color='#2E86AB'>â–¼ 14. Save Final Dataset</font>\n",
        "\n",
        "Package all data into a single file ready for Phase 2 (Fusion MLP Training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_dataset"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"SAVING FINAL DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create final dataset structure\n",
        "final_dataset = {\n",
        "    'metadata': {\n",
        "        'session_id': session_id,\n",
        "        'creation_date': datetime.now().isoformat(),\n",
        "        'num_valid_nodes': len(valid_nodes),\n",
        "        'num_total_pairs': len(dataset),\n",
        "        'num_positive_pairs': len(positive_pairs),\n",
        "        'num_easy_negative_pairs': len(easy_negative_pairs),\n",
        "        'num_hard_negative_pairs': len(hard_negative_pairs),\n",
        "        'positive_ratio': positive_ratio,\n",
        "        'feature_dim': 1536,\n",
        "        'combined_feature_dim': 1536,\n",
        "        'camera_feature_dim': 1280,\n",
        "        'lidar_feature_dim': 256,\n",
        "        'duration_seconds': duration_sec,\n",
        "        'map_dimensions': {'width': map_width, 'height': map_height}\n",
        "    },\n",
        "    'train': {\n",
        "        'features': np.array([d['pairwise_feature'] for d in train_dataset]),\n",
        "        'labels': np.array([d['label'] for d in train_dataset]),\n",
        "        'pair_info': [{k: v for k, v in d.items() if k != 'pairwise_feature'} for d in train_dataset]\n",
        "    },\n",
        "    'val': {\n",
        "        'features': np.array([d['pairwise_feature'] for d in val_dataset]),\n",
        "        'labels': np.array([d['label'] for d in val_dataset]),\n",
        "        'pair_info': [{k: v for k, v in d.items() if k != 'pairwise_feature'} for d in val_dataset]\n",
        "    },\n",
        "    'test': {\n",
        "        'features': np.array([d['pairwise_feature'] for d in test_dataset]),\n",
        "        'labels': np.array([d['label'] for d in test_dataset]),\n",
        "        'pair_info': [{k: v for k, v in d.items() if k != 'pairwise_feature'} for d in test_dataset]\n",
        "    },\n",
        "    'node_database': valid_nodes,  # Keep for analysis/debugging\n",
        "    'validation_report': {\n",
        "        'checks': validation_checks,\n",
        "        'all_passed': all_passed,\n",
        "        'critical_passed': critical_passed\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to pickle\n",
        "output_filename = 'loop_closure_dataset.pkl'\n",
        "with open(output_filename, 'wb') as f:\n",
        "    pickle.dump(final_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"\\nâœ… Dataset saved to: {output_filename}\")\n",
        "\n",
        "# Get file size\n",
        "file_size_mb = os.path.getsize(output_filename) / (1024 * 1024)\n",
        "print(f\"   File size: {file_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_final_report"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL DATASET GENERATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "report = f\"\"\"\n",
        "ğŸ“Š LOOP CLOSURE DATASET GENERATION - FINAL REPORT\n",
        "{'='*70}\n",
        "\n",
        "SESSION INFORMATION:\n",
        "  â€¢ Session ID: {session_id}\n",
        "  â€¢ Date generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "  â€¢ Duration: {duration_sec:.2f} seconds ({duration_sec/60:.2f} minutes)\n",
        "  â€¢ Map dimensions: {map_width:.2f}m Ã— {map_height:.2f}m\n",
        "\n",
        "TRAJECTORY & FEATURES:\n",
        "  â€¢ Total trajectory nodes: {len(trajectory_nodes)}\n",
        "  â€¢ Valid nodes (both modalities): {len(valid_nodes)}\n",
        "  â€¢ Camera alignment rate: {camera_alignment_rate:.1%}\n",
        "  â€¢ LiDAR alignment rate: {lidar_alignment_rate:.1%}\n",
        "\n",
        "CARTOGRAPHER CONSTRAINTS:\n",
        "  â€¢ INTER_SUBMAP (loop closures): {len(inter_submap_constraints)}\n",
        "  â€¢ Constraint-to-node match rate: {match_rate:.1%}\n",
        "\n",
        "DATASET COMPOSITION:\n",
        "  â€¢ Total pairs: {len(dataset)}\n",
        "  â€¢ Positive pairs: {len(positive_pairs)} ({100*len(positive_pairs)/len(dataset):.1f}%)\n",
        "  â€¢ Easy negative pairs: {len(easy_negative_pairs)} ({100*len(easy_negative_pairs)/len(dataset):.1f}%)\n",
        "  â€¢ Hard negative pairs: {len(hard_negative_pairs)} ({100*len(hard_negative_pairs)/len(dataset):.1f}%)\n",
        "      â†’ Type A (perceptual): {len(hard_negative_pairs_type_a)}\n",
        "      â†’ Type B (viewpoint): {len(hard_negative_pairs_type_b)}\n",
        "\n",
        "TRAIN/VAL/TEST SPLITS:\n",
        "  â€¢ Train: {len(train_dataset)} pairs ({100*len(train_dataset)/len(dataset):.1f}%)\n",
        "      â†’ Positive: {sum(d['label'] for d in train_dataset)} ({100*train_pos_ratio:.1f}%)\n",
        "  â€¢ Validation: {len(val_dataset)} pairs ({100*len(val_dataset)/len(dataset):.1f}%)\n",
        "      â†’ Positive: {sum(d['label'] for d in val_dataset)} ({100*val_pos_ratio:.1f}%)\n",
        "  â€¢ Test: {len(test_dataset)} pairs ({100*len(test_dataset)/len(dataset):.1f}%)\n",
        "      â†’ Positive: {sum(d['label'] for d in test_dataset)} ({100*test_pos_ratio:.1f}%)\n",
        "\n",
        "FEATURE STATISTICS:\n",
        "  â€¢ Pairwise feature dimension: {X.shape[1]}D\n",
        "  â€¢ Mean: {np.mean(X):.4f}\n",
        "  â€¢ Std: {np.std(X):.4f}\n",
        "  â€¢ Range: [{np.min(X):.4f}, {np.max(X):.4f}]\n",
        "\n",
        "VALIDATION STATUS:\n",
        "  {'âœ…' if all_passed else 'âš ï¸ ' if critical_passed else 'âŒ'} Overall: {'PASSED' if all_passed else 'PASSED WITH WARNINGS' if critical_passed else 'FAILED'}\n",
        "\"\"\"\n",
        "\n",
        "for check_name, check_result in validation_checks:\n",
        "    report += f\"  {'âœ…' if check_result else 'âŒ'} {check_name}\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "OUTPUT FILES:\n",
        "  â€¢ Dataset: {output_filename} ({file_size_mb:.2f} MB)\n",
        "  â€¢ Diagnostics: dataset_diagnostics.png\n",
        "  â€¢ Cartographer validation: cartographer_validation_report.txt\n",
        "\n",
        "NEXT STEPS:\n",
        "  1. Load dataset with: pickle.load(open('{output_filename}', 'rb'))\n",
        "  2. Train Fusion MLP (Phase 2): 1536â†’512â†’128â†’1 architecture\n",
        "  3. Use BCE loss + hard negative mining\n",
        "  4. Monitor validation performance\n",
        "  5. Export to ONNX/TensorRT for Jetson Nano deployment\n",
        "\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "print(report)\n",
        "\n",
        "# Save report\n",
        "with open('dataset_generation_report.txt', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"\\nâœ… Final report saved to: dataset_generation_report.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if all_passed:\n",
        "    print(\"ğŸ‰ DATASET GENERATION COMPLETE - READY FOR TRAINING!\")\n",
        "elif critical_passed:\n",
        "    print(\"âœ… DATASET GENERATION COMPLETE - USABLE WITH WARNINGS\")\n",
        "else:\n",
        "    print(\"âš ï¸  DATASET GENERATION COMPLETE - REVIEW VALIDATION ISSUES\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}