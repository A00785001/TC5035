{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A00785001/TC5035/blob/main/003-DL-Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKNVEsY6g-kk"
      },
      "source": [
        "# Feature Extraction Notebook - Camera & LiDAR (WITH TRAINING)\n",
        "\n",
        "## Overview\n",
        "This notebook extracts deep features from preprocessed camera images and LiDAR scans for sensor fusion and loop closure detection. It implements two parallel feature extraction branches:\n",
        "\n",
        "1. **Visual Branch**: MobileNet V2 â†’ 1280D features (pretrained on ImageNet)\n",
        "2. **Geometric Branch**: 1D CNN â†’ 256D features (**TRAINED** on your data using contrastive learning)\n",
        "\n",
        "Features are saved in HDF5 format for efficient storage and retrieval.\n",
        "\n",
        "## This notebook features:\n",
        "- âœ“ **Automatic LiDAR network training** using contrastive learning\n",
        "- âœ“ **GPU detection and optimization** for faster training\n",
        "- âœ“ **Comprehensive data validation** at each step\n",
        "- âœ“ **Google Colab compatibility** with Drive mounting\n",
        "- âœ“ **Extensive documentation** for each section\n",
        "\n",
        "## Prerequisites\n",
        "**Required Input Data:**\n",
        "- Preprocessed camera images in `processed_images/` directory\n",
        "  - Images in JPEG format (e.g., `img_00000.jpg`, `img_00001.jpg`, ...)\n",
        "  - `metadata.csv` with columns: filename, timestamp, timestamp_sec, timestamp_nsec, frame_id\n",
        "- Preprocessed LiDAR scans in `processed_lidar/` directory\n",
        "  - Scans in CSV format (e.g., `scan_00000.csv`, `scan_00001.csv`, ...)\n",
        "  - Each CSV contains 360 normalized distance values [0, 1]\n",
        "  - `metadata.csv` with columns: filename, timestamp, timestamp_sec, timestamp_nsec\n",
        "- Both datasets with aligned timestamps\n",
        "\n",
        "**Minimum Data Requirements:**\n",
        "- Camera images: 10+ images (50+ recommended for training)\n",
        "- LiDAR scans: 50+ scans (100+ recommended for good training)\n",
        "- Temporal span: 30+ seconds (for creating diverse training pairs)\n",
        "\n",
        "## Pipeline Architecture\n",
        "```\n",
        "Camera Images (224Ã—224) â†’ MobileNet V2 â†’ 1280D â†’ L2 Norm â†’ Visual Features\n",
        "LiDAR Scans (360,)      â†’ 1D CNN      â†’ 256D  â†’ L2 Norm â†’ Geometric Features\n",
        "                               â†“\n",
        "                    Contrastive Training\n",
        "                    (Self-supervised)\n",
        "                               â†“\n",
        "                      HDF5 Feature Database\n",
        "```\n",
        "\n",
        "## Expected Runtime\n",
        "- **With GPU**: 10-15 minutes total (training: ~5 min)\n",
        "- **CPU only**: 30-45 minutes total (training: ~20 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpDFF3Fvg-kl"
      },
      "source": [
        "## Section 0: Environment Setup & GPU Detection\n",
        "\n",
        "### Purpose\n",
        "- Install required packages\n",
        "- Detect and configure GPU (if available)\n",
        "- Set up Google Drive mounting (for Colab)\n",
        "- Verify environment is ready\n",
        "\n",
        "### Expected Output\n",
        "- Confirmation of GPU availability (or CPU fallback)\n",
        "- List of installed packages\n",
        "- Drive mount status (if using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SERlAYCyg-kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebdfade-8d3a-47b6-baa7-cb87f97cca74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m855.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ“ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet --upgrade torch torchvision\n",
        "!pip install --quiet h5py pandas numpy matplotlib tqdm pillow\n",
        "\n",
        "print(\"âœ“ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G04R92lxg-km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5406082b-e1be-4758-908b-f47a61dba863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import random\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"âœ“ Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "detect_gpu_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18315ae3-c74f-4427-93e9-81a1fc158708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GPU DETECTION & CONFIGURATION\n",
            "======================================================================\n",
            "âœ“ GPU AVAILABLE\n",
            "  Device: Tesla T4\n",
            "  Memory: 14.7 GB\n",
            "  CUDA Version: 12.8\n",
            "\n",
            "âœ“ Training will use GPU acceleration\n",
            "  Expected training time: ~5-10 minutes\n",
            "\n",
            "Active device: cuda\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# GPU Detection and Configuration\n",
        "print(\"=\" * 70)\n",
        "print(\"GPU DETECTION & CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "\n",
        "    print(f\"âœ“ GPU AVAILABLE\")\n",
        "    print(f\"  Device: {gpu_name}\")\n",
        "    print(f\"  Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"\\nâœ“ Training will use GPU acceleration\")\n",
        "    print(f\"  Expected training time: ~5-10 minutes\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(f\"âš  NO GPU DETECTED - Using CPU\")\n",
        "    print(f\"  Training will be slower on CPU\")\n",
        "    print(f\"  Expected training time: ~20-30 minutes\")\n",
        "    print(f\"\\n  TIP: In Colab, enable GPU via:\")\n",
        "    print(f\"       Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
        "\n",
        "print(f\"\\nActive device: {device}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mount_drive_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa40b53-9ae5-4268-9e21-180df407e1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ“ Google Drive mounted successfully\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (for Colab users)\n",
        "# Skip this cell if running locally\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ“ Google Drive mounted successfully\")\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    print(\"â„¹ Not running in Colab - skipping Drive mount\")\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "config_paths_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29a42db-fc5f-451e-d57b-0090467a6913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session ID: 20251022_155137\n",
            "Working folder: /content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot/session_20251022_155137\n",
            "âœ“ Changed directory to: /content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot/session_20251022_155137\n"
          ]
        }
      ],
      "source": [
        "# Configure Session and Paths\n",
        "# MODIFY THESE VARIABLES FOR YOUR DATA\n",
        "\n",
        "#session = '20251016_133216'  # â† Change this to your session ID\n",
        "session = '20251022_155137'\n",
        "\n",
        "# Path configuration (modify if needed)\n",
        "if IN_COLAB:\n",
        "    # Google Drive path for Colab\n",
        "    data_path = \"/content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot\"\n",
        "else:\n",
        "    # Local path\n",
        "    data_path = \"./data\"  # â† Change this to your local data path\n",
        "\n",
        "working_folder = os.path.join(data_path, f'session_{session}')\n",
        "\n",
        "print(f\"Session ID: {session}\")\n",
        "print(f\"Working folder: {working_folder}\")\n",
        "\n",
        "# Change to working directory\n",
        "if os.path.exists(working_folder):\n",
        "    os.chdir(working_folder)\n",
        "    print(f\"âœ“ Changed directory to: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âŒ ERROR: Working folder not found!\")\n",
        "    print(f\"   Please check the path: {working_folder}\")\n",
        "    raise FileNotFoundError(f\"Directory not found: {working_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_validation_header"
      },
      "source": [
        "## Section 0.5: Data Validation\n",
        "\n",
        "### Purpose\n",
        "Verify that all required input data exists and is properly formatted before proceeding with feature extraction.\n",
        "\n",
        "### Expected Inputs\n",
        "**Camera Data** (`processed_images/`):\n",
        "- Directory containing JPEG images\n",
        "- `metadata.csv` with columns: filename, timestamp, timestamp_sec, timestamp_nsec, frame_id\n",
        "- Images should be 640Ã—480 RGB (or similar resolution)\n",
        "\n",
        "**LiDAR Data** (`processed_lidar/`):\n",
        "- Directory containing CSV scan files\n",
        "- `metadata.csv` with columns: filename, timestamp, timestamp_sec, timestamp_nsec\n",
        "- Each scan CSV should have 360 values normalized to [0, 1]\n",
        "\n",
        "### Validation Checks\n",
        "1. âœ“ Directories exist\n",
        "2. âœ“ Metadata files exist and are readable\n",
        "3. âœ“ Required columns present\n",
        "4. âœ“ Sufficient data for training (minimum thresholds)\n",
        "5. âœ“ Sample files can be loaded\n",
        "6. âœ“ Data format is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "validate_data_cell",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "ab3955c6-fa28-4ec7-b277-829dbefd02e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA VALIDATION\n",
            "======================================================================\n",
            "\n",
            "[1/2] Validating Camera Data...\n",
            "  âœ“ Camera directory exists: processed_images\n",
            "  âœ“ Camera metadata file exists\n",
            "  âœ“ Camera metadata loaded: 7193 entries\n",
            "  âœ“ All required columns present\n",
            "  âœ“ Sample image loaded: 224Ã—224 RGB\n",
            "\n",
            "[2/2] Validating LiDAR Data...\n",
            "  âœ“ LiDAR directory exists: processed_lidar\n",
            "  âœ“ LiDAR metadata file exists\n",
            "  âœ“ LiDAR metadata loaded: 2973 entries\n",
            "  âœ“ Sufficient data for training: 2973 scans\n",
            "  âœ“ Sample scan loaded: 360 points\n",
            "  âœ“ Value range: [0.0424, 1.0000]\n",
            "  âœ“ Temporal span: 394.2 seconds\n",
            "\n",
            "======================================================================\n",
            "VALIDATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "âŒ VALIDATION FAILED\n",
            "\n",
            "Errors found:\n",
            "  âŒ LiDAR metadata missing columns: ['timestamp_sec', 'timestamp_nsec']\n",
            "\n",
            "Please fix the errors above before proceeding.\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data validation failed. See errors above.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-301503577.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m# Run validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-301503577.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPlease fix the errors above before proceeding.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data validation failed. See errors above.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ“ ALL VALIDATION CHECKS PASSED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data validation failed. See errors above."
          ]
        }
      ],
      "source": [
        "# Data Validation Function\n",
        "def validate_data():\n",
        "    \"\"\"\n",
        "    Comprehensive validation of input data.\n",
        "    Returns True if all checks pass, raises detailed errors otherwise.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"DATA VALIDATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    errors = []\n",
        "    warnings = []\n",
        "\n",
        "    # ========== CAMERA DATA VALIDATION ==========\n",
        "    print(\"\\n[1/2] Validating Camera Data...\")\n",
        "\n",
        "    camera_dir = \"processed_images\"\n",
        "    camera_metadata_path = os.path.join(camera_dir, \"metadata.csv\")\n",
        "\n",
        "    # Check directory exists\n",
        "    if not os.path.exists(camera_dir):\n",
        "        errors.append(f\"âŒ Camera directory not found: {camera_dir}\")\n",
        "    else:\n",
        "        print(f\"  âœ“ Camera directory exists: {camera_dir}\")\n",
        "\n",
        "    # Check metadata file\n",
        "    if not os.path.exists(camera_metadata_path):\n",
        "        errors.append(f\"âŒ Camera metadata not found: {camera_metadata_path}\")\n",
        "    else:\n",
        "        print(f\"  âœ“ Camera metadata file exists\")\n",
        "\n",
        "        # Load and validate metadata\n",
        "        try:\n",
        "            cam_meta = pd.read_csv(camera_metadata_path)\n",
        "            print(f\"  âœ“ Camera metadata loaded: {len(cam_meta)} entries\")\n",
        "\n",
        "            # Check required columns\n",
        "            required_cols = ['filename', 'timestamp', 'timestamp_sec', 'timestamp_nsec']\n",
        "            missing_cols = [col for col in required_cols if col not in cam_meta.columns]\n",
        "            if missing_cols:\n",
        "                errors.append(f\"âŒ Camera metadata missing columns: {missing_cols}\")\n",
        "            else:\n",
        "                print(f\"  âœ“ All required columns present\")\n",
        "\n",
        "            # Check minimum data\n",
        "            if len(cam_meta) < 10:\n",
        "                warnings.append(f\"âš  Only {len(cam_meta)} camera images (10+ recommended)\")\n",
        "\n",
        "            # Validate sample image\n",
        "            if len(cam_meta) > 0:\n",
        "                sample_img_path = os.path.join(camera_dir, cam_meta.iloc[0]['filename'])\n",
        "                if os.path.exists(sample_img_path):\n",
        "                    try:\n",
        "                        img = Image.open(sample_img_path)\n",
        "                        print(f\"  âœ“ Sample image loaded: {img.size[0]}Ã—{img.size[1]} {img.mode}\")\n",
        "                    except Exception as e:\n",
        "                        errors.append(f\"âŒ Cannot load sample image: {e}\")\n",
        "                else:\n",
        "                    errors.append(f\"âŒ Sample image not found: {sample_img_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            errors.append(f\"âŒ Cannot read camera metadata: {e}\")\n",
        "\n",
        "    # ========== LIDAR DATA VALIDATION ==========\n",
        "    print(\"\\n[2/2] Validating LiDAR Data...\")\n",
        "\n",
        "    lidar_dir = \"processed_lidar\"\n",
        "    lidar_metadata_path = os.path.join(lidar_dir, \"metadata.csv\")\n",
        "\n",
        "    # Check directory exists\n",
        "    if not os.path.exists(lidar_dir):\n",
        "        errors.append(f\"âŒ LiDAR directory not found: {lidar_dir}\")\n",
        "    else:\n",
        "        print(f\"  âœ“ LiDAR directory exists: {lidar_dir}\")\n",
        "\n",
        "    # Check metadata file\n",
        "    if not os.path.exists(lidar_metadata_path):\n",
        "        errors.append(f\"âŒ LiDAR metadata not found: {lidar_metadata_path}\")\n",
        "    else:\n",
        "        print(f\"  âœ“ LiDAR metadata file exists\")\n",
        "\n",
        "        # Load and validate metadata\n",
        "        try:\n",
        "            lidar_meta = pd.read_csv(lidar_metadata_path)\n",
        "            print(f\"  âœ“ LiDAR metadata loaded: {len(lidar_meta)} entries\")\n",
        "\n",
        "            # Check required columns\n",
        "            required_cols = ['filename', 'timestamp', 'timestamp_sec', 'timestamp_nsec']\n",
        "            missing_cols = [col for col in required_cols if col not in lidar_meta.columns]\n",
        "            if missing_cols:\n",
        "                errors.append(f\"âŒ LiDAR metadata missing columns: {missing_cols}\")\n",
        "            else:\n",
        "                print(f\"  âœ“ All required columns present\")\n",
        "\n",
        "            # Check minimum data for training\n",
        "            if len(lidar_meta) < 50:\n",
        "                warnings.append(f\"âš  Only {len(lidar_meta)} LiDAR scans (50+ recommended for training)\")\n",
        "            else:\n",
        "                print(f\"  âœ“ Sufficient data for training: {len(lidar_meta)} scans\")\n",
        "\n",
        "            # Validate sample scan\n",
        "            if len(lidar_meta) > 0:\n",
        "                sample_scan_path = os.path.join(lidar_dir, lidar_meta.iloc[0]['filename'])\n",
        "                if os.path.exists(sample_scan_path):\n",
        "                    try:\n",
        "                        scan = pd.read_csv(sample_scan_path, header=None).values[0]\n",
        "                        print(f\"  âœ“ Sample scan loaded: {len(scan)} points\")\n",
        "                        print(f\"  âœ“ Value range: [{scan.min():.4f}, {scan.max():.4f}]\")\n",
        "\n",
        "                        if len(scan) != 360:\n",
        "                            errors.append(f\"âŒ Expected 360 points, got {len(scan)}\")\n",
        "                        if scan.min() < 0 or scan.max() > 1.1:  # Allow small margin\n",
        "                            warnings.append(f\"âš  Values outside [0,1] range: [{scan.min():.4f}, {scan.max():.4f}]\")\n",
        "                    except Exception as e:\n",
        "                        errors.append(f\"âŒ Cannot load sample scan: {e}\")\n",
        "                else:\n",
        "                    errors.append(f\"âŒ Sample scan not found: {sample_scan_path}\")\n",
        "\n",
        "            # Check temporal span\n",
        "            if len(lidar_meta) > 1:\n",
        "                time_span = lidar_meta['timestamp'].max() - lidar_meta['timestamp'].min()\n",
        "                print(f\"  âœ“ Temporal span: {time_span:.1f} seconds\")\n",
        "                if time_span < 30:\n",
        "                    warnings.append(f\"âš  Short temporal span ({time_span:.1f}s < 30s recommended)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            errors.append(f\"âŒ Cannot read LiDAR metadata: {e}\")\n",
        "\n",
        "    # ========== SUMMARY ==========\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VALIDATION SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if warnings:\n",
        "        print(\"\\nWarnings:\")\n",
        "        for warning in warnings:\n",
        "            print(f\"  {warning}\")\n",
        "\n",
        "    if errors:\n",
        "        print(\"\\nâŒ VALIDATION FAILED\")\n",
        "        print(\"\\nErrors found:\")\n",
        "        for error in errors:\n",
        "            print(f\"  {error}\")\n",
        "        print(\"\\nPlease fix the errors above before proceeding.\")\n",
        "        print(\"=\" * 70)\n",
        "        raise ValueError(\"Data validation failed. See errors above.\")\n",
        "    else:\n",
        "        print(\"\\nâœ“ ALL VALIDATION CHECKS PASSED\")\n",
        "        print(\"  Data is ready for feature extraction!\")\n",
        "        print(\"=\" * 70)\n",
        "        return True\n",
        "\n",
        "# Run validation\n",
        "validate_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFy20u_Sg-km"
      },
      "source": [
        "## Section 1: Camera Feature Extraction (Visual Branch)\n",
        "\n",
        "### Purpose\n",
        "Extract high-level visual features from camera images using MobileNet V2, a lightweight CNN pretrained on ImageNet. These features capture semantic and appearance information crucial for place recognition.\n",
        "\n",
        "### Expected Inputs\n",
        "- **Directory**: `processed_images/`\n",
        "- **Format**: JPEG images (any resolution, will be resized to 224Ã—224)\n",
        "- **Metadata**: `processed_images/metadata.csv`\n",
        "- **Minimum**: 10+ images (more is better)\n",
        "\n",
        "### Architecture: MobileNet V2\n",
        "- **Input**: 224Ã—224Ã—3 RGB images\n",
        "- **Preprocessing**: Scale pixels from [0,255] to [-1,1] (ImageNet normalization)\n",
        "- **Backbone**: MobileNet V2 (pretrained on ImageNet)\n",
        "- **Feature Layer**: Before final classification layer\n",
        "- **Raw Output**: 1280D feature vector\n",
        "- **Post-processing**: L2 normalization\n",
        "- **Final Output**: 1280D normalized feature vector\n",
        "\n",
        "### What This Section Does\n",
        "1. Load preprocessed images from `processed_images/`\n",
        "2. Apply MobileNet V2 preprocessing (ImageNet normalization)\n",
        "3. Extract 1280D features using pretrained MobileNet V2\n",
        "4. L2 normalize features for similarity comparison\n",
        "5. Store features in memory (will save to HDF5 later)\n",
        "\n",
        "### Why MobileNet V2?\n",
        "- **Lightweight**: ~3.5M parameters\n",
        "- **Fast inference**: ~20-30ms on CPU, ~5-10ms on GPU\n",
        "- **Robust features**: Proven for place recognition\n",
        "- **Pretrained**: Trained on ImageNet (1000 classes, 1.2M images)\n",
        "- **Efficient**: Works well on embedded systems (Jetbot)\n",
        "\n",
        "### Output\n",
        "- `camera_features`: Numpy array [N_images, 1280]\n",
        "- `camera_timestamps`: Numpy array [N_images]\n",
        "- `camera_filenames`: Numpy array [N_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBN_w3RDg-km"
      },
      "outputs": [],
      "source": [
        "# Configuration for camera feature extraction\n",
        "CAMERA_INPUT_DIR = \"processed_images\"\n",
        "CAMERA_BATCH_SIZE = 32  # Adjust based on GPU memory (32 for GPU, 8 for CPU)\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    CAMERA_BATCH_SIZE = 8  # Smaller batch for CPU\n",
        "\n",
        "print(f\"Camera Configuration:\")\n",
        "print(f\"  Input directory: {CAMERA_INPUT_DIR}\")\n",
        "print(f\"  Batch size: {CAMERA_BATCH_SIZE}\")\n",
        "print(f\"  Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUgEIcs9g-km"
      },
      "outputs": [],
      "source": [
        "# Load camera metadata\n",
        "camera_metadata = pd.read_csv(os.path.join(CAMERA_INPUT_DIR, 'metadata.csv'))\n",
        "\n",
        "print(f\"\\nğŸ“· Camera Data Summary:\")\n",
        "print(f\"  Total images: {len(camera_metadata)}\")\n",
        "print(f\"  Time range: {camera_metadata['timestamp'].min():.2f} - {camera_metadata['timestamp'].max():.2f} sec\")\n",
        "print(f\"  Duration: {camera_metadata['timestamp'].max() - camera_metadata['timestamp'].min():.2f} sec\")\n",
        "print(f\"\\nFirst few entries:\")\n",
        "display(camera_metadata.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfS_9NeHg-km"
      },
      "outputs": [],
      "source": [
        "# Load MobileNet V2 (pretrained on ImageNet)\n",
        "print(\"Loading MobileNet V2...\")\n",
        "\n",
        "# Load pretrained model\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Remove the final classification layer to get features\n",
        "# MobileNet V2 structure: features â†’ classifier\n",
        "# We want the output of 'features' (before classifier)\n",
        "feature_extractor = nn.Sequential(\n",
        "    mobilenet.features,\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"âœ“ MobileNet V2 loaded on {device}\")\n",
        "print(f\"  Feature dimension: 1280D\")\n",
        "print(f\"  Pretrained: ImageNet (1.2M images, 1000 classes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3GJmnlTg-kn"
      },
      "outputs": [],
      "source": [
        "# Define MobileNet V2 preprocessing\n",
        "# Input: [0, 255] RGB â†’ Output: [-1, 1] normalized (ImageNet stats)\n",
        "mobilenet_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to MobileNet input size\n",
        "    transforms.ToTensor(),  # [0, 255] â†’ [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
        "                        std=[0.229, 0.224, 0.225])     # Scale to ~[-1, 1]\n",
        "])\n",
        "\n",
        "print(\"âœ“ MobileNet V2 preprocessing pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frw9BTwog-kn"
      },
      "outputs": [],
      "source": [
        "# Extract camera features\n",
        "print(\"\\nExtracting camera features...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "camera_features_list = []\n",
        "camera_timestamps_list = []\n",
        "camera_filenames_list = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for idx in tqdm(range(0, len(camera_metadata), CAMERA_BATCH_SIZE), desc=\"ğŸ“· Camera batches\"):\n",
        "        batch_meta = camera_metadata.iloc[idx:idx+CAMERA_BATCH_SIZE]\n",
        "\n",
        "        # Load batch of images\n",
        "        batch_images = []\n",
        "        batch_timestamps = []\n",
        "        batch_filenames = []\n",
        "\n",
        "        for _, row in batch_meta.iterrows():\n",
        "            img_path = os.path.join(CAMERA_INPUT_DIR, row['filename'])\n",
        "\n",
        "            try:\n",
        "                # Load and preprocess image\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_tensor = mobilenet_transform(img)\n",
        "                batch_images.append(img_tensor)\n",
        "                batch_timestamps.append(row['timestamp'])\n",
        "                batch_filenames.append(row['filename'])\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Error loading {img_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(batch_images) == 0:\n",
        "            continue\n",
        "\n",
        "        # Stack into batch tensor\n",
        "        batch_tensor = torch.stack(batch_images).to(device)\n",
        "\n",
        "        # Extract features\n",
        "        features = feature_extractor(batch_tensor)\n",
        "\n",
        "        # L2 normalize features\n",
        "        features = F.normalize(features, p=2, dim=1)\n",
        "\n",
        "        # Move to CPU and store\n",
        "        camera_features_list.append(features.cpu().numpy())\n",
        "        camera_timestamps_list.extend(batch_timestamps)\n",
        "        camera_filenames_list.extend(batch_filenames)\n",
        "\n",
        "# Concatenate all batches\n",
        "camera_features = np.vstack(camera_features_list)\n",
        "camera_timestamps = np.array(camera_timestamps_list)\n",
        "camera_filenames = np.array(camera_filenames_list)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nâœ“ Camera feature extraction complete!\")\n",
        "print(f\"  Shape: {camera_features.shape}\")\n",
        "print(f\"  Feature dimension: {camera_features.shape[1]}D\")\n",
        "print(f\"  Number of images: {camera_features.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTJxr9EEg-kn"
      },
      "outputs": [],
      "source": [
        "# Verify camera feature properties\n",
        "print(\"\\nğŸ“Š Camera Feature Statistics:\")\n",
        "print(f\"  Shape: {camera_features.shape}\")\n",
        "print(f\"  Mean: {camera_features.mean():.4f}\")\n",
        "print(f\"  Std: {camera_features.std():.4f}\")\n",
        "print(f\"  Min: {camera_features.min():.4f}\")\n",
        "print(f\"  Max: {camera_features.max():.4f}\")\n",
        "\n",
        "# Check L2 normalization (should be ~1.0)\n",
        "norms = np.linalg.norm(camera_features, axis=1)\n",
        "print(f\"  L2 norms: mean={norms.mean():.4f}, std={norms.std():.6f}\")\n",
        "print(f\"  âœ“ Features are L2 normalized (norms â‰ˆ 1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-LBtt8g-kn"
      },
      "source": [
        "## Section 2: LiDAR Feature Extraction (Geometric Branch) - UNTRAINED\n",
        "\n",
        "### Purpose\n",
        "Extract geometric features from LiDAR scans using a custom 1D CNN. These features capture spatial structure and geometry information complementary to visual features.\n",
        "\n",
        "### Expected Inputs\n",
        "- **Directory**: `processed_lidar/`\n",
        "- **Format**: CSV files with 360 distance values\n",
        "- **Value range**: [0, 1] (normalized)\n",
        "- **Metadata**: `processed_lidar/metadata.csv`\n",
        "- **Minimum**: 50+ scans for good training (100+ recommended)\n",
        "\n",
        "### Architecture: 1D CNN\n",
        "- **Input**: 360 normalized distance values [0, 1]\n",
        "- **Architecture**:\n",
        "  - Conv1D(1â†’64, kernel=5) + ReLU + BatchNorm\n",
        "  - Conv1D(64â†’128, kernel=5) + ReLU + BatchNorm\n",
        "  - Conv1D(128â†’256, kernel=3) + ReLU + BatchNorm\n",
        "  - Conv1D(256â†’256, kernel=3) + ReLU + BatchNorm\n",
        "  - Global Average Pooling\n",
        "- **Parameters**: ~350K\n",
        "- **Raw Output**: 256D feature vector\n",
        "- **Post-processing**: L2 normalization\n",
        "- **Final Output**: 256D normalized feature vector\n",
        "\n",
        "### What This Section Does\n",
        "1. Define 1D CNN architecture\n",
        "2. Initialize with **random weights** (will train in next section)\n",
        "3. Test the network with a sample scan\n",
        "4. Verify architecture is working\n",
        "\n",
        "### Important Note\n",
        "âš ï¸ **This network starts with random initialization**. Features will be **meaningless until trained**. The next section (2.5) will train the network using contrastive learning.\n",
        "\n",
        "### Why 1D CNN?\n",
        "- Captures local geometric patterns in 360Â° scans\n",
        "- Translation-invariant along angular dimension\n",
        "- Lightweight: ~350K params (10Ã— smaller than MobileNet V2)\n",
        "- Fast: ~20-30ms inference on embedded systems\n",
        "- Proven effective for LiDAR-based place recognition\n",
        "\n",
        "### Output (Preliminary)\n",
        "- Network architecture defined and tested\n",
        "- Ready for training in Section 2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOu8S0VJg-kn"
      },
      "outputs": [],
      "source": [
        "# Configuration for LiDAR feature extraction\n",
        "LIDAR_INPUT_DIR = \"processed_lidar\"\n",
        "LIDAR_BATCH_SIZE = 64  # LiDAR is lighter than images\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    LIDAR_BATCH_SIZE = 32  # Smaller batch for CPU\n",
        "\n",
        "print(f\"LiDAR Configuration:\")\n",
        "print(f\"  Input directory: {LIDAR_INPUT_DIR}\")\n",
        "print(f\"  Batch size: {LIDAR_BATCH_SIZE}\")\n",
        "print(f\"  Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgb680p6g-kn"
      },
      "outputs": [],
      "source": [
        "# Load LiDAR metadata\n",
        "lidar_metadata = pd.read_csv(os.path.join(LIDAR_INPUT_DIR, 'metadata.csv'))\n",
        "\n",
        "print(f\"\\nğŸ¯ LiDAR Data Summary:\")\n",
        "print(f\"  Total scans: {len(lidar_metadata)}\")\n",
        "print(f\"  Time range: {lidar_metadata['timestamp'].min():.2f} - {lidar_metadata['timestamp'].max():.2f} sec\")\n",
        "print(f\"  Duration: {lidar_metadata['timestamp'].max() - lidar_metadata['timestamp'].min():.2f} sec\")\n",
        "print(f\"\\nFirst few entries:\")\n",
        "display(lidar_metadata.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvhSRByQg-kn"
      },
      "outputs": [],
      "source": [
        "# Define 1D CNN for Geometric Feature Extraction\n",
        "class GeometricCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    1D CNN for LiDAR geometric feature extraction.\n",
        "\n",
        "    Architecture:\n",
        "    - 4 Conv1D layers with increasing channels\n",
        "    - BatchNorm + ReLU after each conv\n",
        "    - Global Average Pooling\n",
        "    - Output: 256D feature vector\n",
        "\n",
        "    Parameters: ~350K (matching pipeline specification)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=360, output_dim=256):\n",
        "        super(GeometricCNN, self).__init__()\n",
        "\n",
        "        # Layer 1: 1 â†’ 64 channels\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        # Layer 2: 64 â†’ 128 channels\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Layer 3: 128 â†’ 256 channels\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Layer 4: 256 â†’ 256 channels\n",
        "        self.conv4 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 1, 360)\n",
        "\n",
        "        # Conv block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Conv block 2\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        # Conv block 3\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Conv block 4\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)  # (batch, 256, 1)\n",
        "        x = x.squeeze(-1)  # (batch, 256)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"âœ“ GeometricCNN class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bueq-fiKg-kn"
      },
      "outputs": [],
      "source": [
        "# Initialize the 1D CNN (with random weights)\n",
        "print(\"\\nInitializing Geometric CNN...\")\n",
        "\n",
        "geometric_cnn = GeometricCNN(input_dim=360, output_dim=256).to(device)\n",
        "geometric_cnn.eval()  # Evaluation mode for now\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in geometric_cnn.parameters())\n",
        "trainable_params = sum(p.numel() for p in geometric_cnn.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nâœ“ Geometric CNN initialized on {device}\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Feature dimension: 256D\")\n",
        "print(f\"\\nâš ï¸  Network uses RANDOM INITIALIZATION\")\n",
        "print(f\"  Features will be trained in Section 2.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ron8xH6g-ko"
      },
      "outputs": [],
      "source": [
        "# Test the network with a sample scan\n",
        "print(\"\\nTesting network with sample scan...\")\n",
        "\n",
        "# Load first scan\n",
        "sample_scan = pd.read_csv(\n",
        "    os.path.join(LIDAR_INPUT_DIR, lidar_metadata.iloc[0]['filename']),\n",
        "    header=None\n",
        ").values[0]\n",
        "\n",
        "print(f\"  Sample scan shape: {sample_scan.shape}\")\n",
        "print(f\"  Value range: [{sample_scan.min():.4f}, {sample_scan.max():.4f}]\")\n",
        "\n",
        "# Convert to tensor and add batch/channel dimensions\n",
        "sample_tensor = torch.tensor(sample_scan, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "print(f\"  Tensor shape: {sample_tensor.shape}\")\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    sample_features = geometric_cnn(sample_tensor)\n",
        "    sample_features_norm = F.normalize(sample_features, p=2, dim=1)\n",
        "\n",
        "print(f\"\\n  Output features shape: {sample_features.shape}\")\n",
        "print(f\"  Feature norm (before L2): {torch.norm(sample_features, dim=1).item():.4f}\")\n",
        "print(f\"  Feature norm (after L2): {torch.norm(sample_features_norm, dim=1).item():.4f}\")\n",
        "print(f\"\\nâœ“ Network test successful!\")\n",
        "print(f\"  Network is ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section_header"
      },
      "source": [
        "## Section 2.5: Train the LiDAR Feature Extractor ğŸš€\n",
        "\n",
        "### Purpose\n",
        "Train the 1D CNN using **contrastive learning** on your own data. This teaches the network: \"similar places â†’ similar features, different places â†’ different features\".\n",
        "\n",
        "### Expected Inputs (From Previous Sections)\n",
        "- `lidar_metadata`: DataFrame with scan filenames and timestamps\n",
        "- `geometric_cnn`: Initialized 1D CNN network (currently random)\n",
        "- **Minimum data**: 50+ scans spanning 30+ seconds\n",
        "\n",
        "### Training Approach: Contrastive Learning (Self-Supervised)\n",
        "**No manual labeling needed!** We automatically create training pairs:\n",
        "- **Similar pairs**: Scans close in time (within 5s) â†’ likely same location\n",
        "- **Different pairs**: Scans far apart (>30s) â†’ likely different locations\n",
        "\n",
        "**Loss Function**: Contrastive Loss\n",
        "- Similar pairs: Minimize distance between features\n",
        "- Different pairs: Maximize distance (up to margin)\n",
        "\n",
        "### Training Configuration\n",
        "- **Epochs**: 20 (adjustable)\n",
        "- **Learning rate**: 0.001\n",
        "- **Batch size**: 16\n",
        "- **Optimizer**: Adam\n",
        "- **Similar threshold**: 5 seconds\n",
        "- **Different threshold**: 30 seconds\n",
        "- **Margin**: 1.0\n",
        "\n",
        "### Expected Training Time\n",
        "- **With GPU**: 5-10 minutes (100 scans)\n",
        "- **With CPU**: 20-30 minutes (100 scans)\n",
        "\n",
        "### What This Section Does\n",
        "1. Create training pairs automatically from timestamps\n",
        "2. Define contrastive loss function\n",
        "3. Set up data loaders\n",
        "4. Train the network for 20 epochs\n",
        "5. Visualize training progress\n",
        "6. Re-extract features with trained network\n",
        "\n",
        "### Output\n",
        "- Trained 1D CNN network\n",
        "- Training/validation loss curves\n",
        "- New `lidar_features` with **meaningful** features (not random!)\n",
        "- Network ready for loop closure detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config_cell"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "print(\"=\" * 70)\n",
        "print(\"CONTRASTIVE LEARNING CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Temporal thresholds for creating pairs\n",
        "TEMPORAL_THRESHOLD_SIMILAR = 5.0      # seconds - scans within 5s are \"similar\"\n",
        "TEMPORAL_THRESHOLD_DIFFERENT = 30.0   # seconds - scans >30s apart are \"different\"\n",
        "\n",
        "# Training hyperparameters\n",
        "MARGIN = 1.0                          # margin for contrastive loss\n",
        "LEARNING_RATE = 0.001                 # learning rate\n",
        "BATCH_SIZE = 16                       # batch size\n",
        "NUM_EPOCHS = 20                       # number of training epochs\n",
        "\n",
        "# Adjust for CPU if needed\n",
        "if device.type == 'cpu':\n",
        "    BATCH_SIZE = 8\n",
        "    print(\"âš  CPU detected - reducing batch size to 8\")\n",
        "\n",
        "print(f\"\\nTemporal Thresholds:\")\n",
        "print(f\"  Similar pairs: scans within {TEMPORAL_THRESHOLD_SIMILAR}s\")\n",
        "print(f\"  Different pairs: scans >{TEMPORAL_THRESHOLD_DIFFERENT}s apart\")\n",
        "print(f\"\\nTraining Hyperparameters:\")\n",
        "print(f\"  Margin: {MARGIN}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_pairs_cell"
      },
      "outputs": [],
      "source": [
        "# Create Training Pairs from Temporal Information\n",
        "print(\"\\nCreating training pairs from temporal information...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Sort scans by timestamp\n",
        "sorted_indices = np.argsort(lidar_metadata['timestamp'].values)\n",
        "sorted_timestamps = lidar_metadata['timestamp'].values[sorted_indices]\n",
        "\n",
        "similar_pairs = []\n",
        "different_pairs = []\n",
        "\n",
        "# Create similar pairs (scans close in time)\n",
        "print(\"Creating pairs...\")\n",
        "for i in tqdm(range(len(sorted_timestamps)), desc=\"Processing scans\"):\n",
        "    for j in range(i + 1, len(sorted_timestamps)):\n",
        "        time_diff = abs(sorted_timestamps[j] - sorted_timestamps[i])\n",
        "\n",
        "        if time_diff <= TEMPORAL_THRESHOLD_SIMILAR:\n",
        "            similar_pairs.append((sorted_indices[i], sorted_indices[j], 1))  # label=1 for similar\n",
        "        elif time_diff >= TEMPORAL_THRESHOLD_DIFFERENT:\n",
        "            # Only add some different pairs to balance dataset\n",
        "            if random.random() < 0.3:  # 30% sampling rate\n",
        "                different_pairs.append((sorted_indices[i], sorted_indices[j], 0))  # label=0 for different\n",
        "            break  # No need to check further for this i\n",
        "\n",
        "# Combine and shuffle\n",
        "all_pairs = similar_pairs + different_pairs\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "# Split into train/val (80/20)\n",
        "split_idx = int(0.8 * len(all_pairs))\n",
        "train_pairs = all_pairs[:split_idx]\n",
        "val_pairs = all_pairs[split_idx:]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nğŸ“Š Pair Statistics:\")\n",
        "print(f\"  Similar pairs: {len(similar_pairs)}\")\n",
        "print(f\"  Different pairs: {len(different_pairs)}\")\n",
        "print(f\"  Total pairs: {len(all_pairs)}\")\n",
        "print(f\"  Training pairs: {len(train_pairs)}\")\n",
        "print(f\"  Validation pairs: {len(val_pairs)}\")\n",
        "\n",
        "# Check if we have enough pairs\n",
        "if len(all_pairs) < 50:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {len(all_pairs)} pairs created\")\n",
        "    print(f\"     Recommended: 100+ pairs for good training\")\n",
        "    print(f\"     Consider: More scans or longer temporal span\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Sufficient pairs for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "contrastive_loss_cell"
      },
      "outputs": [],
      "source": [
        "# Define Contrastive Loss Function\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "\n",
        "    For similar pairs (label=1): minimize distance\n",
        "    For different pairs (label=0): maximize distance (up to margin)\n",
        "\n",
        "    Formula:\n",
        "        L = (1/2) * [Y * D^2 + (1-Y) * max(margin - D, 0)^2]\n",
        "    where:\n",
        "        Y = label (1 for similar, 0 for different)\n",
        "        D = Euclidean distance between feature vectors\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, feature1, feature2, label):\n",
        "        # Euclidean distance between feature vectors\n",
        "        distance = F.pairwise_distance(feature1, feature2)\n",
        "\n",
        "        # Contrastive loss\n",
        "        loss_similar = label * torch.pow(distance, 2)\n",
        "        loss_different = (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
        "\n",
        "        loss = torch.mean(loss_similar + loss_different)\n",
        "        return loss\n",
        "\n",
        "# Initialize loss function\n",
        "criterion = ContrastiveLoss(margin=MARGIN)\n",
        "print(\"âœ“ Contrastive loss function defined\")\n",
        "print(f\"  Margin: {MARGIN}\")\n",
        "print(f\"  Similar pairs: minimize distance\")\n",
        "print(f\"  Different pairs: push apart (up to margin)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loader_cell"
      },
      "outputs": [],
      "source": [
        "# Training Data Loader\n",
        "class LidarPairDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for LiDAR scan pairs with contrastive labels\"\"\"\n",
        "    def __init__(self, pairs, scans):\n",
        "        self.pairs = pairs\n",
        "        self.scans = scans\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx1, idx2, label = self.pairs[idx]\n",
        "\n",
        "        scan1 = torch.tensor(self.scans[idx1], dtype=torch.float32).unsqueeze(0)  # (1, 360)\n",
        "        scan2 = torch.tensor(self.scans[idx2], dtype=torch.float32).unsqueeze(0)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return scan1, scan2, label\n",
        "\n",
        "# Load all LiDAR scans into memory\n",
        "print(\"\\nLoading all LiDAR scans into memory...\")\n",
        "all_scans = []\n",
        "for _, row in tqdm(lidar_metadata.iterrows(), total=len(lidar_metadata), desc=\"Loading scans\"):\n",
        "    scan_path = os.path.join(LIDAR_INPUT_DIR, row['filename'])\n",
        "    scan = pd.read_csv(scan_path, header=None).values[0]\n",
        "    all_scans.append(scan)\n",
        "all_scans = np.array(all_scans)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(all_scans)} scans into memory\")\n",
        "print(f\"  Memory usage: ~{all_scans.nbytes / (1024**2):.2f} MB\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LidarPairDataset(train_pairs, all_scans)\n",
        "val_dataset = LidarPairDataset(val_pairs, all_scans)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0  # Set to 0 for compatibility\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Data loaders created\")\n",
        "print(f\"  Training batches: {len(train_loader)}\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")\n",
        "print(f\"  Total training pairs: {len(train_dataset)}\")\n",
        "print(f\"  Total validation pairs: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop_cell"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STARTING CONTRASTIVE TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Re-initialize the network (start fresh)\n",
        "print(\"\\nInitializing fresh network for training...\")\n",
        "geometric_cnn = GeometricCNN(input_dim=360, output_dim=256).to(device)\n",
        "optimizer = torch.optim.Adam(geometric_cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"âœ“ Network initialized on {device}\")\n",
        "print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in geometric_cnn.parameters()):,}\")\n",
        "\n",
        "print(f\"\\nğŸš€ Training for {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # ========== Training Phase ==========\n",
        "    geometric_cnn.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for scan1, scan2, labels in train_loader:\n",
        "        scan1 = scan1.to(device)\n",
        "        scan2 = scan2.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        features1 = geometric_cnn(scan1)\n",
        "        features2 = geometric_cnn(scan2)\n",
        "\n",
        "        # L2 normalize\n",
        "        features1 = F.normalize(features1, p=2, dim=1)\n",
        "        features2 = F.normalize(features2, p=2, dim=1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(features1, features2, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # ========== Validation Phase ==========\n",
        "    geometric_cnn.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for scan1, scan2, labels in val_loader:\n",
        "            scan1 = scan1.to(device)\n",
        "            scan2 = scan2.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            features1 = geometric_cnn(scan1)\n",
        "            features2 = geometric_cnn(scan2)\n",
        "\n",
        "            features1 = F.normalize(features1, p=2, dim=1)\n",
        "            features2 = F.normalize(features2, p=2, dim=1)\n",
        "\n",
        "            loss = criterion(features1, features2, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Track best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch + 1\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch+1:2d}/{NUM_EPOCHS}] \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nâœ“ Training Complete!\")\n",
        "print(f\"  Best validation loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
        "print(f\"  Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"  Final validation loss: {val_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_training_cell"
      },
      "outputs": [],
      "source": [
        "# Visualize Training Progress\n",
        "print(\"\\nGenerating training visualization...\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "epochs_range = range(1, NUM_EPOCHS + 1)\n",
        "ax.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
        "ax.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
        "\n",
        "# Mark best epoch\n",
        "ax.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
        "\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Contrastive Loss', fontsize=12)\n",
        "ax.set_title('Training Progress: Contrastive Learning for LiDAR Features', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(alpha=0.3)\n",
        "ax.set_xlim(1, NUM_EPOCHS)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ“ Training curve saved as 'training_progress.png'\")\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nğŸ“Š Training Summary:\")\n",
        "print(f\"  Initial train loss: {train_losses[0]:.4f}\")\n",
        "print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"  Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n",
        "print(f\"\\n  Initial val loss: {val_losses[0]:.4f}\")\n",
        "print(f\"  Final val loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  Improvement: {((val_losses[0] - val_losses[-1]) / val_losses[0] * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reextract_features_cell"
      },
      "outputs": [],
      "source": [
        "# Re-extract Features with Trained Network\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RE-EXTRACTING LIDAR FEATURES WITH TRAINED NETWORK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lidar_features_list = []\n",
        "lidar_timestamps_list = []\n",
        "lidar_filenames_list = []\n",
        "\n",
        "geometric_cnn.eval()\n",
        "with torch.no_grad():\n",
        "    for idx in tqdm(range(0, len(lidar_metadata), LIDAR_BATCH_SIZE), desc=\"ğŸ¯ LiDAR batches (trained)\"):\n",
        "        batch_meta = lidar_metadata.iloc[idx:idx+LIDAR_BATCH_SIZE]\n",
        "\n",
        "        batch_scans = []\n",
        "        batch_timestamps = []\n",
        "        batch_filenames = []\n",
        "\n",
        "        for _, row in batch_meta.iterrows():\n",
        "            scan_path = os.path.join(LIDAR_INPUT_DIR, row['filename'])\n",
        "\n",
        "            try:\n",
        "                scan = pd.read_csv(scan_path, header=None).values[0]\n",
        "                batch_scans.append(torch.tensor(scan, dtype=torch.float32))\n",
        "                batch_timestamps.append(row['timestamp'])\n",
        "                batch_filenames.append(row['filename'])\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Error loading {scan_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(batch_scans) == 0:\n",
        "            continue\n",
        "\n",
        "        # Stack into batch tensor (batch, 1, 360)\n",
        "        batch_tensor = torch.stack(batch_scans).unsqueeze(1).to(device)\n",
        "\n",
        "        # Extract features\n",
        "        features = geometric_cnn(batch_tensor)\n",
        "\n",
        "        # L2 normalize features\n",
        "        features = F.normalize(features, p=2, dim=1)\n",
        "\n",
        "        # Move to CPU and store\n",
        "        lidar_features_list.append(features.cpu().numpy())\n",
        "        lidar_timestamps_list.extend(batch_timestamps)\n",
        "        lidar_filenames_list.extend(batch_filenames)\n",
        "\n",
        "# Concatenate all batches\n",
        "lidar_features = np.vstack(lidar_features_list)\n",
        "lidar_timestamps = np.array(lidar_timestamps_list)\n",
        "lidar_filenames = np.array(lidar_filenames_list)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nâœ“ LiDAR feature re-extraction complete!\")\n",
        "print(f\"  Shape: {lidar_features.shape}\")\n",
        "print(f\"  Feature dimension: {lidar_features.shape[1]}D\")\n",
        "print(f\"  Number of scans: {lidar_features.shape[0]}\")\n",
        "print(f\"\\nğŸ‰ Network is now TRAINED (not random anymore!)\")\n",
        "print(f\"   Features are meaningful for loop closure detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_trained_features_cell"
      },
      "outputs": [],
      "source": [
        "# Verify Trained Features\n",
        "print(\"\\nğŸ“Š Trained LiDAR Feature Statistics:\")\n",
        "print(f\"  Shape: {lidar_features.shape}\")\n",
        "print(f\"  Mean: {lidar_features.mean():.4f}\")\n",
        "print(f\"  Std: {lidar_features.std():.4f}\")\n",
        "print(f\"  Min: {lidar_features.min():.4f}\")\n",
        "print(f\"  Max: {lidar_features.max():.4f}\")\n",
        "\n",
        "# Check L2 normalization\n",
        "norms = np.linalg.norm(lidar_features, axis=1)\n",
        "print(f\"  L2 norms: mean={norms.mean():.4f}, std={norms.std():.6f}\")\n",
        "print(f\"  âœ“ Features are L2 normalized (norms â‰ˆ 1.0)\")\n",
        "\n",
        "print(f\"\\nâœ“ Network is trained and ready for loop closure detection!\")\n",
        "print(f\"  You can now use these features for place recognition\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EixCyqtng-ko"
      },
      "source": [
        "## Section 3: Save Features to HDF5\n",
        "\n",
        "### Purpose\n",
        "Store extracted features in HDF5 format for efficient access and sensor fusion. HDF5 provides fast random access, compression, and hierarchical organization.\n",
        "\n",
        "### Expected Inputs (From Previous Sections)\n",
        "- `camera_features`: Numpy array [N_cam, 1280]\n",
        "- `camera_timestamps`: Numpy array [N_cam]\n",
        "- `camera_filenames`: Numpy array [N_cam]\n",
        "- `lidar_features`: Numpy array [N_lid, 256] **(TRAINED)**\n",
        "- `lidar_timestamps`: Numpy array [N_lid]\n",
        "- `lidar_filenames`: Numpy array [N_lid]\n",
        "\n",
        "### Output Structure\n",
        "```\n",
        "features.h5\n",
        "â”œâ”€â”€ camera/\n",
        "â”‚   â”œâ”€â”€ features [N_cam, 1280]     # Camera feature vectors\n",
        "â”‚   â”œâ”€â”€ timestamps [N_cam]         # ROS timestamps (float)\n",
        "â”‚   â””â”€â”€ filenames [N_cam]          # Source image filenames\n",
        "â”œâ”€â”€ lidar/\n",
        "â”‚   â”œâ”€â”€ features [N_lid, 256]      # LiDAR feature vectors (TRAINED)\n",
        "â”‚   â”œâ”€â”€ timestamps [N_lid]         # ROS timestamps (float)\n",
        "â”‚   â””â”€â”€ filenames [N_lid]          # Source scan filenames\n",
        "â””â”€â”€ metadata (attributes)\n",
        "    â”œâ”€â”€ creation_date\n",
        "    â”œâ”€â”€ camera_model\n",
        "    â”œâ”€â”€ lidar_model\n",
        "    â”œâ”€â”€ camera_feature_dim\n",
        "    â”œâ”€â”€ lidar_feature_dim\n",
        "    â”œâ”€â”€ training_status\n",
        "    â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "### What This Section Does\n",
        "1. Create HDF5 file with hierarchical structure\n",
        "2. Save camera features (1280D Ã— N_cam)\n",
        "3. Save LiDAR features (256D Ã— N_lid) **WITH TRAINING INFO**\n",
        "4. Store timestamps for temporal alignment\n",
        "5. Store filenames for traceability\n",
        "6. Add comprehensive metadata as attributes\n",
        "7. Generate summary JSON file\n",
        "\n",
        "### Usage Example (Next Stage)\n",
        "```python\n",
        "import h5py\n",
        "\n",
        "# Load features\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    cam_features = f['camera/features'][:]  # (N, 1280)\n",
        "    cam_timestamps = f['camera/timestamps'][:]\n",
        "    \n",
        "    lid_features = f['lidar/features'][:]   # (N, 256) - TRAINED!\n",
        "    lid_timestamps = f['lidar/timestamps'][:]\n",
        "    \n",
        "    # Access metadata\n",
        "    camera_dim = f['camera'].attrs['feature_dim']\n",
        "    training_status = f['lidar'].attrs['training_status']\n",
        "```\n",
        "\n",
        "### Important Notes\n",
        "- **No temporal alignment yet**: Camera and LiDAR features saved separately\n",
        "- **Timestamps preserved**: Use for alignment in fusion stage\n",
        "- **L2 normalized**: Features ready for cosine similarity\n",
        "- **Traceability**: Filenames link back to original data\n",
        "- **Training info**: LiDAR features now include training metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75AvHpsrg-ko"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "OUTPUT_FILE = \"features.h5\"\n",
        "COMPRESSION = \"gzip\"  # Use gzip compression\n",
        "\n",
        "print(f\"Output Configuration:\")\n",
        "print(f\"  Output file: {OUTPUT_FILE}\")\n",
        "print(f\"  Compression: {COMPRESSION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08oc-ZdNg-ko"
      },
      "outputs": [],
      "source": [
        "# Create HDF5 file and save features\n",
        "print(\"\\nCreating HDF5 file...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "with h5py.File(OUTPUT_FILE, 'w') as f:\n",
        "    # ========== Camera Group ==========\n",
        "    print(\"Saving camera features...\")\n",
        "    camera_group = f.create_group('camera')\n",
        "    camera_group.create_dataset('features', data=camera_features, compression=COMPRESSION)\n",
        "    camera_group.create_dataset('timestamps', data=camera_timestamps, compression=COMPRESSION)\n",
        "    camera_group.create_dataset('filenames', data=camera_filenames.astype('S'))\n",
        "\n",
        "    # Add camera metadata\n",
        "    camera_group.attrs['feature_dim'] = camera_features.shape[1]\n",
        "    camera_group.attrs['num_samples'] = camera_features.shape[0]\n",
        "    camera_group.attrs['model'] = 'MobileNet V2'\n",
        "    camera_group.attrs['pretrained'] = 'ImageNet'\n",
        "    camera_group.attrs['normalization'] = 'L2'\n",
        "    camera_group.attrs['input_size'] = '224x224x3'\n",
        "    print(f\"  âœ“ Camera features: {camera_features.shape}\")\n",
        "\n",
        "    # ========== LiDAR Group ==========\n",
        "    print(\"Saving LiDAR features...\")\n",
        "    lidar_group = f.create_group('lidar')\n",
        "    lidar_group.create_dataset('features', data=lidar_features, compression=COMPRESSION)\n",
        "    lidar_group.create_dataset('timestamps', data=lidar_timestamps, compression=COMPRESSION)\n",
        "    lidar_group.create_dataset('filenames', data=lidar_filenames.astype('S'))\n",
        "\n",
        "    # Add lidar metadata (with training info)\n",
        "    lidar_group.attrs['feature_dim'] = lidar_features.shape[1]\n",
        "    lidar_group.attrs['num_samples'] = lidar_features.shape[0]\n",
        "    lidar_group.attrs['model'] = '1D CNN (4 Conv1D + GAP)'\n",
        "    lidar_group.attrs['training_status'] = 'TRAINED (Contrastive Learning)'\n",
        "    lidar_group.attrs['training_method'] = 'Contrastive Loss'\n",
        "    lidar_group.attrs['training_epochs'] = NUM_EPOCHS\n",
        "    lidar_group.attrs['best_val_loss'] = float(best_val_loss)\n",
        "    lidar_group.attrs['training_pairs'] = len(train_pairs)\n",
        "    lidar_group.attrs['normalization'] = 'L2'\n",
        "    lidar_group.attrs['input_size'] = '360'\n",
        "    lidar_group.attrs['parameters'] = f'{total_params:,}'\n",
        "    print(f\"  âœ“ LiDAR features: {lidar_features.shape} (TRAINED)\")\n",
        "\n",
        "    # ========== Global Metadata ==========\n",
        "    f.attrs['creation_date'] = datetime.now().isoformat()\n",
        "    f.attrs['session'] = session\n",
        "    f.attrs['camera_input_dir'] = CAMERA_INPUT_DIR\n",
        "    f.attrs['lidar_input_dir'] = LIDAR_INPUT_DIR\n",
        "    f.attrs['device'] = str(device)\n",
        "    f.attrs['camera_batch_size'] = CAMERA_BATCH_SIZE\n",
        "    f.attrs['lidar_batch_size'] = LIDAR_BATCH_SIZE\n",
        "    f.attrs['temporal_alignment'] = 'Not performed - features extracted independently'\n",
        "    f.attrs['lidar_training_status'] = 'TRAINED'\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nâœ“ Features saved to {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3U_DHibg-ko"
      },
      "outputs": [],
      "source": [
        "# Verify HDF5 file\n",
        "print(\"\\nVerifying HDF5 file...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "with h5py.File(OUTPUT_FILE, 'r') as f:\n",
        "    print(f\"\\nGroups: {list(f.keys())}\")\n",
        "\n",
        "    print(f\"\\nğŸ“· Camera:\")\n",
        "    print(f\"  features: {f['camera/features'].shape}\")\n",
        "    print(f\"  timestamps: {f['camera/timestamps'].shape}\")\n",
        "    print(f\"  filenames: {f['camera/filenames'].shape}\")\n",
        "    print(f\"  feature_dim: {f['camera'].attrs['feature_dim']}\")\n",
        "    print(f\"  model: {f['camera'].attrs['model']}\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ LiDAR:\")\n",
        "    print(f\"  features: {f['lidar/features'].shape}\")\n",
        "    print(f\"  timestamps: {f['lidar/timestamps'].shape}\")\n",
        "    print(f\"  filenames: {f['lidar/filenames'].shape}\")\n",
        "    print(f\"  feature_dim: {f['lidar'].attrs['feature_dim']}\")\n",
        "    print(f\"  model: {f['lidar'].attrs['model']}\")\n",
        "    print(f\"  training_status: {f['lidar'].attrs['training_status']}\")\n",
        "    print(f\"  best_val_loss: {f['lidar'].attrs['best_val_loss']:.4f}\")\n",
        "\n",
        "    print(f\"\\nğŸŒ Global Attributes:\")\n",
        "    print(f\"  creation_date: {f.attrs['creation_date']}\")\n",
        "    print(f\"  session: {f.attrs['session']}\")\n",
        "    print(f\"  device: {f.attrs['device']}\")\n",
        "    print(f\"  lidar_training_status: {f.attrs['lidar_training_status']}\")\n",
        "\n",
        "# Get file size\n",
        "file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
        "print(f\"\\nFile size: {file_size_mb:.2f} MB\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6xOVau8g-ko"
      },
      "outputs": [],
      "source": [
        "# Create summary JSON\n",
        "summary = {\n",
        "    \"feature_extraction_summary\": {\n",
        "        \"creation_date\": datetime.now().isoformat(),\n",
        "        \"session\": session,\n",
        "        \"output_file\": OUTPUT_FILE,\n",
        "        \"file_size_mb\": round(file_size_mb, 2)\n",
        "    },\n",
        "    \"camera\": {\n",
        "        \"model\": \"MobileNet V2\",\n",
        "        \"pretrained\": \"ImageNet\",\n",
        "        \"feature_dim\": int(camera_features.shape[1]),\n",
        "        \"num_samples\": int(camera_features.shape[0]),\n",
        "        \"input_size\": \"224x224x3\",\n",
        "        \"preprocessing\": \"ImageNet normalization\",\n",
        "        \"normalization\": \"L2\",\n",
        "        \"batch_size\": CAMERA_BATCH_SIZE\n",
        "    },\n",
        "    \"lidar\": {\n",
        "        \"model\": \"1D CNN (4 Conv1D + GAP)\",\n",
        "        \"training_status\": \"TRAINED\",\n",
        "        \"training_method\": \"Contrastive Learning\",\n",
        "        \"training_epochs\": NUM_EPOCHS,\n",
        "        \"best_val_loss\": float(best_val_loss),\n",
        "        \"final_train_loss\": float(train_losses[-1]),\n",
        "        \"final_val_loss\": float(val_losses[-1]),\n",
        "        \"training_pairs\": len(train_pairs),\n",
        "        \"validation_pairs\": len(val_pairs),\n",
        "        \"feature_dim\": int(lidar_features.shape[1]),\n",
        "        \"num_samples\": int(lidar_features.shape[0]),\n",
        "        \"parameters\": total_params,\n",
        "        \"input_size\": \"360\",\n",
        "        \"preprocessing\": \"Normalized to [0,1]\",\n",
        "        \"normalization\": \"L2\",\n",
        "        \"batch_size\": LIDAR_BATCH_SIZE\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"similar_threshold_sec\": TEMPORAL_THRESHOLD_SIMILAR,\n",
        "        \"different_threshold_sec\": TEMPORAL_THRESHOLD_DIFFERENT,\n",
        "        \"margin\": MARGIN,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"device\": str(device)\n",
        "    },\n",
        "    \"notes\": {\n",
        "        \"temporal_alignment\": \"Not performed - features extracted independently\",\n",
        "        \"lidar_features\": \"TRAINED using contrastive learning - ready for production\",\n",
        "        \"next_steps\": [\n",
        "            \"Temporal alignment using timestamps\",\n",
        "            \"Sensor fusion (concatenate or attention-based)\",\n",
        "            \"Loop closure detection evaluation\",\n",
        "            \"Integration with SLAM system\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "summary_file = \"feature_extraction_summary.json\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Summary saved to {summary_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKcFNkRfg-ko"
      },
      "outputs": [],
      "source": [
        "# Display final summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FEATURE EXTRACTION COMPLETE âœ“\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nğŸ“¦ Output Files:\")\n",
        "print(f\"  {OUTPUT_FILE} ({file_size_mb:.2f} MB)\")\n",
        "print(f\"  {summary_file}\")\n",
        "print(f\"  training_progress.png\")\n",
        "\n",
        "print(f\"\\nğŸ“· Camera Features:\")\n",
        "print(f\"  Model: MobileNet V2 (pretrained on ImageNet)\")\n",
        "print(f\"  Dimension: {camera_features.shape[1]}D\")\n",
        "print(f\"  Samples: {camera_features.shape[0]:,}\")\n",
        "print(f\"  Normalization: L2\")\n",
        "\n",
        "print(f\"\\nğŸ¯ LiDAR Features:\")\n",
        "print(f\"  Model: 1D CNN ({total_params:,} parameters)\")\n",
        "print(f\"  Training: CONTRASTIVE LEARNING âœ“\")\n",
        "print(f\"  Dimension: {lidar_features.shape[1]}D\")\n",
        "print(f\"  Samples: {lidar_features.shape[0]:,}\")\n",
        "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"  Normalization: L2\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Training Statistics:\")\n",
        "print(f\"  Training pairs: {len(train_pairs):,}\")\n",
        "print(f\"  Validation pairs: {len(val_pairs):,}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Device: {device}\")\n",
        "\n",
        "print(f\"\\nğŸš€ Next Steps:\")\n",
        "print(f\"  1. Temporal alignment using timestamps\")\n",
        "print(f\"  2. Sensor fusion pipeline\")\n",
        "print(f\"  3. Loop closure detection\")\n",
        "print(f\"  4. Integration with SLAM system\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ‰ SUCCESS! Features are ready for loop closure detection\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdTEYpe6g-kp"
      },
      "source": [
        "## Section 4: Visualization & Analysis (Optional)\n",
        "\n",
        "### Purpose\n",
        "Visualize feature distributions and temporal coverage to verify data quality.\n",
        "\n",
        "### What This Section Shows\n",
        "1. Feature value distributions (camera vs LiDAR)\n",
        "2. L2 norm distributions (should be ~1.0)\n",
        "3. Temporal distribution of features\n",
        "4. Data quality metrics\n",
        "\n",
        "### Output Files\n",
        "- `feature_distributions.png` - Feature statistics visualization\n",
        "- `temporal_distribution.png` - Timeline visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3q_1zeJg-ko"
      },
      "outputs": [],
      "source": [
        "# Visualize feature distributions\n",
        "print(\"\\nGenerating feature distribution plots...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Camera feature distribution\n",
        "axes[0, 0].hist(camera_features.flatten(), bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
        "axes[0, 0].set_xlabel('Feature Value', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
        "axes[0, 0].set_title('Camera Feature Distribution (1280D Ã— N)\\nMobileNet V2 (Pretrained)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# LiDAR feature distribution\n",
        "axes[0, 1].hist(lidar_features.flatten(), bins=50, alpha=0.7, edgecolor='black', color='coral')\n",
        "axes[0, 1].set_xlabel('Feature Value', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
        "axes[0, 1].set_title('LiDAR Feature Distribution (256D Ã— N)\\n1D CNN (TRAINED)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Camera L2 norms\n",
        "cam_norms = np.linalg.norm(camera_features, axis=1)\n",
        "axes[1, 0].hist(cam_norms, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n",
        "axes[1, 0].axvline(1.0, color='r', linestyle='--', linewidth=2, label='Expected (1.0)')\n",
        "axes[1, 0].set_xlabel('L2 Norm', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Count', fontsize=11)\n",
        "axes[1, 0].set_title('Camera Feature L2 Norms', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# LiDAR L2 norms\n",
        "lid_norms = np.linalg.norm(lidar_features, axis=1)\n",
        "axes[1, 1].hist(lid_norms, bins=30, alpha=0.7, edgecolor='black', color='coral')\n",
        "axes[1, 1].axvline(1.0, color='r', linestyle='--', linewidth=2, label='Expected (1.0)')\n",
        "axes[1, 1].set_xlabel('L2 Norm', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Count', fontsize=11)\n",
        "axes[1, 1].set_title('LiDAR Feature L2 Norms (Trained)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Feature distributions saved as 'feature_distributions.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnHZ3OOgg-kp"
      },
      "outputs": [],
      "source": [
        "# Analyze temporal coverage\n",
        "print(\"\\nâ± Temporal Coverage Analysis:\")\n",
        "print(\"\\nCamera:\")\n",
        "print(f\"  Time range: {camera_timestamps.min():.2f} - {camera_timestamps.max():.2f} sec\")\n",
        "print(f\"  Duration: {camera_timestamps.max() - camera_timestamps.min():.2f} sec\")\n",
        "print(f\"  Average interval: {np.mean(np.diff(np.sort(camera_timestamps))):.4f} sec\")\n",
        "print(f\"  Samples: {len(camera_timestamps)}\")\n",
        "\n",
        "print(f\"\\nLiDAR:\")\n",
        "print(f\"  Time range: {lidar_timestamps.min():.2f} - {lidar_timestamps.max():.2f} sec\")\n",
        "print(f\"  Duration: {lidar_timestamps.max() - lidar_timestamps.min():.2f} sec\")\n",
        "print(f\"  Average interval: {np.mean(np.diff(np.sort(lidar_timestamps))):.4f} sec\")\n",
        "print(f\"  Samples: {len(lidar_timestamps)}\")\n",
        "\n",
        "# Plot timeline\n",
        "print(\"\\nGenerating temporal distribution plot...\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 4))\n",
        "ax.plot(camera_timestamps, np.ones_like(camera_timestamps), '|',\n",
        "        markersize=10, color='steelblue', label='Camera', alpha=0.7)\n",
        "ax.plot(lidar_timestamps, np.ones_like(lidar_timestamps) * 1.1, '|',\n",
        "        markersize=10, color='coral', label='LiDAR (Trained)', alpha=0.7)\n",
        "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
        "ax.set_yticks([])\n",
        "ax.set_title('Temporal Distribution of Features', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('temporal_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Temporal distribution saved as 'temporal_distribution.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "output_documentation_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 5: Output Documentation ğŸ“š\n",
        "\n",
        "### Overview\n",
        "This section provides comprehensive documentation of all output files, their structure, and how to use them in downstream tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Output Files Summary\n",
        "\n",
        "After running this notebook, you will have the following files:\n",
        "\n",
        "| File | Size | Purpose |\n",
        "|------|------|--------|\n",
        "| `features.h5` | ~2-10 MB | **Main output** - Feature database in HDF5 format |\n",
        "| `feature_extraction_summary.json` | ~2 KB | Metadata and configuration summary |\n",
        "| `training_progress.png` | ~100 KB | Training loss curves |\n",
        "| `feature_distributions.png` | ~200 KB | Feature statistics visualization |\n",
        "| `temporal_distribution.png` | ~100 KB | Timeline visualization |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š HDF5 File Structure (`features.h5`)\n",
        "\n",
        "### Complete Hierarchy\n",
        "```\n",
        "features.h5\n",
        "â”œâ”€â”€ camera/                          (Group)\n",
        "â”‚   â”œâ”€â”€ features                     (Dataset: float32, shape: [N_cam, 1280])\n",
        "â”‚   â”œâ”€â”€ timestamps                   (Dataset: float64, shape: [N_cam])\n",
        "â”‚   â”œâ”€â”€ filenames                    (Dataset: string, shape: [N_cam])\n",
        "â”‚   â””â”€â”€ attributes:\n",
        "â”‚       â”œâ”€â”€ feature_dim = 1280\n",
        "â”‚       â”œâ”€â”€ num_samples = N_cam\n",
        "â”‚       â”œâ”€â”€ model = \"MobileNet V2\"\n",
        "â”‚       â”œâ”€â”€ pretrained = \"ImageNet\"\n",
        "â”‚       â”œâ”€â”€ normalization = \"L2\"\n",
        "â”‚       â””â”€â”€ input_size = \"224x224x3\"\n",
        "â”‚\n",
        "â”œâ”€â”€ lidar/                           (Group)\n",
        "â”‚   â”œâ”€â”€ features                     (Dataset: float32, shape: [N_lid, 256])\n",
        "â”‚   â”œâ”€â”€ timestamps                   (Dataset: float64, shape: [N_lid])\n",
        "â”‚   â”œâ”€â”€ filenames                    (Dataset: string, shape: [N_lid])\n",
        "â”‚   â””â”€â”€ attributes:\n",
        "â”‚       â”œâ”€â”€ feature_dim = 256\n",
        "â”‚       â”œâ”€â”€ num_samples = N_lid\n",
        "â”‚       â”œâ”€â”€ model = \"1D CNN (4 Conv1D + GAP)\"\n",
        "â”‚       â”œâ”€â”€ training_status = \"TRAINED (Contrastive Learning)\"\n",
        "â”‚       â”œâ”€â”€ training_method = \"Contrastive Loss\"\n",
        "â”‚       â”œâ”€â”€ training_epochs = 20\n",
        "â”‚       â”œâ”€â”€ best_val_loss = float (e.g., 0.1234)\n",
        "â”‚       â”œâ”€â”€ training_pairs = int (e.g., 400)\n",
        "â”‚       â”œâ”€â”€ normalization = \"L2\"\n",
        "â”‚       â”œâ”€â”€ input_size = \"360\"\n",
        "â”‚       â””â”€â”€ parameters = \"~350,000\"\n",
        "â”‚\n",
        "â””â”€â”€ attributes (global):\n",
        "    â”œâ”€â”€ creation_date = \"2025-01-15T10:30:45.123456\"\n",
        "    â”œâ”€â”€ session = \"20251016_133216\"\n",
        "    â”œâ”€â”€ camera_input_dir = \"processed_images\"\n",
        "    â”œâ”€â”€ lidar_input_dir = \"processed_lidar\"\n",
        "    â”œâ”€â”€ device = \"cuda\" or \"cpu\"\n",
        "    â”œâ”€â”€ camera_batch_size = 32\n",
        "    â”œâ”€â”€ lidar_batch_size = 64\n",
        "    â”œâ”€â”€ temporal_alignment = \"Not performed - features extracted independently\"\n",
        "    â””â”€â”€ lidar_training_status = \"TRAINED\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Detailed Dataset Descriptions\n",
        "\n",
        "### Camera Group (`/camera/`)\n",
        "\n",
        "#### `features` Dataset\n",
        "- **Type**: `float32`\n",
        "- **Shape**: `[N_cam, 1280]` where N_cam = number of camera images\n",
        "- **Description**: L2-normalized feature vectors extracted from MobileNet V2\n",
        "- **Value Range**: Approximately [-0.5, 0.5] (after L2 normalization)\n",
        "- **Norm**: Each row has L2 norm = 1.0 (Â±1e-6)\n",
        "- **Compression**: gzip\n",
        "- **Example**:\n",
        "  ```python\n",
        "  features[0]  # First image feature: array of 1280 floats\n",
        "  # Shape: (1280,)\n",
        "  # Norm: 1.0\n",
        "  ```\n",
        "\n",
        "#### `timestamps` Dataset\n",
        "- **Type**: `float64`\n",
        "- **Shape**: `[N_cam]`\n",
        "- **Description**: ROS timestamp for each image (seconds.nanoseconds)\n",
        "- **Format**: Unix timestamp with nanosecond precision\n",
        "- **Example**: `1760649872.670898199` (October 16, 2025, 13:32:16.670898199)\n",
        "- **Usage**: For temporal alignment with LiDAR data\n",
        "- **Sorted**: Not guaranteed - use `np.argsort()` if needed\n",
        "\n",
        "#### `filenames` Dataset\n",
        "- **Type**: `string` (fixed-length, max 50 chars)\n",
        "- **Shape**: `[N_cam]`\n",
        "- **Description**: Original image filename\n",
        "- **Format**: `img_00000.jpg`, `img_00001.jpg`, etc.\n",
        "- **Usage**: Traceability back to original data\n",
        "- **Example**: `b'img_00042.jpg'` (stored as bytes, decode with `.decode()`)\n",
        "\n",
        "---\n",
        "\n",
        "### LiDAR Group (`/lidar/`)\n",
        "\n",
        "#### `features` Dataset â­ **TRAINED**\n",
        "- **Type**: `float32`\n",
        "- **Shape**: `[N_lid, 256]` where N_lid = number of LiDAR scans\n",
        "- **Description**: L2-normalized feature vectors from trained 1D CNN\n",
        "- **Value Range**: Approximately [-0.5, 0.5] (after L2 normalization)\n",
        "- **Norm**: Each row has L2 norm = 1.0 (Â±1e-6)\n",
        "- **Training**: Contrastive learning on temporal pairs\n",
        "- **Compression**: gzip\n",
        "- **Example**:\n",
        "  ```python\n",
        "  features[0]  # First scan feature: array of 256 floats\n",
        "  # Shape: (256,)\n",
        "  # Norm: 1.0\n",
        "  ```\n",
        "\n",
        "#### `timestamps` Dataset\n",
        "- **Type**: `float64`\n",
        "- **Shape**: `[N_lid]`\n",
        "- **Description**: ROS timestamp for each scan (seconds.nanoseconds)\n",
        "- **Format**: Unix timestamp with nanosecond precision\n",
        "- **Example**: `1760649872.670898199`\n",
        "- **Usage**: For temporal alignment with camera data\n",
        "- **Sorted**: Not guaranteed - use `np.argsort()` if needed\n",
        "\n",
        "#### `filenames` Dataset\n",
        "- **Type**: `string` (fixed-length, max 50 chars)\n",
        "- **Shape**: `[N_lid]`\n",
        "- **Description**: Original scan filename\n",
        "- **Format**: `scan_00000.csv`, `scan_00001.csv`, etc.\n",
        "- **Usage**: Traceability back to original data\n",
        "- **Example**: `b'scan_00042.csv'` (stored as bytes)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“„ JSON Summary Structure (`feature_extraction_summary.json`)\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"feature_extraction_summary\": {\n",
        "    \"creation_date\": \"2025-01-15T10:30:45.123456\",\n",
        "    \"session\": \"20251016_133216\",\n",
        "    \"output_file\": \"features.h5\",\n",
        "    \"file_size_mb\": 5.23\n",
        "  },\n",
        "  \n",
        "  \"camera\": {\n",
        "    \"model\": \"MobileNet V2\",\n",
        "    \"pretrained\": \"ImageNet\",\n",
        "    \"feature_dim\": 1280,\n",
        "    \"num_samples\": 100,\n",
        "    \"input_size\": \"224x224x3\",\n",
        "    \"preprocessing\": \"ImageNet normalization\",\n",
        "    \"normalization\": \"L2\",\n",
        "    \"batch_size\": 32\n",
        "  },\n",
        "  \n",
        "  \"lidar\": {\n",
        "    \"model\": \"1D CNN (4 Conv1D + GAP)\",\n",
        "    \"training_status\": \"TRAINED\",\n",
        "    \"training_method\": \"Contrastive Learning\",\n",
        "    \"training_epochs\": 20,\n",
        "    \"best_val_loss\": 0.1234,\n",
        "    \"final_train_loss\": 0.0987,\n",
        "    \"final_val_loss\": 0.1256,\n",
        "    \"training_pairs\": 450,\n",
        "    \"validation_pairs\": 112,\n",
        "    \"feature_dim\": 256,\n",
        "    \"num_samples\": 100,\n",
        "    \"parameters\": 350000,\n",
        "    \"input_size\": \"360\",\n",
        "    \"preprocessing\": \"Normalized to [0,1]\",\n",
        "    \"normalization\": \"L2\",\n",
        "    \"batch_size\": 64\n",
        "  },\n",
        "  \n",
        "  \"training_config\": {\n",
        "    \"similar_threshold_sec\": 5.0,\n",
        "    \"different_threshold_sec\": 30.0,\n",
        "    \"margin\": 1.0,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"batch_size\": 16,\n",
        "    \"device\": \"cuda\"\n",
        "  },\n",
        "  \n",
        "  \"notes\": {\n",
        "    \"temporal_alignment\": \"Not performed - features extracted independently\",\n",
        "    \"lidar_features\": \"TRAINED using contrastive learning - ready for production\",\n",
        "    \"next_steps\": [\n",
        "      \"Temporal alignment using timestamps\",\n",
        "      \"Sensor fusion (concatenate or attention-based)\",\n",
        "      \"Loop closure detection evaluation\",\n",
        "      \"Integration with SLAM system\"\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’» Usage Examples\n",
        "\n",
        "### Example 1: Load All Features\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Open HDF5 file\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    # Load camera features\n",
        "    cam_features = f['camera/features'][:]      # Shape: (N_cam, 1280)\n",
        "    cam_timestamps = f['camera/timestamps'][:]  # Shape: (N_cam,)\n",
        "    cam_filenames = f['camera/filenames'][:]    # Shape: (N_cam,)\n",
        "    \n",
        "    # Load LiDAR features\n",
        "    lid_features = f['lidar/features'][:]       # Shape: (N_lid, 256)\n",
        "    lid_timestamps = f['lidar/timestamps'][:]   # Shape: (N_lid,)\n",
        "    lid_filenames = f['lidar/filenames'][:]     # Shape: (N_lid,)\n",
        "    \n",
        "    # Access metadata\n",
        "    cam_dim = f['camera'].attrs['feature_dim']  # 1280\n",
        "    lid_trained = f['lidar'].attrs['training_status']  # \"TRAINED\"\n",
        "\n",
        "print(f\"Loaded {len(cam_features)} camera features\")\n",
        "print(f\"Loaded {len(lid_features)} LiDAR features (TRAINED)\")\n",
        "```\n",
        "\n",
        "### Example 2: Temporal Alignment\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def find_closest_lidar(cam_timestamp, lid_timestamps, max_diff=0.1):\n",
        "    \"\"\"Find closest LiDAR scan to camera timestamp\"\"\"\n",
        "    diffs = np.abs(lid_timestamps - cam_timestamp)\n",
        "    idx = np.argmin(diffs)\n",
        "    if diffs[idx] < max_diff:\n",
        "        return idx\n",
        "    return None\n",
        "\n",
        "# Load data\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    cam_features = f['camera/features'][:]\n",
        "    cam_timestamps = f['camera/timestamps'][:]\n",
        "    lid_features = f['lidar/features'][:]\n",
        "    lid_timestamps = f['lidar/timestamps'][:]\n",
        "\n",
        "# Align features\n",
        "aligned_pairs = []\n",
        "for i, cam_t in enumerate(cam_timestamps):\n",
        "    lid_idx = find_closest_lidar(cam_t, lid_timestamps)\n",
        "    if lid_idx is not None:\n",
        "        # Concatenate features\n",
        "        fused = np.concatenate([cam_features[i], lid_features[lid_idx]])\n",
        "        aligned_pairs.append({\n",
        "            'timestamp': cam_t,\n",
        "            'camera_feature': cam_features[i],\n",
        "            'lidar_feature': lid_features[lid_idx],\n",
        "            'fused_feature': fused  # Shape: (1536,) = 1280 + 256\n",
        "        })\n",
        "\n",
        "print(f\"Aligned {len(aligned_pairs)} pairs\")\n",
        "```\n",
        "\n",
        "### Example 3: Compute Similarity Matrix\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load LiDAR features (trained)\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    features = f['lidar/features'][:]  # Shape: (N, 256)\n",
        "    timestamps = f['lidar/timestamps'][:]\n",
        "\n",
        "# Compute pairwise cosine similarity\n",
        "similarity_matrix = cosine_similarity(features)  # Shape: (N, N)\n",
        "\n",
        "# Find loop closures (high similarity, far in time)\n",
        "time_diff_matrix = np.abs(timestamps[:, None] - timestamps[None, :])\n",
        "loop_candidates = (\n",
        "    (similarity_matrix > 0.7) &      # High similarity\n",
        "    (time_diff_matrix > 30.0)        # >30 seconds apart\n",
        ")\n",
        "\n",
        "# Get loop closure pairs\n",
        "i_indices, j_indices = np.where(loop_candidates)\n",
        "print(f\"Found {len(i_indices)} potential loop closures\")\n",
        "\n",
        "# Example: Best match for scan 50\n",
        "query_idx = 50\n",
        "similarities = similarity_matrix[query_idx]\n",
        "# Exclude nearby scans (within 30 seconds)\n",
        "valid_mask = time_diff_matrix[query_idx] > 30.0\n",
        "valid_similarities = similarities.copy()\n",
        "valid_similarities[~valid_mask] = -1\n",
        "best_match_idx = np.argmax(valid_similarities)\n",
        "print(f\"Best match for scan {query_idx}: scan {best_match_idx}\")\n",
        "print(f\"Similarity: {similarities[best_match_idx]:.4f}\")\n",
        "```\n",
        "\n",
        "### Example 4: Load Specific Samples\n",
        "```python\n",
        "import h5py\n",
        "\n",
        "# Load only specific indices (memory efficient)\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    # Load features for scans 10-20\n",
        "    subset = f['lidar/features'][10:20]  # Shape: (10, 256)\n",
        "    \n",
        "    # Load single feature\n",
        "    single = f['camera/features'][42]  # Shape: (1280,)\n",
        "    \n",
        "    # Load with fancy indexing\n",
        "    indices = [0, 5, 10, 15, 20]\n",
        "    selected = f['lidar/features'][indices]  # Shape: (5, 256)\n",
        "```\n",
        "\n",
        "### Example 5: Verify Data Integrity\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def verify_features(filename='features.h5'):\n",
        "    \"\"\"Verify feature file integrity\"\"\"\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        # Check camera features\n",
        "        cam_features = f['camera/features'][:]\n",
        "        cam_norms = np.linalg.norm(cam_features, axis=1)\n",
        "        assert np.allclose(cam_norms, 1.0, atol=1e-5), \"Camera features not L2 normalized!\"\n",
        "        \n",
        "        # Check LiDAR features\n",
        "        lid_features = f['lidar/features'][:]\n",
        "        lid_norms = np.linalg.norm(lid_features, axis=1)\n",
        "        assert np.allclose(lid_norms, 1.0, atol=1e-5), \"LiDAR features not L2 normalized!\"\n",
        "        \n",
        "        # Check dimensions\n",
        "        assert cam_features.shape[1] == 1280, f\"Wrong camera dim: {cam_features.shape[1]}\"\n",
        "        assert lid_features.shape[1] == 256, f\"Wrong LiDAR dim: {lid_features.shape[1]}\"\n",
        "        \n",
        "        # Check training status\n",
        "        status = f['lidar'].attrs['training_status']\n",
        "        assert 'TRAINED' in status, f\"LiDAR not trained: {status}\"\n",
        "        \n",
        "        print(\"âœ“ All verification checks passed!\")\n",
        "        return True\n",
        "\n",
        "verify_features()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Visualization Files\n",
        "\n",
        "### `training_progress.png`\n",
        "**Contents:**\n",
        "- Training loss curve (blue line)\n",
        "- Validation loss curve (red line)\n",
        "- Best epoch marked (green dashed line)\n",
        "- X-axis: Epochs (1-20)\n",
        "- Y-axis: Contrastive Loss\n",
        "\n",
        "**Interpretation:**\n",
        "- Decreasing loss = network learning\n",
        "- Training < Validation = good generalization\n",
        "- Flat curves after epoch 15 = convergence\n",
        "- Validation increasing = possible overfitting (rare with contrastive learning)\n",
        "\n",
        "### `feature_distributions.png`\n",
        "**Contents (4 subplots):**\n",
        "1. **Top-left**: Camera feature value histogram\n",
        "   - Shows distribution of all 1280D feature values\n",
        "   - Should be roughly centered around 0\n",
        "   \n",
        "2. **Top-right**: LiDAR feature value histogram\n",
        "   - Shows distribution of all 256D feature values\n",
        "   - Should be roughly centered around 0\n",
        "   \n",
        "3. **Bottom-left**: Camera L2 norms\n",
        "   - All values should be â‰ˆ 1.0 (red dashed line)\n",
        "   - Tight distribution around 1.0 = good normalization\n",
        "   \n",
        "4. **Bottom-right**: LiDAR L2 norms\n",
        "   - All values should be â‰ˆ 1.0 (red dashed line)\n",
        "   - Tight distribution around 1.0 = good normalization\n",
        "\n",
        "### `temporal_distribution.png`\n",
        "**Contents:**\n",
        "- Blue vertical lines: Camera feature timestamps\n",
        "- Orange vertical lines: LiDAR feature timestamps\n",
        "- X-axis: Time (seconds from start)\n",
        "- Shows temporal coverage and alignment opportunities\n",
        "\n",
        "**Interpretation:**\n",
        "- Dense lines = good temporal coverage\n",
        "- Overlapping blue/orange = alignment possible\n",
        "- Gaps = missing data periods\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ Advanced Usage\n",
        "\n",
        "### Export to NumPy Arrays\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Export to .npy files for faster loading\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    np.save('camera_features.npy', f['camera/features'][:])\n",
        "    np.save('lidar_features.npy', f['lidar/features'][:])\n",
        "    np.save('camera_timestamps.npy', f['camera/timestamps'][:])\n",
        "    np.save('lidar_timestamps.npy', f['lidar/timestamps'][:])\n",
        "\n",
        "# Load (much faster)\n",
        "cam_feat = np.load('camera_features.npy')  # Fast!\n",
        "```\n",
        "\n",
        "### Convert to PyTorch Dataset\n",
        "```python\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, h5_file, modality='lidar'):\n",
        "        self.h5_file = h5_file\n",
        "        self.modality = modality\n",
        "        with h5py.File(h5_file, 'r') as f:\n",
        "            self.length = f[f'{modality}/features'].shape[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.h5_file, 'r') as f:\n",
        "            feature = f[f'{self.modality}/features'][idx]\n",
        "            timestamp = f[f'{self.modality}/timestamps'][idx]\n",
        "        return torch.tensor(feature), timestamp\n",
        "\n",
        "# Usage\n",
        "dataset = FeatureDataset('features.h5', modality='lidar')\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "```\n",
        "\n",
        "### Integration with FAISS (Fast Similarity Search)\n",
        "```python\n",
        "import h5py\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Load features\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    features = f['lidar/features'][:].astype('float32')\n",
        "\n",
        "# Create FAISS index (L2 normalized, use inner product)\n",
        "dimension = features.shape[1]  # 256\n",
        "index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)\n",
        "index.add(features)  # Add all features\n",
        "\n",
        "# Query: Find 5 most similar scans to scan 0\n",
        "query = features[0:1]  # Shape: (1, 256)\n",
        "k = 5\n",
        "similarities, indices = index.search(query, k)\n",
        "print(f\"Most similar scans: {indices[0]}\")\n",
        "print(f\"Similarities: {similarities[0]}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Quality Indicators\n",
        "\n",
        "### Good Features (What to Expect)\n",
        "âœ… **L2 Norms**: All within [0.999, 1.001]  \n",
        "âœ… **Value Distribution**: Roughly centered around 0  \n",
        "âœ… **Training Loss**: Decreasing trend, converges < 0.2  \n",
        "âœ… **Validation Loss**: Similar to training loss (Â±10%)  \n",
        "âœ… **No NaN/Inf**: All values finite  \n",
        "âœ… **Temporal Coverage**: Even distribution across time  \n",
        "\n",
        "### Potential Issues\n",
        "âŒ **L2 Norms â‰  1.0**: Normalization failed  \n",
        "âŒ **Training Loss > 0.5**: Network not learning  \n",
        "âŒ **Val Loss >> Train Loss**: Overfitting (rare)  \n",
        "âŒ **NaN/Inf Values**: Numerical instability  \n",
        "âŒ **Sparse Temporal Coverage**: Insufficient data  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Data Specifications\n",
        "\n",
        "### Camera Features\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Dimension | 1280 |\n",
        "| Data Type | float32 |\n",
        "| Normalization | L2 (norm = 1.0) |\n",
        "| Source Model | MobileNet V2 (pretrained) |\n",
        "| Training | ImageNet |\n",
        "| Memory per feature | 5 KB (1280 Ã— 4 bytes) |\n",
        "| Similarity Metric | Cosine similarity |\n",
        "\n",
        "### LiDAR Features\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Dimension | 256 |\n",
        "| Data Type | float32 |\n",
        "| Normalization | L2 (norm = 1.0) |\n",
        "| Source Model | 1D CNN (4 layers) |\n",
        "| Training | **Contrastive Learning** |\n",
        "| Memory per feature | 1 KB (256 Ã— 4 bytes) |\n",
        "| Similarity Metric | Cosine similarity |\n",
        "\n",
        "### Timestamps\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Data Type | float64 |\n",
        "| Format | Unix timestamp (seconds.nanoseconds) |\n",
        "| Precision | Nanosecond (1e-9 seconds) |\n",
        "| Range | Depends on recording session |\n",
        "| Sorted | No (use np.argsort if needed) |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ Next Steps Guide\n",
        "\n",
        "### 1. Temporal Alignment\n",
        "**Goal**: Match camera and LiDAR features by timestamp  \n",
        "**Method**: Find closest timestamp pairs within threshold (e.g., 0.1s)  \n",
        "**Output**: Aligned feature pairs for sensor fusion  \n",
        "\n",
        "### 2. Sensor Fusion\n",
        "**Options**:\n",
        "- **Early fusion**: Concatenate features [1280 + 256 = 1536D]\n",
        "- **Late fusion**: Separate matching, then combine scores\n",
        "- **Attention fusion**: Learn weights for each modality\n",
        "\n",
        "### 3. Loop Closure Detection\n",
        "**Method**: Compute similarity between current and past features  \n",
        "**Threshold**: Typically 0.7-0.9 for cosine similarity  \n",
        "**Verification**: Geometric verification with RANSAC  \n",
        "\n",
        "### 4. Performance Evaluation\n",
        "**Metrics**:\n",
        "- Precision-Recall curves\n",
        "- F1 score at different thresholds\n",
        "- ROC curves\n",
        "- Average Precision (AP)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_notes_section"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You have successfully:\n",
        "1. âœ… Extracted camera features using pretrained MobileNet V2\n",
        "2. âœ… **TRAINED** a 1D CNN for LiDAR features using contrastive learning\n",
        "3. âœ… Saved features to HDF5 format\n",
        "4. âœ… Generated comprehensive documentation and visualizations\n",
        "\n",
        "### Key Achievements\n",
        "- **Camera**: 1280D features from pretrained MobileNet V2 (ImageNet)\n",
        "- **LiDAR**: 256D features from **TRAINED** 1D CNN (contrastive learning)\n",
        "- **Training**: Self-supervised (no manual labels needed)\n",
        "- **Quality**: L2 normalized, ready for cosine similarity\n",
        "\n",
        "### Next Steps\n",
        "1. **Temporal Alignment**: Match camera and LiDAR features by timestamp\n",
        "2. **Sensor Fusion**: Combine modalities (concatenation, attention, etc.)\n",
        "3. **Loop Closure Detection**: Evaluate on loop closure task\n",
        "4. **SLAM Integration**: Integrate with your SLAM system\n",
        "\n",
        "### Output Files\n",
        "- `features.h5` - Feature database (HDF5)\n",
        "- `feature_extraction_summary.json` - Processing metadata\n",
        "- `training_progress.png` - Training curve\n",
        "- `feature_distributions.png` - Feature statistics\n",
        "- `temporal_distribution.png` - Timeline visualization\n",
        "\n",
        "### Important Notes\n",
        "âœ… LiDAR features are **TRAINED** - ready for production use  \n",
        "âœ… All features are L2 normalized - use cosine similarity  \n",
        "âœ… Timestamps preserved - ready for temporal alignment  \n",
        "âœ… Traceability maintained - filenames link to original data  \n",
        "\n",
        "---\n",
        "\n",
        "**Questions or issues?** Check the validation sections or error messages above.\n",
        "\n",
        "**Good luck with your loop closure detection! ğŸš€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}