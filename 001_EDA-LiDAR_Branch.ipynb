{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgIYXrjmumyz"
      },
      "source": [
        "# Exploratory Data Analysis - LiDAR Branch\n",
        "Extract and analyze LiDAR scan data from ROS bags collected by a Jetson-based robot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284Xh9wFJZtO"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 1. Initialization and Setup</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBQiCupLumy0"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet rosbags opencv-python pillow numpy matplotlib tqdm seaborn pandas scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahHvN_bEumy1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import io\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrvgY5gk-xnC"
      },
      "outputs": [],
      "source": [
        "from rosbags.rosbag1 import Reader\n",
        "from rosbags.typesys import Stores, get_typestore\n",
        "import os\n",
        "from pathlib import Path\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x4Qx0Oeumy2"
      },
      "outputs": [],
      "source": [
        "# Initialize typestore for ROS1 message deserialization\n",
        "typestore = get_typestore(Stores.ROS1_NOETIC)\n",
        "print(\"Typestore initialized for ROS1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGRyCYIPJZtQ"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 2. Storage Mounting</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4A4E6Bwumy3"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5WLGQmlJZtR"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 3. ROS Bag Load</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym09k1qwvnNB"
      },
      "outputs": [],
      "source": [
        "# Specify the path to the sessions folder\n",
        "data_path = \"/content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDG2Pve9vx_k"
      },
      "outputs": [],
      "source": [
        "# Specify the session\n",
        "session = '20251016_133216'\n",
        "print(f\"Using session: {session}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV8nBLXqumy4"
      },
      "outputs": [],
      "source": [
        "working_folder = data_path + 'session_' + session\n",
        "bag_name = 'session_data.bag'\n",
        "\n",
        "# Change to the specified subfolder\n",
        "os.chdir(working_folder)\n",
        "print(f\"Changed directory to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao2LOd4v_hu8"
      },
      "outputs": [],
      "source": [
        "bag_file = working_folder + '/' + bag_name\n",
        "print(f\"Bag file name: {bag_name}\")\n",
        "\n",
        "# Set bag_path for the rest of the notebook\n",
        "bag_path = bag_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QoTWqrumy5"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 4. ROS Bag Basic EDA</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlJ6IqErumy5"
      },
      "source": [
        "### 1. Basic Bag Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzgll0Seumy5"
      },
      "outputs": [],
      "source": [
        "# Open the bag and get basic information\n",
        "print(\"=\" * 60)\n",
        "print(\"BASIC BAG INFORMATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    # Get basic statistics\n",
        "    duration = reader.duration * 1e-9  # Convert to seconds\n",
        "    duration_sec = duration\n",
        "    total_messages = reader.message_count\n",
        "\n",
        "    # Get file size\n",
        "    file_size_bytes = os.path.getsize(bag_path)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "    # Calculate average message rate\n",
        "    avg_message_rate = total_messages / duration if duration > 0 else 0\n",
        "\n",
        "    print(f\"\\nüìÅ File: {bag_name}\")\n",
        "    print(f\"üíæ Size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"‚è±Ô∏è  Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
        "    print(f\"üì¨ Total Messages: {total_messages:,}\")\n",
        "    print(f\"üìä Average Rate: {avg_message_rate:.2f} msg/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1ydJGRHumy6"
      },
      "source": [
        "### 2. Topic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFMEMKgIumy6"
      },
      "outputs": [],
      "source": [
        "# Analyze topics and connections\n",
        "print(\"=\" * 60)\n",
        "print(\"TOPIC ANALYSIS (ROS1 BAG)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import struct\n",
        "\n",
        "def parse_tf_binary(rawdata):\n",
        "    \"\"\"Parse TF message from raw bytes\"\"\"\n",
        "    transforms = set()\n",
        "    for start_offset in [0, 4, 8, 12, 16]:\n",
        "        try:\n",
        "            offset = start_offset\n",
        "            array_len = struct.unpack('<I', rawdata[offset:offset+4])[0]\n",
        "            if array_len > 100 or array_len == 0:\n",
        "                continue\n",
        "            offset += 4\n",
        "            for _ in range(min(array_len, 10)):\n",
        "                if offset + 100 > len(rawdata):\n",
        "                    break\n",
        "                frame_len = struct.unpack('<I', rawdata[offset:offset+4])[0]\n",
        "                if frame_len > 0 and frame_len < 100:\n",
        "                    offset += 4\n",
        "                    parent = rawdata[offset:offset+frame_len].decode('utf-8', errors='ignore').strip('/')\n",
        "                    offset += frame_len\n",
        "                    child_len = struct.unpack('<I', rawdata[offset:offset+4])[0]\n",
        "                    if child_len > 0 and child_len < 100:\n",
        "                        offset += 4\n",
        "                        child = rawdata[offset:offset+child_len].decode('utf-8', errors='ignore').strip('/')\n",
        "                        if parent and child:\n",
        "                            transforms.add(f\"{parent}‚Üí{child}\")\n",
        "                        offset += child_len + 48\n",
        "                else:\n",
        "                    break\n",
        "            if transforms:\n",
        "                return transforms\n",
        "        except:\n",
        "            continue\n",
        "    return transforms\n",
        "\n",
        "# Get basic topic information with connection indices\n",
        "with Reader(bag_path) as reader:\n",
        "    topics_data = []\n",
        "    connection_map = {}  # Store connection index mapping\n",
        "\n",
        "    for conn_idx, connection in enumerate(reader.connections):\n",
        "        topics_data.append({\n",
        "            'Topic': connection.topic,\n",
        "            'Message Type': connection.msgtype,\n",
        "            'Count': connection.msgcount,\n",
        "            'Connection_Index': conn_idx,\n",
        "            'Description': \"\"\n",
        "        })\n",
        "        # Map for later use\n",
        "        connection_map[conn_idx] = {'topic': connection.topic, 'msgtype': connection.msgtype, 'msgcount': connection.msgcount}\n",
        "\n",
        "    df_topics = pd.DataFrame(topics_data)\n",
        "\n",
        "# Analyze TF connections\n",
        "print(\"\\nüîç Analyzing TF connections...\")\n",
        "with Reader(bag_path) as reader:\n",
        "    tf_connection_data = {}\n",
        "\n",
        "    for conn_idx, conn in enumerate(reader.connections):\n",
        "        if conn.topic in ['/tf', '/tf_static']:\n",
        "            tf_connection_data[conn_idx] = {'transforms': set(), 'samples': 0}\n",
        "\n",
        "    for connection, timestamp, rawdata in reader.messages():\n",
        "        if connection.topic in ['/tf', '/tf_static']:\n",
        "            conn_idx = None\n",
        "            for idx, conn in enumerate(reader.connections):\n",
        "                if (conn.topic == connection.topic and\n",
        "                    conn.msgtype == connection.msgtype and\n",
        "                    conn.msgcount == connection.msgcount):\n",
        "                    conn_idx = idx\n",
        "                    break\n",
        "\n",
        "            if conn_idx is not None and conn_idx in tf_connection_data:\n",
        "                if tf_connection_data[conn_idx]['samples'] < 10:\n",
        "                    try:\n",
        "                        msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "                        for tf in msg.transforms:\n",
        "                            parent = tf.header.frame_id.strip('/')\n",
        "                            child = tf.child_frame_id.strip('/')\n",
        "                            tf_connection_data[conn_idx]['transforms'].add(f\"{parent}‚Üí{child}\")\n",
        "                    except:\n",
        "                        binary_transforms = parse_tf_binary(rawdata)\n",
        "                        if binary_transforms:\n",
        "                            tf_connection_data[conn_idx]['transforms'].update(binary_transforms)\n",
        "                    tf_connection_data[conn_idx]['samples'] += 1\n",
        "\n",
        "# Classify TF connections\n",
        "for idx, row in df_topics.iterrows():\n",
        "    if row['Topic'] in ['/tf', '/tf_static']:\n",
        "        conn_idx = row['Connection_Index']\n",
        "\n",
        "        if conn_idx in tf_connection_data:\n",
        "            transforms = tf_connection_data[conn_idx]['transforms']\n",
        "\n",
        "            if transforms:\n",
        "                transforms_str = ', '.join(sorted(transforms)[:5])\n",
        "                if len(transforms) > 5:\n",
        "                    transforms_str += f\" (+{len(transforms)-5} more)\"\n",
        "\n",
        "                transforms_lower = ' '.join(transforms).lower()\n",
        "\n",
        "                if 'map' in transforms_lower and 'odom' in transforms_lower:\n",
        "                    classification = \"SLAM\"\n",
        "                elif 'odom' in transforms_lower and 'base' in transforms_lower:\n",
        "                    classification = \"Odometry\"\n",
        "                elif 'imu' in transforms_lower or 'laser' in transforms_lower or 'camera' in transforms_lower:\n",
        "                    classification = \"Static\"\n",
        "                else:\n",
        "                    classification = \"Other\"\n",
        "\n",
        "                df_topics.at[idx, 'Description'] = f\"{classification}: {transforms_str}\"\n",
        "            else:\n",
        "                freq = row['Count'] / duration_sec\n",
        "                if freq > 40:\n",
        "                    classification = \"SLAM\"\n",
        "                elif freq > 30:\n",
        "                    classification = \"Odometry\"\n",
        "                elif freq > 15:\n",
        "                    classification = \"Static\"\n",
        "                else:\n",
        "                    classification = \"Other\"\n",
        "                df_topics.at[idx, 'Description'] = f\"{classification} (by frequency)\"\n",
        "\n",
        "# Display\n",
        "df_topics_display = df_topics.drop('Connection_Index', axis=1)\n",
        "df_topics_display = df_topics_display.sort_values('Count', ascending=False)\n",
        "df_topics_display['Frequency (Hz)'] = df_topics_display['Count'] / duration_sec\n",
        "\n",
        "print(f\"\\nüìä Total Connections: {len(df_topics_display)}\")\n",
        "print(f\"üìä Unique Topics: {df_topics_display['Topic'].nunique()}\")\n",
        "print(f\"\\n{df_topics_display.to_string(index=False)}\")\n",
        "\n",
        "df_topics_display.to_csv('topic_statistics.csv', index=False)\n",
        "print(f\"\\n‚úÖ Topic statistics saved to: topic_statistics.csv\")\n",
        "\n",
        "unique_topics = df_topics_display['Topic'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_raw_message"
      },
      "source": [
        "### 3. Sample Raw Message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_raw_message_code"
      },
      "outputs": [],
      "source": [
        "# Print a sample raw message from the LiDAR topic\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE RAW MESSAGE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    # Find the LiDAR topic\n",
        "    lidar_topics = [conn.topic for conn in reader.connections if 'scan' in conn.topic.lower()]\n",
        "    \n",
        "    if lidar_topics:\n",
        "        target_topic = lidar_topics[0]\n",
        "        print(f\"\\nüì° Displaying sample message from: {target_topic}\\n\")\n",
        "        \n",
        "        # Get first message from LiDAR topic\n",
        "        for connection, timestamp, rawdata in reader.messages():\n",
        "            if connection.topic == target_topic:\n",
        "                try:\n",
        "                    # Deserialize the message\n",
        "                    msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "                    \n",
        "                    print(f\"Message Type: {connection.msgtype}\")\n",
        "                    print(f\"Timestamp: {timestamp * 1e-9:.6f} seconds\\n\")\n",
        "                    print(\"Message Structure:\")\n",
        "                    print(\"-\" * 60)\n",
        "                    \n",
        "                    # Print header\n",
        "                    if hasattr(msg, 'header'):\n",
        "                        print(f\"Header:\")\n",
        "                        print(f\"  seq: {msg.header.seq}\")\n",
        "                        print(f\"  stamp: {msg.header.stamp.sec}.{msg.header.stamp.nanosec}\")\n",
        "                        print(f\"  frame_id: '{msg.header.frame_id}'\\n\")\n",
        "                    \n",
        "                    # Print scan parameters\n",
        "                    print(f\"Scan Parameters:\")\n",
        "                    if hasattr(msg, 'angle_min'):\n",
        "                        print(f\"  angle_min: {msg.angle_min:.6f} rad ({np.rad2deg(msg.angle_min):.2f}¬∞)\")\n",
        "                    if hasattr(msg, 'angle_max'):\n",
        "                        print(f\"  angle_max: {msg.angle_max:.6f} rad ({np.rad2deg(msg.angle_max):.2f}¬∞)\")\n",
        "                    if hasattr(msg, 'angle_increment'):\n",
        "                        print(f\"  angle_increment: {msg.angle_increment:.6f} rad ({np.rad2deg(msg.angle_increment):.4f}¬∞)\")\n",
        "                    if hasattr(msg, 'time_increment'):\n",
        "                        print(f\"  time_increment: {msg.time_increment:.9f} seconds\")\n",
        "                    if hasattr(msg, 'scan_time'):\n",
        "                        print(f\"  scan_time: {msg.scan_time:.6f} seconds\")\n",
        "                    if hasattr(msg, 'range_min'):\n",
        "                        print(f\"  range_min: {msg.range_min:.3f} m\")\n",
        "                    if hasattr(msg, 'range_max'):\n",
        "                        print(f\"  range_max: {msg.range_max:.3f} m\\n\")\n",
        "                    \n",
        "                    # Print range data sample\n",
        "                    if hasattr(msg, 'ranges'):\n",
        "                        ranges = np.array(msg.ranges)\n",
        "                        print(f\"Range Data:\")\n",
        "                        print(f\"  Total points: {len(ranges)}\")\n",
        "                        print(f\"  First 10 ranges: {ranges[:10]}\")\n",
        "                        print(f\"  Last 10 ranges: {ranges[-10:]}\")\n",
        "                        print(f\"  Valid ranges: {np.sum(np.isfinite(ranges))} ({100*np.sum(np.isfinite(ranges))/len(ranges):.1f}%)\\n\")\n",
        "                    \n",
        "                    # Print intensity data sample if available\n",
        "                    if hasattr(msg, 'intensities') and len(msg.intensities) > 0:\n",
        "                        intensities = np.array(msg.intensities)\n",
        "                        print(f\"Intensity Data:\")\n",
        "                        print(f\"  Total points: {len(intensities)}\")\n",
        "                        print(f\"  First 10 intensities: {intensities[:10]}\")\n",
        "                        print(f\"  Min: {np.min(intensities):.2f}, Max: {np.max(intensities):.2f}, Mean: {np.mean(intensities):.2f}\")\n",
        "                    \n",
        "                    break  # Only show first message\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error deserializing message: {e}\")\n",
        "                    break\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No LiDAR scan topic found in bag file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZr6g6F93l_"
      },
      "source": [
        "### 4. Message Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qrxfvlO93l_"
      },
      "outputs": [],
      "source": [
        "# Analyze message frequency distribution\n",
        "print(\"=\" * 60)\n",
        "print(\"MESSAGE FREQUENCY DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    connection_timestamps = defaultdict(list)  # Track by connection index\n",
        "    topic_timestamps = defaultdict(list)  # Also track by topic for backward compatibility\n",
        "\n",
        "    print(\"\\nCollecting timestamps...\")\n",
        "    for connection, timestamp, rawdata in tqdm(reader.messages(), total=reader.message_count):\n",
        "        topic = connection.topic\n",
        "        timestamp_sec = timestamp * 1e-9\n",
        "\n",
        "        # Add to topic_timestamps\n",
        "        topic_timestamps[topic].append(timestamp_sec)\n",
        "\n",
        "        # Find connection index for detailed tracking\n",
        "        conn_idx = None\n",
        "        for idx, conn in enumerate(reader.connections):\n",
        "            if (conn.topic == connection.topic and\n",
        "                conn.msgtype == connection.msgtype and\n",
        "                conn.msgcount == connection.msgcount):\n",
        "                conn_idx = idx\n",
        "                break\n",
        "\n",
        "        if conn_idx is not None:\n",
        "            connection_timestamps[conn_idx].append(timestamp_sec)\n",
        "\n",
        "    print(\"\\nüìà Inter-Message Interval Statistics (by connection):\")\n",
        "    print(f\"{'Topic':<40} {'Description':<30} {'Mean (ms)':<12} {'Std (ms)':<12}\")\n",
        "    print(\"-\" * 94)\n",
        "\n",
        "    for conn_idx in sorted(connection_timestamps.keys()):\n",
        "        timestamps = connection_timestamps[conn_idx]\n",
        "        if len(timestamps) > 1:\n",
        "            intervals = np.diff(timestamps) * 1000\n",
        "\n",
        "            # Get topic info from df_topics\n",
        "            topic_row = df_topics[df_topics['Connection_Index'] == conn_idx].iloc[0]\n",
        "            topic_name = topic_row['Topic']\n",
        "            description = topic_row['Description'][:28] if topic_row['Description'] else ''\n",
        "\n",
        "            print(f\"{topic_name:<40} {description:<30} {np.mean(intervals):<12.2f} {np.std(intervals):<12.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00sOQJiZ93mA"
      },
      "source": [
        "### 5. Message Size Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK-5nzV393mA"
      },
      "outputs": [],
      "source": [
        "# Analyze message sizes per connection\n",
        "print(\"=\" * 60)\n",
        "print(\"MESSAGE SIZE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    connection_sizes = defaultdict(list)\n",
        "\n",
        "    print(\"\\nCollecting message sizes...\")\n",
        "    for connection, timestamp, rawdata in tqdm(reader.messages(), total=reader.message_count):\n",
        "        # Find connection index\n",
        "        conn_idx = None\n",
        "        for idx, conn in enumerate(reader.connections):\n",
        "            if (conn.topic == connection.topic and\n",
        "                conn.msgtype == connection.msgtype and\n",
        "                conn.msgcount == connection.msgcount):\n",
        "                conn_idx = idx\n",
        "                break\n",
        "\n",
        "        if conn_idx is not None:\n",
        "            connection_sizes[conn_idx].append(len(rawdata))\n",
        "\n",
        "    # Calculate statistics\n",
        "    size_stats = []\n",
        "    for conn_idx, sizes in connection_sizes.items():\n",
        "        sizes_array = np.array(sizes)\n",
        "        topic_row = df_topics[df_topics['Connection_Index'] == conn_idx].iloc[0]\n",
        "\n",
        "        size_stats.append({\n",
        "            'Topic': topic_row['Topic'],\n",
        "            'Description': topic_row['Description'][:40] if topic_row['Description'] else '',\n",
        "            'Avg Size (bytes)': np.mean(sizes_array),\n",
        "            'Total Size (MB)': np.sum(sizes_array) / (1024 * 1024)\n",
        "        })\n",
        "\n",
        "    df_sizes = pd.DataFrame(size_stats)\n",
        "    df_sizes = df_sizes.sort_values('Total Size (MB)', ascending=False)\n",
        "\n",
        "    print(\"\\nüìè Message Size Statistics:\")\n",
        "    print(df_sizes.to_string(index=False))\n",
        "\n",
        "    df_sizes.to_csv('message_size_statistics.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppgdsxWxJZtT"
      },
      "outputs": [],
      "source": [
        "# Generate ROS Bag Basic EDA summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"ROS BAG BASIC EDA - SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rosbag_summary = f\"\"\"\n",
        "üìä ROS BAG BASIC EDA SUMMARY\n",
        "{'='*60}\n",
        "\n",
        "FILE INFORMATION:\n",
        "  ‚Ä¢ File: {bag_name}\n",
        "  ‚Ä¢ Size: {file_size_mb:.2f} MB\n",
        "  ‚Ä¢ Duration: {duration_sec:.2f} seconds ({duration_sec/60:.2f} minutes)\n",
        "\n",
        "MESSAGE STATISTICS:\n",
        "  ‚Ä¢ Total Messages: {total_messages:,}\n",
        "  ‚Ä¢ Unique Topics: {len(unique_topics)}\n",
        "  ‚Ä¢ Message Types: {df_topics['Message Type'].nunique()}\n",
        "  ‚Ä¢ Average Rate: {avg_message_rate:.2f} msg/sec\n",
        "\n",
        "DATA QUALITY:\n",
        "  ‚Ä¢ Average Data Rate: {file_size_mb / (duration_sec/60):.2f} MB/min\n",
        "\n",
        "TOP 3 TOPICS BY SIZE:\n",
        "\"\"\"\n",
        "\n",
        "for idx, row in df_sizes.head(3).iterrows():\n",
        "    rosbag_summary += f\"  {idx+1}. {row['Topic']}: {row['Total Size (MB)']:.2f} MB\\n\"\n",
        "\n",
        "print(rosbag_summary)\n",
        "\n",
        "# Save report\n",
        "with open('rosbag_basic_eda_summary.txt', 'w') as f:\n",
        "    f.write(rosbag_summary)\n",
        "\n",
        "print(\"\\n‚úÖ ROS Bag Basic EDA summary saved to: rosbag_basic_eda_summary.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ4VqAAGJZtT"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 5. LiDAR Topic Basic EDA</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOazsd51JZtT"
      },
      "source": [
        "### Hardware Documentation: RPLiDAR System\n",
        "\n",
        "---\n",
        "\n",
        "## üîµ Hardware Specifications\n",
        "\n",
        "**LiDAR Sensor: SLAMTEC RPLIDAR A1M8-R6**\n",
        "\n",
        "The Waveshare AI ROS Pro Kit is equipped with the SLAMTEC RPLIDAR A1M8-R6, a budget-friendly 360¬∞ laser scanner designed for robotics education and SLAM applications.\n",
        "\n",
        "**Model Information:**\n",
        "- **Full Model:** RPLIDAR A1M8-R6 (latest revision)\n",
        "- **Manufacturer:** SLAMTEC (Shanghai Slamtec Co., Ltd.)\n",
        "- **Technology:** Laser Triangulation Ranging\n",
        "- **Product Line:** Entry-level 2D LiDAR\n",
        "\n",
        "**Range Performance:**\n",
        "- **Distance Range:** 0.15m - 12m\n",
        "  - Effective range: 6-12m (depending on surface reflectivity)\n",
        "  - White/light surfaces: Up to 12m\n",
        "  - Dark surfaces: 6-8m typical\n",
        "- **Distance Resolution:** <0.5mm (0.2cm typical)\n",
        "- **Distance Accuracy:** ¬±0.5mm to ¬±1cm\n",
        "- **Angular Range:** 0¬∞ - 360¬∞ (full rotation)\n",
        "- **Angular Resolution:** ‚â§1¬∞ (approximately 360 points per scan)\n",
        "\n",
        "**Scanning Performance:**\n",
        "- **Sample Rate:** Up to 8,000 samples/second (maximum)\n",
        "- **Sample Frequency:** 2,000-2,010 Hz per individual measurement\n",
        "- **Sample Duration:** 0.5ms per point\n",
        "- **Scan Rate:** 1-10 Hz (user configurable via PWM)\n",
        "  - **Typical/Default:** 5.5 Hz\n",
        "  - **Configurable Range:** 2-10 Hz\n",
        "- **Rotation Direction:** Clockwise\n",
        "\n",
        "**Laser Specifications:**\n",
        "- **Laser Type:** Infrared laser diode\n",
        "- **Wavelength:** 775-795nm (near-infrared)\n",
        "- **Laser Power:** <5mW\n",
        "- **Safety Class:** Class 1 (Eye Safe)\n",
        "- **Beam Divergence:** Optimized for triangulation\n",
        "\n",
        "**Physical Specifications:**\n",
        "- **Dimensions:** 96.8mm (W) √ó 70mm (D) √ó 55mm (H)\n",
        "- **Weight:** 170g (including motor and electronics)\n",
        "- **Housing:** Protective enclosure with rotating core\n",
        "\n",
        "**Electrical & Interface:**\n",
        "- **Power Supply:** 5V DC\n",
        "- **Interface Options:**\n",
        "  - Primary: 3.3V TTL UART (5-pin, 2.0mm pitch connector)\n",
        "  - Alternative: USB via included adapter\n",
        "- **UART Baud Rate:** 115200 bps (default) or 57600 bps (with adapter)\n",
        "- **Motor Control:** PWM input for scan rate adjustment\n",
        "- **On Jetson Nano:** Communicates via `/dev/ttyACM1`\n",
        "\n",
        "**Unique Technology - OPTMAG:**\n",
        "- Wireless power transmission (no slip rings)\n",
        "- Optical data communication\n",
        "- Extended lifespan (eliminates mechanical wear)\n",
        "- Maintenance-free operation\n",
        "\n",
        "---\n",
        "\n",
        "## üì° ROS Topics Published\n",
        "\n",
        "The RPLIDAR A1M8 publishes one primary topic:\n",
        "\n",
        "### `/scan` (or `/rplidar/scan`)\n",
        "\n",
        "**Message Type:** `sensor_msgs/LaserScan`\n",
        "\n",
        "**Message Fields:**\n",
        "\n",
        "**Header:**\n",
        "- `stamp` - Timestamp of scan start\n",
        "- `frame_id` - Coordinate frame (typically \"laser\", \"base_laser\", or \"laser_frame\")\n",
        "\n",
        "**Scan Configuration:**\n",
        "- `angle_min` - Start angle (radians, typically -œÄ ‚âà -3.14159)\n",
        "- `angle_max` - End angle (radians, typically +œÄ ‚âà +3.14159)\n",
        "- `angle_increment` - Angular step between measurements (radians, ~0.0174533 = 1¬∞)\n",
        "- `time_increment` - Time between measurements (~0.0005 sec = 0.5ms)\n",
        "- `scan_time` - Time for complete rotation\n",
        "  - At 5.5 Hz: ~0.182 seconds\n",
        "  - At 10 Hz: ~0.1 seconds\n",
        "\n",
        "**Range Configuration:**\n",
        "- `range_min` - Minimum valid range (0.15 meters)\n",
        "- `range_max` - Maximum valid range (12.0 meters)\n",
        "\n",
        "**Data Arrays:**\n",
        "- `ranges[]` - Array of distance measurements in meters\n",
        "  - Array length: ~360 measurements (one per degree)\n",
        "  - Invalid readings: `inf` (no return/out of range) or `nan` (sensor error)\n",
        "- `intensities[]` - Signal strength values (may be empty or populated)\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Expected Data Characteristics for A1M8\n",
        "\n",
        "**Normal Operation:**\n",
        "- **Points per scan:** ~360 points (360¬∞ √∑ 1¬∞ resolution)\n",
        "- **Scan frequency:** 5.5 Hz typical (configurable 2-10 Hz)\n",
        "- **Scan period:** ~182ms at 5.5 Hz, ~100ms at 10 Hz\n",
        "- **Data completeness:** 85-95% valid readings typical\n",
        "- **Sample rate:** 8,000 samples/sec maximum\n",
        "- **Points per revolution:** Varies with scan rate (more points at slower rates)\n",
        "\n",
        "**Range Performance by Distance:**\n",
        "- **0.15m - 6m:** Highly reliable, all surface types\n",
        "- **6m - 12m:** Reliable for light-colored surfaces\n",
        "- **>12m:** Returns `inf` (out of range)\n",
        "- **<0.15m:** Returns `nan` or `0` (too close)\n",
        "\n",
        "**Invalid Readings:**\n",
        "- `inf` values: No obstacle detected (beyond max range or no reflection)\n",
        "- `nan` values: Sensor error or invalid measurement\n",
        "- Out-of-range values: < 0.15m or > 12.0m\n",
        "- Typical invalid rate: 5-15% depending on environment\n",
        "\n",
        "**Surface Material Sensitivity:**\n",
        "- **Excellent reflection (8-12m):**\n",
        "  - White walls and surfaces\n",
        "  - Light-colored materials\n",
        "  - Paper, cardboard\n",
        "  - Matte painted surfaces\n",
        "- **Good reflection (6-10m):**\n",
        "  - Wood surfaces\n",
        "  - Concrete\n",
        "  - Colored surfaces\n",
        "- **Reduced range (3-6m):**\n",
        "  - Dark surfaces (black, dark blue, dark brown)\n",
        "  - Rough textured surfaces\n",
        "- **Poor/unreliable detection:**\n",
        "  - Transparent materials (glass, clear acrylic)\n",
        "  - Mirrors and highly reflective surfaces\n",
        "  - Shiny metal (angle-dependent)\n",
        "  - Very dark or light-absorbing materials\n",
        "\n",
        "**Environmental Factors:**\n",
        "- **Indoor operation:** Excellent performance\n",
        "- **Outdoor with shade:** Good performance (6-8m typical)\n",
        "- **Direct sunlight:** Significantly reduced range and increased noise\n",
        "- **Dust/fog:** Causes false detections and reduced range\n",
        "- **Smoke:** Creates phantom obstacles at closer ranges\n",
        "- **Rain:** Not recommended (water droplets cause errors)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Issues to Check - A1M8 Specific\n",
        "\n",
        "1. **Scan Rate Variations:** Check if scan time is consistent (~182ms at 5.5Hz)\n",
        "2. **Angular Coverage:** Verify full 360¬∞ coverage (no missing angles)\n",
        "3. **Data Completeness:** Monitor valid vs invalid point ratio (should be >85%)\n",
        "4. **Range Consistency:** Check for sudden jumps or noise in measurements\n",
        "5. **Motor Speed Stability:** Ensure consistent rotation (no jitter in scan_time)\n",
        "6. **Communication Issues:** \n",
        "   - Check `/dev/ttyACM1` connection\n",
        "   - Verify baud rate (115200 or 57600)\n",
        "   - Look for USB disconnections\n",
        "7. **Mounting Vibration:** Can cause measurement noise (secure firmly)\n",
        "8. **Power Supply:** Ensure stable 5V (voltage drops affect motor speed)\n",
        "9. **Reflective Artifacts:** Mirrors can cause ghost readings\n",
        "10. **Sunlight Interference:** Avoid direct sunlight exposure\n",
        "\n",
        "**Known Limitations:**\n",
        "- No blind spot at 0¬∞ (unlike some older models)\n",
        "- Cannot detect transparent or mirror surfaces\n",
        "- Reduced accuracy at maximum range (>10m)\n",
        "- Sensitive to ambient lighting in outdoor environments\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Expected Values for Waveshare AI ROS Pro Kit\n",
        "\n",
        "```python\n",
        "# Typical LaserScan message structure for RPLIDAR A1M8\n",
        "header:\n",
        "  frame_id: \"laser_frame\" or \"base_laser\"\n",
        "angle_min: -3.14159 (radians = -180¬∞)\n",
        "angle_max: 3.14159 (radians = +180¬∞)\n",
        "angle_increment: 0.0174533 (radians ‚âà 1¬∞)\n",
        "time_increment: 0.0005 (sec = 0.5ms)\n",
        "scan_time: 0.182 (sec for 5.5 Hz) or 0.1 (sec for 10 Hz)\n",
        "range_min: 0.15 (meters)\n",
        "range_max: 12.0 (meters)\n",
        "ranges: [~360 measurements]\n",
        "intensities: [optional - may be empty or ~360 values]\n",
        "```\n",
        "\n",
        "**Performance Benchmarks:**\n",
        "- **Indoor effective range:** 8-12m typical\n",
        "- **Outdoor effective range:** 6-8m in shade, 3-5m in bright light\n",
        "- **Angular accuracy:** ¬±1¬∞ typical\n",
        "- **Distance accuracy:** ¬±5mm to ¬±10mm depending on range\n",
        "- **Measurement rate:** 8,000 samples/sec √∑ 360¬∞ ‚âà 22 samples per degree\n",
        "- **Update rate:** 5.5 Hz typical (182ms per full scan)\n",
        "- **Data latency:** <10ms typical\n",
        "\n",
        "**Comparison to A2/A3 Models:**\n",
        "- A1M8: 12m range, 8K samples/sec, 5.5Hz typical\n",
        "- A2: 18m range, 8K samples/sec, 10Hz typical\n",
        "- A3: 25m range, 16K samples/sec, 20Hz typical\n",
        "\n",
        "The A1M8 is the entry-level model, perfect for indoor robotics education and hobbyist SLAM applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MmwWQJZtT"
      },
      "source": [
        "### LiDAR Scan Topic Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS8abkWdJZtT"
      },
      "outputs": [],
      "source": [
        "# Find and inspect LiDAR scan topics\n",
        "print(\"=\" * 60)\n",
        "print(\"LIDAR SCAN TOPIC INSPECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Find scan topics\n",
        "scan_topics = []\n",
        "for topic in unique_topics:\n",
        "    if 'scan' in topic.lower() or 'lidar' in topic.lower():\n",
        "        scan_topics.append(topic)\n",
        "\n",
        "if scan_topics:\n",
        "    print(f\"\\n‚úì Found {len(scan_topics)} scan topic(s): {scan_topics}\")\n",
        "    lidar_topic = scan_topics[0]\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scan topics found!\")\n",
        "    print(\"Available topics:\", unique_topics)\n",
        "    lidar_topic = None\n",
        "\n",
        "if lidar_topic:\n",
        "    # Parse first message to get scan parameters\n",
        "    with Reader(bag_path) as reader:\n",
        "        for connection, timestamp, rawdata in reader.messages():\n",
        "            if connection.topic == lidar_topic:\n",
        "                try:\n",
        "                    msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "                    \n",
        "                    print(\"\\n\\nüîµ LIDAR SCAN PARAMETERS:\")\n",
        "                    print(\"=\" * 60)\n",
        "                    \n",
        "                    print(\"\\nüìê Angular Configuration:\")\n",
        "                    print(f\"   Angle Min: {msg.angle_min:.6f} rad ({np.rad2deg(msg.angle_min):.2f}¬∞)\")\n",
        "                    print(f\"   Angle Max: {msg.angle_max:.6f} rad ({np.rad2deg(msg.angle_max):.2f}¬∞)\")\n",
        "                    print(f\"   Angle Increment: {msg.angle_increment:.6f} rad ({np.rad2deg(msg.angle_increment):.4f}¬∞)\")\n",
        "                    fov = np.rad2deg(msg.angle_max - msg.angle_min)\n",
        "                    print(f\"   Total FOV: {fov:.2f}¬∞\")\n",
        "                    print(f\"   Points per Scan: {len(msg.ranges)}\")\n",
        "                    \n",
        "                    print(\"\\nüìè Range Configuration:\")\n",
        "                    print(f\"   Min Range: {msg.range_min:.2f} m\")\n",
        "                    print(f\"   Max Range: {msg.range_max:.2f} m\")\n",
        "                    \n",
        "                    print(\"\\n‚è±Ô∏è Timing Parameters:\")\n",
        "                    print(f\"   Time Increment: {msg.time_increment*1000:.4f} ms per measurement\")\n",
        "                    print(f\"   Scan Time: {msg.scan_time:.4f} seconds\")\n",
        "                    expected_freq = 1/msg.scan_time if msg.scan_time > 0 else 0\n",
        "                    print(f\"   Expected Scan Rate: {expected_freq:.2f} Hz\")\n",
        "                    \n",
        "                    # Analyze first scan\n",
        "                    ranges = np.array(msg.ranges)\n",
        "                    valid_mask = np.isfinite(ranges) & (ranges >= msg.range_min) & (ranges <= msg.range_max)\n",
        "                    valid_count = np.sum(valid_mask)\n",
        "                    \n",
        "                    print(\"\\nüìä Sample Scan Data:\")\n",
        "                    print(f\"   Valid Points: {valid_count} / {len(ranges)} ({100*valid_count/len(ranges):.1f}%)\")\n",
        "                    if valid_count > 0:\n",
        "                        print(f\"   Range - Min: {np.min(ranges[valid_mask]):.2f} m\")\n",
        "                        print(f\"   Range - Mean: {np.mean(ranges[valid_mask]):.2f} m\")\n",
        "                        print(f\"   Range - Max: {np.max(ranges[valid_mask]):.2f} m\")\n",
        "                    \n",
        "                    # Check for intensity data\n",
        "                    if hasattr(msg, 'intensities') and len(msg.intensities) > 0:\n",
        "                        print(f\"\\n   Intensity Data: Available ({len(msg.intensities)} values)\")\n",
        "                    else:\n",
        "                        print(f\"\\n   Intensity Data: Not available\")\n",
        "                    \n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ö†Ô∏è Error parsing scan message: {e}\")\n",
        "                    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1190_timing"
      },
      "source": [
        "### 1. LiDAR Topic Timing Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timing_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze timing characteristics of LiDAR topic\n",
        "print(\"=\" * 60)\n",
        "print(\"LIDAR TOPIC TIMING ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if lidar_topic and lidar_topic in topic_timestamps:\n",
        "    scan_timestamps = np.array(topic_timestamps[lidar_topic])\n",
        "    \n",
        "    if len(scan_timestamps) > 1:\n",
        "        intervals = np.diff(scan_timestamps)\n",
        "        frequencies = 1.0 / intervals\n",
        "        \n",
        "        print(f\"\\nüìä Timing Statistics for {lidar_topic}:\")\n",
        "        print(f\"   Total Scans: {len(scan_timestamps)}\")\n",
        "        print(f\"   Duration: {scan_timestamps[-1] - scan_timestamps[0]:.2f} seconds\")\n",
        "        \n",
        "        print(f\"\\n‚è±Ô∏è Inter-Scan Intervals:\")\n",
        "        print(f\"   Mean: {np.mean(intervals)*1000:.2f} ms\")\n",
        "        print(f\"   Std Dev: {np.std(intervals)*1000:.2f} ms\")\n",
        "        print(f\"   Min: {np.min(intervals)*1000:.2f} ms\")\n",
        "        print(f\"   Max: {np.max(intervals)*1000:.2f} ms\")\n",
        "        \n",
        "        print(f\"\\nüì° Scan Frequency:\")\n",
        "        print(f\"   Mean: {np.mean(frequencies):.2f} Hz\")\n",
        "        print(f\"   Std Dev: {np.std(frequencies):.2f} Hz\")\n",
        "        print(f\"   Jitter (CV): {(np.std(intervals)/np.mean(intervals))*100:.2f}%\")\n",
        "        \n",
        "        # Check for dropped scans\n",
        "        expected_interval = 0.1  # 10 Hz expected\n",
        "        dropped_threshold = expected_interval * 1.5\n",
        "        dropped_count = np.sum(intervals > dropped_threshold)\n",
        "        print(f\"\\n‚ö†Ô∏è Data Quality:\")\n",
        "        print(f\"   Potential dropped scans: {dropped_count} ({100*dropped_count/len(intervals):.2f}%)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No timing data available for LiDAR topic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1318_summary"
      },
      "source": [
        "### 2. LiDAR Topic Basic EDA Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary_report"
      },
      "outputs": [],
      "source": [
        "# Generate LiDAR Topic Basic EDA summary\n",
        "print(\"=\" * 60)\n",
        "print(\"LIDAR TOPIC BASIC EDA - SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if lidar_topic:\n",
        "    # Get topic statistics\n",
        "    topic_info = df_topics_display[df_topics_display['Topic'] == lidar_topic].iloc[0]\n",
        "    size_info = df_sizes[df_sizes['Topic'] == lidar_topic].iloc[0] if lidar_topic in df_sizes['Topic'].values else None\n",
        "    \n",
        "    lidar_topic_summary = f\"\"\"\n",
        "üîµ LIDAR TOPIC BASIC EDA SUMMARY\n",
        "{'='*60}\n",
        "\n",
        "TOPIC INFORMATION:\n",
        "  ‚Ä¢ Topic: {lidar_topic}\n",
        "  ‚Ä¢ Message Type: {topic_info['Message Type']}\n",
        "  ‚Ä¢ Total Messages: {topic_info['Count']:,}\n",
        "  ‚Ä¢ Frequency: {topic_info['Frequency (Hz)']:.2f} Hz\n",
        "\n",
        "TIMING CHARACTERISTICS:\n",
        "  ‚Ä¢ Mean Interval: {np.mean(intervals)*1000:.2f} ms\n",
        "  ‚Ä¢ Timing Jitter: {(np.std(intervals)/np.mean(intervals))*100:.2f}%\n",
        "  ‚Ä¢ Dropped Scans: {dropped_count} ({100*dropped_count/len(intervals):.2f}%)\n",
        "\"\"\"\n",
        "    \n",
        "    if size_info is not None:\n",
        "        lidar_topic_summary += f\"\"\"\n",
        "DATA SIZE:\n",
        "  ‚Ä¢ Average Size: {size_info['Avg Size (bytes)']:.2f} bytes\n",
        "  ‚Ä¢ Total Size: {size_info['Total Size (MB)']:.2f} MB\n",
        "\"\"\"\n",
        "    \n",
        "    print(lidar_topic_summary)\n",
        "    \n",
        "    # Save report\n",
        "    with open('lidar_topic_basic_eda_summary.txt', 'w') as f:\n",
        "        f.write(lidar_topic_summary)\n",
        "    \n",
        "    print(\"\\n‚úÖ LiDAR Topic Basic EDA summary saved to: lidar_topic_basic_eda_summary.txt\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No LiDAR topic found for summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_header"
      },
      "source": [
        "## <font color='#2E86AB'>‚ñº 6. LiDAR Scan EDA</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_1"
      },
      "source": [
        "### 1. Extract and Display Sample Scans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_scans"
      },
      "outputs": [],
      "source": [
        "# Extract all LiDAR scans\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING LIDAR SCANS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "scans = []\n",
        "scan_timestamps = []\n",
        "scan_params = {}\n",
        "\n",
        "if lidar_topic:\n",
        "    print(f\"\\nüîÑ Extracting scans from {lidar_topic}...\")\n",
        "    \n",
        "    with Reader(bag_path) as reader:\n",
        "        connections = [x for x in reader.connections if x.topic == lidar_topic]\n",
        "        \n",
        "        for connection, timestamp, rawdata in tqdm(reader.messages(connections=connections)):\n",
        "            try:\n",
        "                msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "                \n",
        "                # Store first scan parameters\n",
        "                if not scan_params:\n",
        "                    scan_params = {\n",
        "                        'angle_min': msg.angle_min,\n",
        "                        'angle_max': msg.angle_max,\n",
        "                        'angle_increment': msg.angle_increment,\n",
        "                        'time_increment': msg.time_increment,\n",
        "                        'scan_time': msg.scan_time,\n",
        "                        'range_min': msg.range_min,\n",
        "                        'range_max': msg.range_max,\n",
        "                    }\n",
        "                \n",
        "                scans.append(np.array(msg.ranges))\n",
        "                scan_timestamps.append(timestamp / 1e9)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error parsing scan: {e}\")\n",
        "    \n",
        "    scan_timestamps = np.array(scan_timestamps)\n",
        "    if len(scan_timestamps) > 0:\n",
        "        scan_timestamps = scan_timestamps - scan_timestamps[0]\n",
        "    \n",
        "    print(f\"\\n‚úÖ Extracted {len(scans)} LiDAR scans\")\n",
        "    print(f\"   Duration: {scan_timestamps[-1]:.2f} seconds\")\n",
        "    print(f\"   Points per scan: {len(scans[0]) if len(scans) > 0 else 0}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No LiDAR topic available for extraction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display_sample_scans"
      },
      "outputs": [],
      "source": [
        "# Visualize sample scans in polar and Cartesian coordinates\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE LIDAR SCANS - POLAR VIEW\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0:\n",
        "    # Select sample scans\n",
        "    sample_indices = [0, len(scans)//4, len(scans)//2, 3*len(scans)//4]\n",
        "    \n",
        "    # Polar plots\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    \n",
        "    for idx, scan_idx in enumerate(sample_indices):\n",
        "        scan = scans[scan_idx]\n",
        "        angles = np.arange(len(scan)) * scan_params['angle_increment'] + scan_params['angle_min']\n",
        "        \n",
        "        # Filter valid readings\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        valid_angles = angles[valid_mask]\n",
        "        \n",
        "        # Polar plot\n",
        "        ax = plt.subplot(2, 2, idx + 1, projection='polar')\n",
        "        scatter = ax.scatter(valid_angles, valid_ranges, c=valid_ranges, cmap='viridis', s=2, alpha=0.6)\n",
        "        ax.set_ylim(0, scan_params['range_max'])\n",
        "        ax.set_title(f\"Scan #{scan_idx} (t={scan_timestamps[scan_idx]:.1f}s)\\nValid: {np.sum(valid_mask)}/{len(scan)} points ({100*np.sum(valid_mask)/len(scan):.1f}%)\", \n",
        "                     fontsize=10, pad=20)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.colorbar(scatter, ax=ax, label='Range (m)', fraction=0.046, pad=0.04)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_scans_polar.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Polar visualization saved to: sample_scans_polar.png\")\n",
        "    \n",
        "    # Cartesian plots\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAMPLE LIDAR SCANS - CARTESIAN VIEW\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, scan_idx in enumerate(sample_indices):\n",
        "        scan = scans[scan_idx]\n",
        "        angles = np.arange(len(scan)) * scan_params['angle_increment'] + scan_params['angle_min']\n",
        "        \n",
        "        # Filter valid readings\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        valid_angles = angles[valid_mask]\n",
        "        \n",
        "        # Convert to Cartesian\n",
        "        x = valid_ranges * np.cos(valid_angles)\n",
        "        y = valid_ranges * np.sin(valid_angles)\n",
        "        \n",
        "        # Plot\n",
        "        ax = axes[idx]\n",
        "        scatter = ax.scatter(x, y, c=valid_ranges, cmap='viridis', s=10, alpha=0.6)\n",
        "        ax.scatter(0, 0, c='red', s=150, marker='x', linewidths=3, label='Robot', zorder=5)\n",
        "        ax.set_xlabel('X (meters)', fontsize=10)\n",
        "        ax.set_ylabel('Y (meters)', fontsize=10)\n",
        "        ax.set_title(f\"Scan #{scan_idx} (t={scan_timestamps[scan_idx]:.1f}s)\", fontsize=11)\n",
        "        ax.axis('equal')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(fontsize=9)\n",
        "        plt.colorbar(scatter, ax=ax, label='Range (m)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_scans_cartesian.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Cartesian visualization saved to: sample_scans_cartesian.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scans available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_2"
      },
      "source": [
        "### 2. Scan Properties Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scan_properties"
      },
      "outputs": [],
      "source": [
        "# Analyze scan properties\n",
        "print(\"=\" * 60)\n",
        "print(\"SCAN PROPERTIES ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0:\n",
        "    # Calculate per-scan statistics\n",
        "    valid_counts = []\n",
        "    invalid_counts = []\n",
        "    mean_ranges = []\n",
        "    min_ranges = []\n",
        "    max_ranges = []\n",
        "    \n",
        "    for scan in scans:\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        \n",
        "        valid_counts.append(np.sum(valid_mask))\n",
        "        invalid_counts.append(np.sum(~valid_mask))\n",
        "        \n",
        "        if len(valid_ranges) > 0:\n",
        "            mean_ranges.append(np.mean(valid_ranges))\n",
        "            min_ranges.append(np.min(valid_ranges))\n",
        "            max_ranges.append(np.max(valid_ranges))\n",
        "        else:\n",
        "            mean_ranges.append(np.nan)\n",
        "            min_ranges.append(np.nan)\n",
        "            max_ranges.append(np.nan)\n",
        "    \n",
        "    print(f\"\\nüìä Scan Properties Statistics:\")\n",
        "    print(f\"   Total Scans: {len(scans)}\")\n",
        "    print(f\"   Points per Scan: {len(scans[0])}\")\n",
        "    \n",
        "    print(f\"\\nüìà Valid Points per Scan:\")\n",
        "    print(f\"   Mean: {np.mean(valid_counts):.1f} ({100*np.mean(valid_counts)/len(scans[0]):.1f}%)\")\n",
        "    print(f\"   Std Dev: {np.std(valid_counts):.1f}\")\n",
        "    print(f\"   Range: [{np.min(valid_counts)}, {np.max(valid_counts)}]\")\n",
        "    \n",
        "    print(f\"\\nüìâ Invalid Points per Scan:\")\n",
        "    print(f\"   Mean: {np.mean(invalid_counts):.1f} ({100*np.mean(invalid_counts)/len(scans[0]):.1f}%)\")\n",
        "    print(f\"   Std Dev: {np.std(invalid_counts):.1f}\")\n",
        "    \n",
        "    print(f\"\\nüìè Range Statistics:\")\n",
        "    print(f\"   Mean Range: {np.nanmean(mean_ranges):.2f} m\")\n",
        "    print(f\"   Closest Obstacle (overall): {np.nanmin(min_ranges):.2f} m\")\n",
        "    print(f\"   Furthest Detection (overall): {np.nanmax(max_ranges):.2f} m\")\n",
        "    \n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Valid/Invalid points over time\n",
        "    axes[0, 0].plot(scan_timestamps, valid_counts, label='Valid Points', color='green', linewidth=1)\n",
        "    axes[0, 0].plot(scan_timestamps, invalid_counts, label='Invalid Points', color='red', linewidth=1, alpha=0.7)\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Point Count')\n",
        "    axes[0, 0].set_title('Valid vs Invalid Points Over Time')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Data completeness percentage\n",
        "    completeness = np.array(valid_counts) / len(scans[0]) * 100\n",
        "    axes[0, 1].plot(scan_timestamps, completeness, color='steelblue', linewidth=1)\n",
        "    axes[0, 1].axhline(np.mean(completeness), color='red', linestyle='--', \n",
        "                       label=f'Mean: {np.mean(completeness):.1f}%')\n",
        "    axes[0, 1].set_xlabel('Time (seconds)')\n",
        "    axes[0, 1].set_ylabel('Completeness (%)')\n",
        "    axes[0, 1].set_title('Data Completeness Over Time')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].set_ylim([0, 105])\n",
        "    \n",
        "    # Mean range over time\n",
        "    axes[1, 0].plot(scan_timestamps, mean_ranges, color='orange', linewidth=1)\n",
        "    axes[1, 0].set_xlabel('Time (seconds)')\n",
        "    axes[1, 0].set_ylabel('Mean Range (meters)')\n",
        "    axes[1, 0].set_title('Average Detection Range Over Time')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Closest obstacle over time\n",
        "    axes[1, 1].plot(scan_timestamps, min_ranges, color='darkred', linewidth=1)\n",
        "    axes[1, 1].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Danger zone (0.5m)')\n",
        "    axes[1, 1].axhline(1.0, color='orange', linestyle='--', alpha=0.5, label='Warning zone (1.0m)')\n",
        "    axes[1, 1].set_xlabel('Time (seconds)')\n",
        "    axes[1, 1].set_ylabel('Closest Obstacle (meters)')\n",
        "    axes[1, 1].set_title('Minimum Range Over Time')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('scan_properties.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Scan properties analysis saved to: scan_properties.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scans available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_3"
      },
      "source": [
        "### 3. Range Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "range_distribution"
      },
      "outputs": [],
      "source": [
        "# Analyze range distribution across all scans\n",
        "print(\"=\" * 60)\n",
        "print(\"RANGE DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0:\n",
        "    # Collect all valid ranges\n",
        "    all_ranges = []\n",
        "    for scan in scans:\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        all_ranges.extend(scan[valid_mask])\n",
        "    \n",
        "    all_ranges = np.array(all_ranges)\n",
        "    \n",
        "    print(f\"\\nüìä Range Statistics:\")\n",
        "    print(f\"   Total valid points: {len(all_ranges):,}\")\n",
        "    print(f\"   Mean range: {np.mean(all_ranges):.2f} m\")\n",
        "    print(f\"   Median range: {np.median(all_ranges):.2f} m\")\n",
        "    print(f\"   Std dev: {np.std(all_ranges):.2f} m\")\n",
        "    print(f\"   Min: {np.min(all_ranges):.2f} m\")\n",
        "    print(f\"   Max: {np.max(all_ranges):.2f} m\")\n",
        "    \n",
        "    # Percentiles\n",
        "    percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "    percentile_values = np.percentile(all_ranges, percentiles)\n",
        "    print(f\"\\nüìà Range Percentiles:\")\n",
        "    for p, v in zip(percentiles, percentile_values):\n",
        "        print(f\"   {p}th percentile: {v:.2f} m\")\n",
        "    \n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Range histogram\n",
        "    axes[0, 0].hist(all_ranges, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0, 0].axvline(np.mean(all_ranges), color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {np.mean(all_ranges):.2f}m')\n",
        "    axes[0, 0].axvline(np.median(all_ranges), color='orange', linestyle='--', linewidth=2,\n",
        "                       label=f'Median: {np.median(all_ranges):.2f}m')\n",
        "    axes[0, 0].set_xlabel('Range (meters)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].set_title('Range Distribution - All Scans')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Cumulative distribution\n",
        "    sorted_ranges = np.sort(all_ranges)\n",
        "    cumulative = np.arange(1, len(sorted_ranges) + 1) / len(sorted_ranges) * 100\n",
        "    axes[0, 1].plot(sorted_ranges, cumulative, color='darkgreen', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Range (meters)')\n",
        "    axes[0, 1].set_ylabel('Cumulative Percentage (%)')\n",
        "    axes[0, 1].set_title('Cumulative Range Distribution')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot by distance bins\n",
        "    bins = [0, 1, 2, 4, 8, scan_params['range_max']]\n",
        "    bin_labels = ['0-1m', '1-2m', '2-4m', '4-8m', f'8-{scan_params[\"range_max\"]:.0f}m']\n",
        "    binned_data = [all_ranges[(all_ranges >= bins[i]) & (all_ranges < bins[i+1])] \n",
        "                   for i in range(len(bins)-1)]\n",
        "    \n",
        "    bp = axes[1, 0].boxplot([data for data in binned_data if len(data) > 0], \n",
        "                   labels=[label for i, label in enumerate(bin_labels) if len(binned_data[i]) > 0],\n",
        "                   showmeans=True, patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "    axes[1, 0].set_xlabel('Distance Range')\n",
        "    axes[1, 0].set_ylabel('Measurements (meters)')\n",
        "    axes[1, 0].set_title('Range Distribution by Distance Bins')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Heatmap: Range variation over time and angle (subsampled)\n",
        "    subsample_scans = min(50, len(scans))\n",
        "    subsample_angles = min(180, len(scans[0]))\n",
        "    scan_indices = np.linspace(0, len(scans)-1, subsample_scans, dtype=int)\n",
        "    angle_indices = np.linspace(0, len(scans[0])-1, subsample_angles, dtype=int)\n",
        "    \n",
        "    angles = np.arange(len(scans[0])) * scan_params['angle_increment'] + scan_params['angle_min']\n",
        "    angles_deg = np.rad2deg(angles)\n",
        "    \n",
        "    heatmap_data = np.array([[scans[s][a] for a in angle_indices] for s in scan_indices])\n",
        "    heatmap_data[~np.isfinite(heatmap_data)] = np.nan\n",
        "    \n",
        "    im = axes[1, 1].imshow(heatmap_data.T, aspect='auto', cmap='viridis', \n",
        "                           extent=[scan_timestamps[scan_indices[0]], scan_timestamps[scan_indices[-1]], \n",
        "                                   angles_deg[angle_indices[0]], angles_deg[angle_indices[-1]]],\n",
        "                           origin='lower')\n",
        "    axes[1, 1].set_xlabel('Time (seconds)')\n",
        "    axes[1, 1].set_ylabel('Angle (degrees)')\n",
        "    axes[1, 1].set_title('Range Heatmap (Time vs Angle)')\n",
        "    plt.colorbar(im, ax=axes[1, 1], label='Range (m)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('range_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Range distribution analysis saved to: range_distribution.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scans available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_4"
      },
      "source": [
        "### 4. Angular Coverage Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "angular_coverage"
      },
      "outputs": [],
      "source": [
        "# Analyze coverage across different angles\n",
        "print(\"=\" * 60)\n",
        "print(\"ANGULAR COVERAGE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0:\n",
        "    # Calculate average range for each angle across all scans\n",
        "    num_angles = len(scans[0])\n",
        "    angles = np.arange(num_angles) * scan_params['angle_increment'] + scan_params['angle_min']\n",
        "    angles_deg = np.rad2deg(angles)\n",
        "    \n",
        "    angle_ranges = []\n",
        "    angle_valid_pct = []\n",
        "    angle_std = []\n",
        "    \n",
        "    for angle_idx in range(num_angles):\n",
        "        angle_data = [scan[angle_idx] for scan in scans]\n",
        "        valid_mask = np.isfinite(angle_data) & (np.array(angle_data) >= scan_params['range_min']) & \\\n",
        "                     (np.array(angle_data) <= scan_params['range_max'])\n",
        "        \n",
        "        valid_data = np.array(angle_data)[valid_mask]\n",
        "        angle_ranges.append(np.mean(valid_data) if len(valid_data) > 0 else np.nan)\n",
        "        angle_valid_pct.append(np.sum(valid_mask) / len(scans) * 100)\n",
        "        angle_std.append(np.std(valid_data) if len(valid_data) > 0 else np.nan)\n",
        "    \n",
        "    angle_ranges = np.array(angle_ranges)\n",
        "    angle_valid_pct = np.array(angle_valid_pct)\n",
        "    angle_std = np.array(angle_std)\n",
        "    \n",
        "    print(f\"\\nüìê Angular Coverage Statistics:\")\n",
        "    print(f\"   Total angles: {num_angles}\")\n",
        "    print(f\"   Angular resolution: {np.rad2deg(scan_params['angle_increment']):.3f}¬∞\")\n",
        "    print(f\"   Coverage: {np.rad2deg(scan_params['angle_max'] - scan_params['angle_min']):.1f}¬∞\")\n",
        "    \n",
        "    valid_angles = np.sum(~np.isnan(angle_ranges))\n",
        "    print(f\"\\n   Angles with valid data: {valid_angles}/{num_angles} ({100*valid_angles/num_angles:.1f}%)\")\n",
        "    \n",
        "    # Find blind spots (angles with low valid rate)\n",
        "    blind_threshold = 50\n",
        "    blind_angles = np.where(angle_valid_pct < blind_threshold)[0]\n",
        "    print(f\"   Potential blind spots (<{blind_threshold}% valid): {len(blind_angles)} angles\")\n",
        "    \n",
        "    if len(blind_angles) > 0:\n",
        "        blind_ranges = [(angles_deg[blind_angles[i]], angles_deg[blind_angles[min(i+1, len(blind_angles)-1)]]) \n",
        "                       for i in range(0, len(blind_angles), max(1, len(blind_angles)//5))]\n",
        "        print(f\"   Blind spot angular ranges: {blind_ranges[:5]}\")\n",
        "    \n",
        "    # Plotting\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "    \n",
        "    # Average range by angle (linear)\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    ax1.plot(angles_deg, angle_ranges, linewidth=1, color='steelblue')\n",
        "    ax1.axhline(np.nanmean(angle_ranges), color='red', linestyle='--', \n",
        "                label=f'Overall Mean: {np.nanmean(angle_ranges):.2f}m')\n",
        "    ax1.set_xlabel('Angle (degrees)')\n",
        "    ax1.set_ylabel('Average Range (meters)')\n",
        "    ax1.set_title('Average Range by Angle')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Valid percentage by angle\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    ax2.plot(angles_deg, angle_valid_pct, linewidth=1, color='green')\n",
        "    ax2.axhline(blind_threshold, color='red', linestyle='--', \n",
        "                label=f'Blind threshold ({blind_threshold}%)')\n",
        "    ax2.set_xlabel('Angle (degrees)')\n",
        "    ax2.set_ylabel('Valid Measurements (%)')\n",
        "    ax2.set_title('Data Validity by Angle')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 105])\n",
        "    \n",
        "    # Standard deviation by angle\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    ax3.plot(angles_deg, angle_std, linewidth=1, color='orange')\n",
        "    ax3.set_xlabel('Angle (degrees)')\n",
        "    ax3.set_ylabel('Std Deviation (meters)')\n",
        "    ax3.set_title('Range Variability by Angle')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Polar plot of average range\n",
        "    ax4 = plt.subplot(2, 3, 4, projection='polar')\n",
        "    ax4.plot(angles, angle_ranges, linewidth=2, color='darkblue')\n",
        "    ax4.fill(angles, angle_ranges, alpha=0.3, color='skyblue')\n",
        "    ax4.set_title('Average Range - Polar View', pad=20, fontsize=11)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Polar plot of validity\n",
        "    ax5 = plt.subplot(2, 3, 5, projection='polar')\n",
        "    validity_normalized = angle_valid_pct / 100 * scan_params['range_max']\n",
        "    ax5.plot(angles, validity_normalized, linewidth=2, color='darkgreen')\n",
        "    ax5.fill(angles, validity_normalized, alpha=0.3, color='lightgreen')\n",
        "    ax5.set_title('Data Validity - Polar View\\n(scaled to max range)', pad=20, fontsize=11)\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Histogram of average ranges\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    ax6.hist(angle_ranges[~np.isnan(angle_ranges)], bins=30, color='coral', \n",
        "             edgecolor='black', alpha=0.7)\n",
        "    ax6.set_xlabel('Average Range (meters)')\n",
        "    ax6.set_ylabel('Frequency')\n",
        "    ax6.set_title('Distribution of Average Ranges per Angle')\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('angular_coverage.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Angular coverage analysis saved to: angular_coverage.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scans available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_5"
      },
      "source": [
        "### 5. Obstacle Detection Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obstacle_detection"
      },
      "outputs": [],
      "source": [
        "# Analyze obstacles and safety zones\n",
        "print(\"=\" * 60)\n",
        "print(\"OBSTACLE DETECTION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0:\n",
        "    # Define safety zones\n",
        "    danger_zone = 0.5  # meters\n",
        "    warning_zone = 1.0  # meters\n",
        "    safe_zone = 2.0  # meters\n",
        "    \n",
        "    # Analyze each scan\n",
        "    danger_counts = []\n",
        "    warning_counts = []\n",
        "    safe_counts = []\n",
        "    closest_obstacles = []\n",
        "    \n",
        "    for scan in scans:\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        \n",
        "        if len(valid_ranges) > 0:\n",
        "            danger_counts.append(np.sum(valid_ranges < danger_zone))\n",
        "            warning_counts.append(np.sum((valid_ranges >= danger_zone) & (valid_ranges < warning_zone)))\n",
        "            safe_counts.append(np.sum(valid_ranges >= warning_zone))\n",
        "            closest_obstacles.append(np.min(valid_ranges))\n",
        "        else:\n",
        "            danger_counts.append(0)\n",
        "            warning_counts.append(0)\n",
        "            safe_counts.append(0)\n",
        "            closest_obstacles.append(np.nan)\n",
        "    \n",
        "    print(f\"\\nüö® Safety Zone Statistics:\")\n",
        "    print(f\"   Danger zone (<{danger_zone}m):\")\n",
        "    print(f\"      Avg detections per scan: {np.mean(danger_counts):.1f}\")\n",
        "    print(f\"      Max detections: {np.max(danger_counts)}\")\n",
        "    print(f\"      Scans with danger: {np.sum(np.array(danger_counts) > 0)} ({100*np.sum(np.array(danger_counts) > 0)/len(scans):.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n   Warning zone ({danger_zone}m-{warning_zone}m):\")\n",
        "    print(f\"      Avg detections per scan: {np.mean(warning_counts):.1f}\")\n",
        "    print(f\"      Scans with warnings: {np.sum(np.array(warning_counts) > 0)} ({100*np.sum(np.array(warning_counts) > 0)/len(scans):.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n   Closest obstacle:\")\n",
        "    print(f\"      Overall minimum: {np.nanmin(closest_obstacles):.2f} m\")\n",
        "    print(f\"      Average minimum: {np.nanmean(closest_obstacles):.2f} m\")\n",
        "    \n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Safety zones over time (stacked area)\n",
        "    axes[0, 0].fill_between(scan_timestamps, 0, danger_counts, \n",
        "                            alpha=0.7, label=f'Danger (<{danger_zone}m)', color='red')\n",
        "    axes[0, 0].fill_between(scan_timestamps, 0, warning_counts, \n",
        "                            alpha=0.5, label=f'Warning ({danger_zone}-{warning_zone}m)', color='orange')\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Point Count')\n",
        "    axes[0, 0].set_title('Safety Zone Detections Over Time')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Closest obstacle distance over time\n",
        "    axes[0, 1].plot(scan_timestamps, closest_obstacles, linewidth=1, color='darkblue')\n",
        "    axes[0, 1].axhline(danger_zone, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Danger threshold ({danger_zone}m)')\n",
        "    axes[0, 1].axhline(warning_zone, color='orange', linestyle='--', linewidth=2,\n",
        "                       label=f'Warning threshold ({warning_zone}m)')\n",
        "    axes[0, 1].set_xlabel('Time (seconds)')\n",
        "    axes[0, 1].set_ylabel('Distance (meters)')\n",
        "    axes[0, 1].set_title('Closest Obstacle Distance Over Time')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Safety zone distribution (box plots)\n",
        "    zone_data = [danger_counts, warning_counts, safe_counts]\n",
        "    zone_labels = [f'Danger\\n(<{danger_zone}m)', \n",
        "                   f'Warning\\n({danger_zone}-{warning_zone}m)', \n",
        "                   f'Safe\\n(>{warning_zone}m)']\n",
        "    \n",
        "    bp = axes[1, 0].boxplot(zone_data, labels=zone_labels, patch_artist=True,\n",
        "                            showmeans=True)\n",
        "    colors = ['lightcoral', 'lightyellow', 'lightgreen']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "    axes[1, 0].set_ylabel('Points per Scan')\n",
        "    axes[1, 0].set_title('Safety Zone Distribution')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Obstacle density map (aggregate multiple scans)\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "    \n",
        "    sample_scans = min(30, len(scans))\n",
        "    scan_step = len(scans) // sample_scans\n",
        "    \n",
        "    for i in range(0, len(scans), scan_step):\n",
        "        if i >= len(scans):\n",
        "            break\n",
        "        scan = scans[i]\n",
        "        angles = np.arange(len(scan)) * scan_params['angle_increment'] + scan_params['angle_min']\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        valid_angles = angles[valid_mask]\n",
        "        \n",
        "        x = valid_ranges * np.cos(valid_angles)\n",
        "        y = valid_ranges * np.sin(valid_angles)\n",
        "        all_x.extend(x)\n",
        "        all_y.extend(y)\n",
        "    \n",
        "    h = axes[1, 1].hexbin(all_x, all_y, gridsize=50, cmap='YlOrRd', mincnt=1, alpha=0.8)\n",
        "    axes[1, 1].scatter(0, 0, c='blue', s=150, marker='o', label='Robot', zorder=5, edgecolors='white', linewidths=2)\n",
        "    \n",
        "    # Draw safety circles\n",
        "    circle_danger = plt.Circle((0, 0), danger_zone, fill=False, color='red', \n",
        "                               linestyle='--', linewidth=2, label=f'Danger zone ({danger_zone}m)')\n",
        "    circle_warning = plt.Circle((0, 0), warning_zone, fill=False, color='orange', \n",
        "                                linestyle='--', linewidth=2, label=f'Warning zone ({warning_zone}m)')\n",
        "    axes[1, 1].add_patch(circle_danger)\n",
        "    axes[1, 1].add_patch(circle_warning)\n",
        "    \n",
        "    axes[1, 1].set_xlabel('X (meters)')\n",
        "    axes[1, 1].set_ylabel('Y (meters)')\n",
        "    axes[1, 1].set_title(f'Obstacle Density Map (Aggregated {sample_scans} scans)')\n",
        "    axes[1, 1].axis('equal')\n",
        "    axes[1, 1].legend(loc='upper right', fontsize=9)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    plt.colorbar(h, ax=axes[1, 1], label='Point Density')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('obstacle_detection.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Obstacle detection analysis saved to: obstacle_detection.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scans available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_6"
      },
      "source": [
        "### 6. Scan Timing and Quality Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality_metrics"
      },
      "outputs": [],
      "source": [
        "# Analyze scan frequency and data quality\n",
        "print(\"=\" * 60)\n",
        "print(\"SCAN TIMING AND QUALITY METRICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scan_timestamps) > 1:\n",
        "    # Timing analysis\n",
        "    intervals = np.diff(scan_timestamps)\n",
        "    frequencies = 1.0 / intervals\n",
        "    \n",
        "    expected_interval = scan_params['scan_time']\n",
        "    expected_freq = 1.0 / expected_interval if expected_interval > 0 else 10\n",
        "    \n",
        "    print(f\"\\n‚è±Ô∏è Timing Statistics:\")\n",
        "    print(f\"   Mean interval: {np.mean(intervals)*1000:.2f} ms\")\n",
        "    print(f\"   Std dev: {np.std(intervals)*1000:.2f} ms\")\n",
        "    print(f\"   Min/Max: [{np.min(intervals)*1000:.2f}, {np.max(intervals)*1000:.2f}] ms\")\n",
        "    print(f\"   Jitter (CV): {(np.std(intervals)/np.mean(intervals))*100:.2f}%\")\n",
        "    \n",
        "    print(f\"\\nüì° Frequency Statistics:\")\n",
        "    print(f\"   Mean frequency: {np.mean(frequencies):.2f} Hz\")\n",
        "    print(f\"   Expected frequency: {expected_freq:.2f} Hz\")\n",
        "    print(f\"   Frequency error: {abs(np.mean(frequencies) - expected_freq):.2f} Hz ({abs(np.mean(frequencies) - expected_freq)/expected_freq*100:.1f}%)\")\n",
        "    \n",
        "    # Dropped scans\n",
        "    dropped_threshold = expected_interval * 1.5\n",
        "    dropped_count = np.sum(intervals > dropped_threshold)\n",
        "    print(f\"\\n‚ö†Ô∏è Dropped Scans:\")\n",
        "    print(f\"   Potential drops: {dropped_count} ({100*dropped_count/len(intervals):.2f}%)\")\n",
        "    \n",
        "    # Quality metrics per scan\n",
        "    print(\"\\nüìä Computing quality metrics...\")\n",
        "    \n",
        "    quality_scores = []\n",
        "    noise_levels = []\n",
        "    \n",
        "    for scan in tqdm(scans):\n",
        "        valid_mask = np.isfinite(scan) & (scan >= scan_params['range_min']) & (scan <= scan_params['range_max'])\n",
        "        valid_ranges = scan[valid_mask]\n",
        "        \n",
        "        # Completeness score\n",
        "        completeness = np.sum(valid_mask) / len(scan)\n",
        "        \n",
        "        # Noise level (high-frequency changes)\n",
        "        if len(valid_ranges) > 1:\n",
        "            diffs = np.abs(np.diff(valid_ranges))\n",
        "            noise = np.mean(diffs)\n",
        "            noise_levels.append(noise)\n",
        "        else:\n",
        "            noise_levels.append(np.nan)\n",
        "        \n",
        "        # Combined quality score (simple: just completeness for now)\n",
        "        quality_scores.append(completeness)\n",
        "    \n",
        "    quality_scores = np.array(quality_scores)\n",
        "    noise_levels = np.array(noise_levels)\n",
        "    \n",
        "    print(f\"\\n‚úì Quality Metrics Summary:\")\n",
        "    print(f\"   Mean completeness: {np.mean(quality_scores)*100:.2f}%\")\n",
        "    print(f\"   Mean noise level: {np.nanmean(noise_levels):.4f} m\")\n",
        "    print(f\"   Overall quality score: {np.mean(quality_scores):.3f}/1.000\")\n",
        "    \n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    # Scan intervals over time\n",
        "    axes[0, 0].plot(scan_timestamps[1:], intervals * 1000, linewidth=1)\n",
        "    axes[0, 0].axhline(expected_interval * 1000, color='red', linestyle='--', \n",
        "                       label=f'Expected: {expected_interval*1000:.2f} ms')\n",
        "    axes[0, 0].axhline(dropped_threshold * 1000, color='orange', linestyle='--', \n",
        "                       label=f'Drop threshold: {dropped_threshold*1000:.2f} ms')\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Interval (ms)')\n",
        "    axes[0, 0].set_title('Inter-Scan Intervals Over Time')\n",
        "    axes[0, 0].legend(fontsize=9)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Frequency over time\n",
        "    axes[0, 1].plot(scan_timestamps[1:], frequencies, linewidth=1, color='green')\n",
        "    axes[0, 1].axhline(expected_freq, color='red', linestyle='--', \n",
        "                       label=f'Expected: {expected_freq:.2f} Hz')\n",
        "    axes[0, 1].set_xlabel('Time (seconds)')\n",
        "    axes[0, 1].set_ylabel('Frequency (Hz)')\n",
        "    axes[0, 1].set_title('Scan Frequency Over Time')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Quality score over time\n",
        "    axes[0, 2].plot(scan_timestamps, quality_scores, linewidth=1, color='purple')\n",
        "    axes[0, 2].axhline(np.mean(quality_scores), color='red', linestyle='--', \n",
        "                       label=f'Mean: {np.mean(quality_scores):.3f}')\n",
        "    axes[0, 2].set_xlabel('Time (seconds)')\n",
        "    axes[0, 2].set_ylabel('Quality Score (0-1)')\n",
        "    axes[0, 2].set_title('Data Quality Score Over Time')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    axes[0, 2].set_ylim([0, 1.05])\n",
        "    \n",
        "    # Interval histogram\n",
        "    axes[1, 0].hist(intervals * 1000, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[1, 0].axvline(expected_interval * 1000, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Expected: {expected_interval*1000:.2f} ms')\n",
        "    axes[1, 0].axvline(np.mean(intervals) * 1000, color='orange', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {np.mean(intervals)*1000:.2f} ms')\n",
        "    axes[1, 0].set_xlabel('Interval (ms)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Interval Distribution')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Frequency histogram\n",
        "    axes[1, 1].hist(frequencies, bins=30, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1, 1].axvline(expected_freq, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Expected: {expected_freq:.2f} Hz')\n",
        "    axes[1, 1].axvline(np.mean(frequencies), color='orange', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {np.mean(frequencies):.2f} Hz')\n",
        "    axes[1, 1].set_xlabel('Frequency (Hz)')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].set_title('Frequency Distribution')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Noise level over time\n",
        "    axes[1, 2].plot(scan_timestamps, noise_levels, linewidth=1, color='coral')\n",
        "    axes[1, 2].set_xlabel('Time (seconds)')\n",
        "    axes[1, 2].set_ylabel('Noise Level (m)')\n",
        "    axes[1, 2].set_title('Measurement Noise Over Time')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('timing_quality_metrics.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Timing and quality analysis saved to: timing_quality_metrics.png\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Insufficient data for timing analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_7"
      },
      "source": [
        "### 7. LiDAR Scan EDA Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Generate comprehensive LiDAR Scan EDA summary\n",
        "print(\"=\" * 60)\n",
        "print(\"LIDAR SCAN EDA - SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(scans) > 0 and len(all_ranges) > 0:\n",
        "    lidar_scan_summary = f\"\"\"\n",
        "üîµ LIDAR SCAN EDA SUMMARY\n",
        "{'='*60}\n",
        "\n",
        "BASIC INFORMATION:\n",
        "  ‚Ä¢ Topic: {lidar_topic}\n",
        "  ‚Ä¢ Total Scans: {len(scans)}\n",
        "  ‚Ä¢ Duration: {scan_timestamps[-1]:.2f} seconds ({scan_timestamps[-1]/60:.2f} minutes)\n",
        "  ‚Ä¢ Average Scan Rate: {len(scans)/scan_timestamps[-1]:.2f} Hz\n",
        "  ‚Ä¢ Expected Scan Rate: {expected_freq:.2f} Hz\n",
        "\n",
        "SENSOR CONFIGURATION:\n",
        "  ‚Ä¢ Points per Scan: {len(scans[0])}\n",
        "  ‚Ä¢ Angular Coverage: {np.rad2deg(scan_params['angle_max'] - scan_params['angle_min']):.1f}¬∞\n",
        "  ‚Ä¢ Angular Resolution: {np.rad2deg(scan_params['angle_increment']):.3f}¬∞\n",
        "  ‚Ä¢ Range: [{scan_params['range_min']:.2f}m - {scan_params['range_max']:.2f}m]\n",
        "  ‚Ä¢ Scan Time: {scan_params['scan_time']:.4f} s\n",
        "\n",
        "RANGE STATISTICS:\n",
        "  ‚Ä¢ Total Valid Points: {len(all_ranges):,}\n",
        "  ‚Ä¢ Mean Range: {np.mean(all_ranges):.2f} m\n",
        "  ‚Ä¢ Median Range: {np.median(all_ranges):.2f} m\n",
        "  ‚Ä¢ Std Deviation: {np.std(all_ranges):.2f} m\n",
        "  ‚Ä¢ Min/Max Range: [{np.min(all_ranges):.2f}m, {np.max(all_ranges):.2f}m]\n",
        "\n",
        "DATA QUALITY:\n",
        "  ‚Ä¢ Average Completeness: {np.mean(quality_scores)*100:.2f}%\n",
        "  ‚Ä¢ Average Valid Points per Scan: {np.mean(valid_counts):.1f} ({100*np.mean(valid_counts)/len(scans[0]):.1f}%)\n",
        "  ‚Ä¢ Average Invalid Points per Scan: {np.mean(invalid_counts):.1f} ({100*np.mean(invalid_counts)/len(scans[0]):.1f}%)\n",
        "  ‚Ä¢ Overall Quality Score: {np.mean(quality_scores):.3f}/1.000\n",
        "  ‚Ä¢ Average Noise Level: {np.nanmean(noise_levels):.4f} m\n",
        "\n",
        "TIMING ANALYSIS:\n",
        "  ‚Ä¢ Mean Scan Interval: {np.mean(intervals)*1000:.2f} ms\n",
        "  ‚Ä¢ Timing Jitter (CV): {(np.std(intervals)/np.mean(intervals))*100:.2f}%\n",
        "  ‚Ä¢ Potential Dropped Scans: {dropped_count} ({100*dropped_count/len(intervals):.2f}%)\n",
        "  ‚Ä¢ Frequency Error: {abs(np.mean(frequencies) - expected_freq)/expected_freq*100:.2f}%\n",
        "\n",
        "OBSTACLE DETECTION:\n",
        "  ‚Ä¢ Closest Obstacle (Overall): {np.nanmin(closest_obstacles):.2f} m\n",
        "  ‚Ä¢ Average Closest Obstacle: {np.nanmean(closest_obstacles):.2f} m\n",
        "  ‚Ä¢ Danger Zone Detections (<{danger_zone}m): {np.sum(np.array(danger_counts) > 0)} scans ({100*np.sum(np.array(danger_counts) > 0)/len(scans):.1f}%)\n",
        "  ‚Ä¢ Warning Zone Detections ({danger_zone}-{warning_zone}m): {np.sum(np.array(warning_counts) > 0)} scans ({100*np.sum(np.array(warning_counts) > 0)/len(scans):.1f}%)\n",
        "\n",
        "ANGULAR COVERAGE:\n",
        "  ‚Ä¢ Angles with Valid Data: {int(np.sum(valid_angles))}/{num_angles} ({100*np.sum(valid_angles)/num_angles:.1f}%)\n",
        "  ‚Ä¢ Potential Blind Spots (<{blind_threshold}% valid): {len(blind_angles)} angles\n",
        "\n",
        "OUTPUT FILES:\n",
        "  ‚Ä¢ sample_scans_polar.png\n",
        "  ‚Ä¢ sample_scans_cartesian.png\n",
        "  ‚Ä¢ scan_properties.png\n",
        "  ‚Ä¢ range_distribution.png\n",
        "  ‚Ä¢ angular_coverage.png\n",
        "  ‚Ä¢ obstacle_detection.png\n",
        "  ‚Ä¢ timing_quality_metrics.png\n",
        "\"\"\"\n",
        "\n",
        "    print(lidar_scan_summary)\n",
        "\n",
        "    # Save summary report\n",
        "    with open('lidar_scan_eda_summary.txt', 'w') as f:\n",
        "        f.write(lidar_scan_summary)\n",
        "\n",
        "    print(\"\\n‚úÖ LiDAR Scan EDA summary saved to: lidar_scan_eda_summary.txt\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scan data available for summary\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
