{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A00785001/TC5035/blob/main/001_rosbag_camera_reader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AS0OLHkOAYV"
      },
      "source": [
        "# ROS Bag Camera Data Extractor & MobileNet V2 Preprocessor\n",
        "Extract camera images from ROS bags and prepare them for MobileNet V2 feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWucIWc2OAYW"
      },
      "source": [
        "## ğŸ“‹ Notebook Overview\n",
        "\n",
        "### Purpose\n",
        "This notebook extracts camera images from ROS bag files and prepares them for **MobileNet V2 feature extraction** as part of a sensor fusion pipeline for loop closure detection on a Waveshare Jetbot.\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "**Input:** ROS bag file containing camera images (from topics like `/camera/image_raw` or `/csi_cam_0/image_raw`)\n",
        "\n",
        "**Processing Steps:**\n",
        "1. **Extract** images from ROS bag messages\n",
        "2. **Convert** from ROS image format (bgr8/rgb8/mono8) to RGB numpy arrays\n",
        "3. **Resize** all images to 224Ã—224 pixels (MobileNet V2 input requirement)\n",
        "4. **Save** as high-quality JPEGs (quality=95)\n",
        "5. **Preserve** ROS timestamps for temporal alignment with LiDAR data\n",
        "6. **Generate** comprehensive metadata (CSV + JSON)\n",
        "\n",
        "**Output:** Organized dataset ready for visual feature extraction\n",
        "```\n",
        "processed_images/\n",
        "â”œâ”€â”€ img_00000.jpg          # 224Ã—224 RGB images\n",
        "â”œâ”€â”€ img_00001.jpg\n",
        "â”œâ”€â”€ img_00002.jpg\n",
        "â”œâ”€â”€ ...\n",
        "â”œâ”€â”€ metadata.csv           # Per-image timestamps, dimensions, file sizes\n",
        "â””â”€â”€ dataset_info.json      # Dataset-level metadata\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Why This Preprocessing?\n",
        "\n",
        "**For Sensor Fusion:**\n",
        "- **Camera + LiDAR fusion** for robust loop closure detection\n",
        "- **Temporal alignment** using preserved ROS timestamps\n",
        "- **Visual features** (from MobileNet V2) + **Geometric features** (from LiDAR 1D CNN)\n",
        "\n",
        "**224Ã—224 Image Size:**\n",
        "- âœ“ MobileNet V2 requirement (non-negotiable)\n",
        "- âœ“ Standard for ImageNet-pretrained models\n",
        "- âœ“ Good balance: detail vs computational cost\n",
        "- âœ“ Embedded-friendly (Jetbot compatible)\n",
        "\n",
        "**JPEG Format:**\n",
        "- âœ“ Excellent compression (~45KB per image)\n",
        "- âœ“ Fast to decode\n",
        "- âœ“ Universally supported\n",
        "- âœ“ Quality=95 preserves visual details\n",
        "\n",
        "**Timestamp Preservation:**\n",
        "- âœ“ Critical for aligning with LiDAR scans\n",
        "- âœ“ Enables temporal synchronization (Â±50ms typical threshold)\n",
        "- âœ“ Required for sequence-based loop closure\n",
        "- âœ“ Maintains temporal ordering\n",
        "\n",
        "---\n",
        "\n",
        "### Pipeline Context\n",
        "\n",
        "This notebook is **Part 1** of a multi-stage pipeline:\n",
        "\n",
        "1. **This Notebook** â†’ Extract & preprocess camera images\n",
        "2. **LiDAR Notebook** â†’ Extract & preprocess LaserScan data\n",
        "3. **Feature Extraction** â†’ MobileNet V2 (visual) + 1D CNN (geometric)\n",
        "4. **Sensor Fusion** â†’ Combine features for loop closure detection\n",
        "\n",
        "---\n",
        "\n",
        "### Key Technical Details\n",
        "\n",
        "**Image Format:**\n",
        "- **Color Space:** RGB (converted from ROS bgr8)\n",
        "- **Pixel Range:** [0, 255] uint8 (NOT normalized yet)\n",
        "- **Size:** 224Ã—224Ã—3\n",
        "- **Note:** Model-specific normalization happens during feature extraction\n",
        "\n",
        "**Metadata CSV Columns:**\n",
        "- `filename` - Image filename (img_XXXXX.jpg)\n",
        "- `timestamp` - ROS timestamp (float seconds with microsecond precision)\n",
        "- `timestamp_sec` - Integer seconds part\n",
        "- `timestamp_nsec` - Integer nanoseconds part  \n",
        "- `frame_id` - Sequential frame number\n",
        "- `original_width` - Original image width before resize\n",
        "- `original_height` - Original image height before resize\n",
        "- `file_size_kb` - Processed JPEG file size\n",
        "\n",
        "**Dataset Info JSON Contains:**\n",
        "- Bag file metadata (duration, start/end times)\n",
        "- Camera info (topic, encoding, frame rate)\n",
        "- Processing parameters (resize method, JPEG quality)\n",
        "- Statistics (total images, FPS, dataset size)\n",
        "- Sensor fusion notes (coordinate frames, alignment info)\n",
        "\n",
        "---\n",
        "\n",
        "### Requirements\n",
        "\n",
        "**Python Packages:**\n",
        "- `rosbags` - Pure Python ROS bag reader (Colab-safe, no ROS installation needed)\n",
        "- `opencv-python` - Image processing\n",
        "- `pillow` - Image I/O and resizing\n",
        "- `numpy` - Array operations\n",
        "- `matplotlib` - Visualization\n",
        "- `tqdm` - Progress bars\n",
        "\n",
        "**Input:**\n",
        "- ROS bag file with camera topic (bgr8, rgb8, or mono8 encoding)\n",
        "\n",
        "**Output:**\n",
        "- ~45KB per image (JPEG)\n",
        "- Typical dataset: 1000 images â‰ˆ 45MB\n",
        "\n",
        "---\n",
        "\n",
        "### Usage\n",
        "\n",
        "1. **Run install cell** - Install dependencies\n",
        "2. **Upload ROS bag** - Use Colab file upload\n",
        "3. **Select camera topic** - From list of available topics\n",
        "4. **Run extraction** - Progress bar shows status\n",
        "5. **Run processing** - Resize and save images\n",
        "6. **Download output** - `processed_images/` folder with all data\n",
        "\n",
        "---\n",
        "\n",
        "### Notes\n",
        "\n",
        "âš  **This notebook uses `rosbags` library** (NOT `bagpy`)\n",
        "- Pure Python, no ROS installation needed\n",
        "- Colab-compatible\n",
        "- Fast and memory-efficient\n",
        "- Does NOT create huge CSV files\n",
        "\n",
        "âœ… **Images are NOT normalized here**\n",
        "- Pixel range stays [0, 255]\n",
        "- Different models need different preprocessing\n",
        "- Normalization happens during feature extraction\n",
        "\n",
        "âœ… **Timestamps are critical**\n",
        "- Used for camera-LiDAR temporal alignment\n",
        "- Microsecond precision preserved\n",
        "- Essential for sensor fusion pipeline\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! ğŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMonP7hAOAYX"
      },
      "source": [
        "## Section 1: Extract Images from ROS Bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjLfR6iEOAYX",
        "outputId": "8dad0608-615f-48f8-9e07-8a0d4343dacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/137.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/119.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/753.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m747.5/753.1 kB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet rosbags opencv-python pillow numpy matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp7u_QcWOAYY",
        "outputId": "b8dcaabc-341f-45ea-93ce-07ade6f51090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from rosbags.rosbag1 import Reader\n",
        "from rosbags.typesys import Stores, get_typestore\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkDVPMtKOAYY",
        "outputId": "352a661f-65e2-43ac-fac7-384279699cc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Typestore initialized for ROS1\n"
          ]
        }
      ],
      "source": [
        "# Initialize typestore for ROS1 message deserialization\n",
        "typestore = get_typestore(Stores.ROS1_NOETIC)\n",
        "print(\"Typestore initialized for ROS1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6fefdeb",
        "outputId": "29b3aff9-6f52-4d9a-d802-2f04ad517094"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1a1DqWWs2NZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12abee2-af5e-4faf-add8-ced3887a9126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed directory to: /content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data\n"
          ]
        }
      ],
      "source": [
        "# Specify the path to your ROS bag file within the shared folder\n",
        "data_path = \"/content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/\"\n",
        "ROS_bgas_path = \"ROS_bags/\"\n",
        "\n",
        "# Change to the specified subfolder\n",
        "os.chdir(data_path)\n",
        "\n",
        "print(f\"Changed directory to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROS_bag_subfolder = 'session_20251016_133216'\n",
        "print(f\"Using session: {ROS_bag_subfolder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYkArHt8o69",
        "outputId": "b76a0f2a-b24f-485c-8966-04178d81dce0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using session: session_20251016_133216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_name = 'session_data.bag'\n",
        "bag_file = data_path +_+ ROS_bgas_path + ROS_bag_subfolder + '/'+ bag_name"
      ],
      "metadata": {
        "id": "2lNm6nozOFKW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvDNVeXMOAYZ",
        "outputId": "a7dd8342-a886-4fb7-e750-34374cebcc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening bag file: /content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/ROS_bags/session_20251016_133216/session_data.bag\n",
            "\n",
            "Bag duration: 463.17 seconds\n",
            "Start time: 1760649872.66\n",
            "Message count: 98205\n",
            "\n",
            "Topics in bag:\n",
            "  /csi_cam_0/camera_info (sensor_msgs/msg/CameraInfo)\n",
            "  /constraint_list (visualization_msgs/msg/MarkerArray)\n",
            "  /csi_cam_0/image_raw/compressed (sensor_msgs/msg/CompressedImage)\n",
            "  /imu (sensor_msgs/msg/Imu)\n",
            "  /odom (nav_msgs/msg/Odometry)\n",
            "  /scan (sensor_msgs/msg/LaserScan)\n",
            "  /tf (tf2_msgs/msg/TFMessage)\n",
            "  /tf (tf2_msgs/msg/TFMessage)\n",
            "  /tf (tf2_msgs/msg/TFMessage)\n",
            "  /tf (tf2_msgs/msg/TFMessage)\n",
            "  /submap_list (cartographer_ros_msgs/msg/SubmapList)\n",
            "  /trajectory_node_list (visualization_msgs/msg/MarkerArray)\n",
            "  /cmd_vel (geometry_msgs/msg/Twist)\n",
            "\n",
            "âœ“ Bag file opened successfully\n"
          ]
        }
      ],
      "source": [
        "# Read the bag file and list topics\n",
        "print(f\"Opening bag file: {bag_file}\")\n",
        "bag_path = Path(bag_file)\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    # Get bag info\n",
        "    print(f\"\\nBag duration: {reader.duration / 1e9:.2f} seconds\")\n",
        "    print(f\"Start time: {reader.start_time / 1e9:.2f}\")\n",
        "    print(f\"Message count: {reader.message_count}\")\n",
        "\n",
        "    # List all topics\n",
        "    print(f\"\\nTopics in bag:\")\n",
        "    for connection in reader.connections:\n",
        "        print(f\"  {connection.topic} ({connection.msgtype})\")\n",
        "\n",
        "print(f\"\\nâœ“ Bag file opened successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L7EA66ROAYZ",
        "outputId": "9ca95477-8463-4304-945d-536db209fc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using camera topic: /csi_cam_0/image_raw/compressed\n"
          ]
        }
      ],
      "source": [
        "# Specify your camera topic (common topics: /camera/image_raw, /usb_cam/image_raw)\n",
        "camera_topic = \"/csi_cam_0/image_raw/compressed\"\n",
        "print(f\"Using camera topic: {camera_topic}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2yhgN3SOAYZ",
        "outputId": "f5c3a96e-7be5-4f3d-bd2a-e27d30f60d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporary extraction directory: temp_extracted_images/\n"
          ]
        }
      ],
      "source": [
        "# Create temporary extraction directory\n",
        "TEMP_EXTRACTION_DIR = \"temp_extracted_images\"\n",
        "os.makedirs(TEMP_EXTRACTION_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Temporary extraction directory: {TEMP_EXTRACTION_DIR}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiVYtJR9OAYa",
        "outputId": "348e12f4-adbb-4cbb-ffe6-58ae94ba747a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image conversion function ready\n"
          ]
        }
      ],
      "source": [
        "# Function to convert ROS Image message to numpy array\n",
        "def ros_image_to_numpy(msg):\n",
        "    \"\"\"\n",
        "    Convert ROS Image message to numpy array.\n",
        "    Handles common encodings: bgr8, rgb8, mono8, etc.\n",
        "    \"\"\"\n",
        "    height = msg.height\n",
        "    width = msg.width\n",
        "    encoding = msg.encoding\n",
        "\n",
        "    # Convert bytes to numpy array\n",
        "    if encoding == 'bgr8':\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width, 3)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    elif encoding == 'rgb8':\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width, 3)\n",
        "    elif encoding == 'mono8':\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Convert to RGB\n",
        "    elif encoding == 'bgra8':\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width, 4)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n",
        "    elif encoding == 'rgba8':\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width, 4)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "    else:\n",
        "        print(f\"Warning: Unsupported encoding {encoding}, attempting bgr8\")\n",
        "        img = np.frombuffer(msg.data, dtype=np.uint8).reshape(height, width, 3)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return img\n",
        "\n",
        "print(\"Image conversion function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zv0aciKOAYa",
        "outputId": "c3621ef4-34b7-479f-a3d7-38f62d09652f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting images from topic: /csi_cam_0/image_raw/compressed\n",
            "Found topic: /csi_cam_0/image_raw/compressed\n",
            "Message type: sensor_msgs/msg/CompressedImage\n",
            "Total messages: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 5022.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "Error processing frame 0: 'sensor_msgs__msg__CompressedImage' object has no attribute 'height'\n",
            "\n",
            "âœ“ Extracted 0 images to temp_extracted_images/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract images from bag\n",
        "print(f\"Extracting images from topic: {camera_topic}\")\n",
        "\n",
        "extracted_images = []\n",
        "image_count = 0\n",
        "\n",
        "with Reader(bag_path) as reader:\n",
        "    # Get connections for our topic\n",
        "    connections = [c for c in reader.connections if c.topic == camera_topic]\n",
        "\n",
        "    if not connections:\n",
        "        print(f\"ERROR: Topic {camera_topic} not found in bag!\")\n",
        "        print(\"Available topics:\")\n",
        "        for c in reader.connections:\n",
        "            print(f\"  - {c.topic}\")\n",
        "    else:\n",
        "        print(f\"Found topic: {camera_topic}\")\n",
        "        print(f\"Message type: {connections[0].msgtype}\")\n",
        "\n",
        "        # Count messages first for progress bar\n",
        "        total_messages = sum(1 for _ in reader.messages(connections=connections))\n",
        "        print(f\"Total messages: {total_messages}\")\n",
        "\n",
        "        # Extract images\n",
        "        for connection, timestamp, rawdata in tqdm(reader.messages(connections=connections),\n",
        "                                                     total=total_messages,\n",
        "                                                     desc=\"Extracting images\"):\n",
        "            # Deserialize message\n",
        "            msg = typestore.deserialize_ros1(rawdata, connection.msgtype)\n",
        "\n",
        "            # Convert to numpy image\n",
        "            try:\n",
        "                img_array = ros_image_to_numpy(msg)\n",
        "\n",
        "                # Save temporarily\n",
        "                img_filename = f\"frame_{image_count:06d}.jpg\"\n",
        "                img_path = os.path.join(TEMP_EXTRACTION_DIR, img_filename)\n",
        "\n",
        "                # Save using PIL\n",
        "                img_pil = Image.fromarray(img_array)\n",
        "                img_pil.save(img_path, 'JPEG', quality=95)\n",
        "\n",
        "                # Store metadata\n",
        "                extracted_images.append({\n",
        "                    'filename': img_filename,\n",
        "                    'timestamp': timestamp / 1e9,  # Convert nanoseconds to seconds\n",
        "                    'timestamp_nsec': timestamp,\n",
        "                    'frame_id': image_count,\n",
        "                    'original_width': msg.width,\n",
        "                    'original_height': msg.height,\n",
        "                    'encoding': msg.encoding\n",
        "                })\n",
        "\n",
        "                image_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing frame {image_count}: {e}\")\n",
        "                continue\n",
        "\n",
        "print(f\"\\nâœ“ Extracted {image_count} images to {TEMP_EXTRACTION_DIR}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rv8IioROAYa"
      },
      "outputs": [],
      "source": [
        "# Display sample extracted images\n",
        "print(f\"Displaying sample extracted images...\")\n",
        "\n",
        "sample_images = []\n",
        "for i in range(min(6, len(extracted_images))):\n",
        "    img_path = os.path.join(TEMP_EXTRACTION_DIR, extracted_images[i]['filename'])\n",
        "    img = Image.open(img_path)\n",
        "    sample_images.append(np.array(img))\n",
        "\n",
        "if sample_images:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, img in enumerate(sample_images):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Frame {i}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nOriginal image shape: {sample_images[0].shape}\")\n",
        "    print(f\"Total images extracted: {len(extracted_images)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evu7_W9wOAYa"
      },
      "source": [
        "## Section 2: Process Images for MobileNet V2 Feature Extraction\n",
        "\n",
        "### Purpose\n",
        "This section prepares camera images for visual feature extraction using MobileNet V2 (or other vision models). The goal is to resize, normalize, and organize images for later feature extraction in the sensor fusion pipeline.\n",
        "\n",
        "### What This Section Does\n",
        "1. **Resizes** all images to 224Ã—224 (MobileNet V2 input size)\n",
        "2. **Saves** processed images as high-quality JPEGs\n",
        "3. **Preserves** ROS timestamps for temporal alignment with LiDAR\n",
        "4. **Generates** comprehensive metadata (CSV + JSON) for reproducibility\n",
        "5. **Organizes** data in a simple, flat folder structure\n",
        "\n",
        "### Processing Pipeline\n",
        "```\n",
        "ROS Bag Images â†’ Resize to 224Ã—224 â†’ Save as JPEG â†’ Record Metadata\n",
        "```\n",
        "\n",
        "### Output Structure\n",
        "```\n",
        "processed_images/\n",
        "â”œâ”€â”€ img_00000.jpg          # Resized to 224Ã—224, RGB format\n",
        "â”œâ”€â”€ img_00001.jpg\n",
        "â”œâ”€â”€ img_00002.jpg\n",
        "â”œâ”€â”€ ...\n",
        "â”œâ”€â”€ metadata.csv           # Per-image data (timestamps, sizes)\n",
        "â””â”€â”€ dataset_info.json      # Dataset-level metadata\n",
        "```\n",
        "\n",
        "### File Formats\n",
        "\n",
        "**Images (*.jpg):**\n",
        "- Format: JPEG, quality=95\n",
        "- Size: 224Ã—224Ã—3 (RGB)\n",
        "- Pixel range: [0, 255] (uint8)\n",
        "- Note: MobileNet V2 preprocessing (scaling to [-1,1]) happens during feature extraction\n",
        "\n",
        "**metadata.csv:**\n",
        "| Column | Type | Description |\n",
        "|--------|------|-------------|\n",
        "| filename | str | Image filename (img_XXXXX.jpg) |\n",
        "| timestamp | float | ROS timestamp (seconds) |\n",
        "| timestamp_sec | int | Timestamp seconds part |\n",
        "| timestamp_nsec | int | Timestamp nanoseconds part |\n",
        "| frame_id | int | Sequential frame number |\n",
        "| original_width | int | Original image width |\n",
        "| original_height | int | Original image height |\n",
        "| file_size_kb | float | Processed file size in KB |\n",
        "\n",
        "**dataset_info.json Structure:**\n",
        "```json\n",
        "{\n",
        "  \"dataset_metadata\": {\n",
        "    \"creation_date\": \"2024-10-20T15:30:00Z\",\n",
        "    \"ros_bag_info\": {\n",
        "      \"source_file\": \"data.bag\",\n",
        "      \"bag_duration_sec\": 300.5,\n",
        "      \"bag_start_time\": 1634567890.0,\n",
        "      \"bag_end_time\": 1634568190.5\n",
        "    },\n",
        "    \"camera_info\": {\n",
        "      \"topic\": \"/camera/image_raw\",\n",
        "      \"encoding\": \"bgr8\",\n",
        "      \"frame_rate_hz\": 30.0,\n",
        "      \"camera_model\": \"unknown\"\n",
        "    },\n",
        "    \"processing_info\": {\n",
        "      \"target_size\": [224, 224],\n",
        "      \"resize_method\": \"LANCZOS\",\n",
        "      \"jpeg_quality\": 95,\n",
        "      \"total_images_processed\": 1500,\n",
        "      \"processing_script\": \"rosbag_camera_extractor_v1.ipynb\"\n",
        "    }\n",
        "  },\n",
        "  \"sensor_fusion_notes\": {\n",
        "    \"time_synchronization\": \"ROS timestamps preserved\",\n",
        "    \"coordinate_frame\": \"camera_optical_frame\",\n",
        "    \"notes\": \"Images ready for MobileNet V2 feature extraction\"\n",
        "  },\n",
        "  \"statistics\": {\n",
        "    \"fps_actual\": 29.8,\n",
        "    \"dropped_frames\": 5,\n",
        "    \"avg_file_size_kb\": 45.5,\n",
        "    \"total_dataset_size_mb\": 68.25\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Processing Rationale\n",
        "\n",
        "**Why 224Ã—224?**\n",
        "- MobileNet V2 input requirement (can't change)\n",
        "- Standard size for ImageNet-pretrained models\n",
        "- Good balance of detail vs computational cost\n",
        "- Embedded-friendly (Jetbot can handle this size)\n",
        "\n",
        "**Why JPEG format?**\n",
        "- Good compression ratio (~45KB per image)\n",
        "- Universally supported\n",
        "- Fast to decode\n",
        "- Quality=95 preserves visual details\n",
        "\n",
        "**Why preserve original timestamps?**\n",
        "- Critical for sensor fusion with LiDAR\n",
        "- Enables temporal alignment (Â±50ms typical)\n",
        "- Required for loop closure detection\n",
        "- Maintains temporal ordering of sequence\n",
        "\n",
        "**Why no preprocessing yet?**\n",
        "- Different models need different preprocessing:\n",
        "  - MobileNet V2: [-1, 1] scaling\n",
        "  - DINOv2: [0, 1] + ImageNet normalization\n",
        "  - ResNet: Different normalization\n",
        "- Keep raw [0, 255] for flexibility\n",
        "- Apply model-specific preprocessing during feature extraction\n",
        "\n",
        "### Important Notes\n",
        "- **No normalization yet**: Images are saved in [0, 255] range. Apply `preprocess_input()` during feature extraction.\n",
        "- **Timestamps preserved**: Critical for aligning with LiDAR data in sensor fusion.\n",
        "- **No train/val/test split**: Data is unlabeled; splitting happens after labeling.\n",
        "- **Lossless workflow**: Original images preserved in ROS bag; can regenerate if needed.\n",
        "\n",
        "### Next Step: Feature Extraction\n",
        "After running this section, images are ready for MobileNet V2 (or DINOv2) feature extraction. See the feature extraction notebook for the next pipeline stage.\n",
        "\n",
        "### Usage Example (for next pipeline stage)\n",
        "```python\n",
        "# Load processed images for feature extraction\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "# Load an image\n",
        "img = Image.open(f\"processed_images/{metadata.iloc[0]['filename']}\")\n",
        "# Shape: (224, 224, 3), Range: [0, 255]\n",
        "\n",
        "# Apply MobileNet V2 preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0, 255] â†’ [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])  # ~[-1, 1]\n",
        "])\n",
        "img_tensor = preprocess(img).unsqueeze(0)\n",
        "# Shape: (1, 3, 224, 224), Range: ~[-1, 1]\n",
        "\n",
        "# Extract features (in feature extraction notebook)\n",
        "# from torchvision import models\n",
        "# mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "# features = mobilenet.features(img_tensor)  # Output: (1, 1280, 7, 7)\n",
        "# features = features.mean([2, 3])  # Global average pooling â†’ (1, 1280)\n",
        "```\n",
        "\n",
        "### MobileNet V2 Requirements (for reference)\n",
        "- Input size: 224Ã—224Ã—3\n",
        "- Preprocessing: Scale to [-1, 1] using `tf.keras.applications.mobilenet_v2.preprocess_input`\n",
        "- Output: 1280D feature vector (before final classification layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqvFTknVOAYb"
      },
      "outputs": [],
      "source": [
        "# MobileNet V2 input parameters\n",
        "IMG_SIZE = 224\n",
        "TARGET_SIZE = (IMG_SIZE, IMG_SIZE)\n",
        "JPEG_QUALITY = 95\n",
        "\n",
        "print(f\"Target image size: {TARGET_SIZE}\")\n",
        "print(f\"JPEG quality: {JPEG_QUALITY}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKgCwPXnOAYb"
      },
      "outputs": [],
      "source": [
        "# Create output folder structure\n",
        "OUTPUT_DIR = \"processed_images\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Created output directory: {OUTPUT_DIR}/\")\n",
        "print(\"Images will be saved with metadata (CSV + JSON)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuvQGAMPOAYb"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess and save images\n",
        "def preprocess_and_save_image(img_path, save_path, target_size=(224, 224), quality=95):\n",
        "    \"\"\"\n",
        "    Preprocess image for MobileNet V2:\n",
        "    1. Load image\n",
        "    2. Resize to 224x224\n",
        "    3. Save as JPEG\n",
        "\n",
        "    Returns: (success, original_width, original_height, file_size_kb)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image\n",
        "        img = Image.open(img_path)\n",
        "        original_size = img.size  # (width, height)\n",
        "\n",
        "        # Resize to target size\n",
        "        img_resized = img.resize(target_size, Image.LANCZOS)\n",
        "\n",
        "        # Save preprocessed image\n",
        "        img_resized.save(save_path, 'JPEG', quality=quality)\n",
        "\n",
        "        # Get file size\n",
        "        file_size_kb = os.path.getsize(save_path) / 1024\n",
        "\n",
        "        return True, original_size[0], original_size[1], file_size_kb\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        return False, 0, 0, 0\n",
        "\n",
        "print(\"Preprocessing function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UISKeusWOAYb"
      },
      "outputs": [],
      "source": [
        "# Process all images and collect metadata\n",
        "print(\"Processing images...\")\n",
        "\n",
        "metadata_list = []\n",
        "total_size_kb = 0\n",
        "failed_count = 0\n",
        "\n",
        "for i, img_info in enumerate(tqdm(extracted_images, desc=\"Processing\")):\n",
        "    # Generate output filename\n",
        "    output_filename = f\"img_{i:05d}.jpg\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "    # Source path\n",
        "    source_path = os.path.join(TEMP_EXTRACTION_DIR, img_info['filename'])\n",
        "\n",
        "    # Process and save image\n",
        "    success, orig_w, orig_h, file_size = preprocess_and_save_image(\n",
        "        source_path, output_path, TARGET_SIZE, JPEG_QUALITY\n",
        "    )\n",
        "\n",
        "    if success:\n",
        "        # Parse timestamp\n",
        "        timestamp = img_info['timestamp']\n",
        "        timestamp_sec = int(timestamp)\n",
        "        timestamp_nsec = int((timestamp - timestamp_sec) * 1e9)\n",
        "\n",
        "        # Store metadata\n",
        "        metadata_list.append({\n",
        "            'filename': output_filename,\n",
        "            'timestamp': timestamp,\n",
        "            'timestamp_sec': timestamp_sec,\n",
        "            'timestamp_nsec': timestamp_nsec,\n",
        "            'frame_id': i,\n",
        "            'original_width': img_info['original_width'],\n",
        "            'original_height': img_info['original_height'],\n",
        "            'file_size_kb': round(file_size, 2)\n",
        "        })\n",
        "\n",
        "        total_size_kb += file_size\n",
        "    else:\n",
        "        failed_count += 1\n",
        "\n",
        "print(f\"\\nâœ“ Completed: {len(metadata_list)} images processed\")\n",
        "print(f\"âœ— Failed: {failed_count} images\")\n",
        "print(f\"Total dataset size: {total_size_kb/1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARtHVmNbOAYb"
      },
      "outputs": [],
      "source": [
        "# Save CSV metadata (per-image data)\n",
        "csv_path = os.path.join(OUTPUT_DIR, 'metadata.csv')\n",
        "\n",
        "with open(csv_path, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['filename', 'timestamp', 'timestamp_sec', 'timestamp_nsec',\n",
        "                  'frame_id', 'original_width', 'original_height', 'file_size_kb']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for row in metadata_list:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"âœ“ CSV metadata saved: {csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SeuzKHaOAYb"
      },
      "outputs": [],
      "source": [
        "# Calculate statistics\n",
        "if metadata_list:\n",
        "    avg_file_size = sum(m['file_size_kb'] for m in metadata_list) / len(metadata_list)\n",
        "\n",
        "    # Calculate FPS\n",
        "    if len(metadata_list) > 1:\n",
        "        time_diff = metadata_list[-1]['timestamp'] - metadata_list[0]['timestamp']\n",
        "        fps_actual = len(metadata_list) / time_diff if time_diff > 0 else 0\n",
        "    else:\n",
        "        fps_actual = 0\n",
        "        time_diff = 0\n",
        "else:\n",
        "    avg_file_size = 0\n",
        "    fps_actual = 0\n",
        "    time_diff = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD8yRPxROAYb"
      },
      "outputs": [],
      "source": [
        "# Create JSON metadata (dataset-level info)\n",
        "json_metadata = {\n",
        "    \"dataset_metadata\": {\n",
        "        \"creation_date\": datetime.now().isoformat(),\n",
        "        \"ros_bag_info\": {\n",
        "            \"source_file\": bag_file,\n",
        "            \"bag_duration_sec\": time_diff,\n",
        "            \"bag_start_time\": metadata_list[0]['timestamp'] if metadata_list else 0,\n",
        "            \"bag_end_time\": metadata_list[-1]['timestamp'] if metadata_list else 0\n",
        "        },\n",
        "        \"camera_info\": {\n",
        "            \"topic\": camera_topic,\n",
        "            \"encoding\": extracted_images[0]['encoding'] if extracted_images else 'unknown',\n",
        "            \"frame_rate_hz\": round(fps_actual, 2),\n",
        "            \"camera_model\": \"unknown\"\n",
        "        },\n",
        "        \"processing_info\": {\n",
        "            \"target_size\": [IMG_SIZE, IMG_SIZE],\n",
        "            \"resize_method\": \"LANCZOS\",\n",
        "            \"jpeg_quality\": JPEG_QUALITY,\n",
        "            \"total_images_processed\": len(metadata_list),\n",
        "            \"processing_script\": \"rosbag_camera_extractor_v2_rosbags.ipynb\"\n",
        "        }\n",
        "    },\n",
        "    \"sensor_fusion_notes\": {\n",
        "        \"time_synchronization\": \"ROS timestamps preserved in CSV\",\n",
        "        \"coordinate_frame\": \"camera_optical_frame\",\n",
        "        \"notes\": \"Images ready for MobileNet V2 feature extraction. Apply preprocess_input() before inference.\"\n",
        "    },\n",
        "    \"statistics\": {\n",
        "        \"fps_actual\": round(fps_actual, 2),\n",
        "        \"dropped_frames\": failed_count,\n",
        "        \"avg_file_size_kb\": round(avg_file_size, 2),\n",
        "        \"total_dataset_size_mb\": round(total_size_kb / 1024, 2)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save JSON metadata\n",
        "json_path = os.path.join(OUTPUT_DIR, 'dataset_info.json')\n",
        "with open(json_path, 'w') as jsonfile:\n",
        "    json.dump(json_metadata, jsonfile, indent=2)\n",
        "\n",
        "print(f\"âœ“ JSON metadata saved: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJqSdJyfOAYb"
      },
      "outputs": [],
      "source": [
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET PROCESSING COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Output directory: {OUTPUT_DIR}/\")\n",
        "print(f\"Total images: {len(metadata_list)}\")\n",
        "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Dataset size: {total_size_kb/1024:.2f} MB\")\n",
        "print(f\"Average FPS: {fps_actual:.2f}\" if fps_actual > 0 else \"FPS: N/A\")\n",
        "print(f\"\\nMetadata files:\")\n",
        "print(f\"  - {csv_path}\")\n",
        "print(f\"  - {json_path}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVzGmKEWOAYc"
      },
      "source": [
        "## Output Structure Documentation\n",
        "\n",
        "### Complete File Structure\n",
        "\n",
        "After running this notebook, you will have the following directory structure:\n",
        "\n",
        "```\n",
        "processed_images/\n",
        "â”œâ”€â”€ img_00000.jpg              # First processed image\n",
        "â”œâ”€â”€ img_00001.jpg              # Second processed image\n",
        "â”œâ”€â”€ img_00002.jpg              # Third processed image\n",
        "â”œâ”€â”€ img_00003.jpg\n",
        "â”œâ”€â”€ ...\n",
        "â”œâ”€â”€ img_NNNNN.jpg              # Last processed image (N = total images - 1)\n",
        "â”œâ”€â”€ metadata.csv               # Per-image metadata (timestamps, sizes, etc.)\n",
        "â””â”€â”€ dataset_info.json          # Dataset-level metadata and statistics\n",
        "```\n",
        "\n",
        "### Image Files (*.jpg)\n",
        "\n",
        "**Format Specifications:**\n",
        "- **File format**: JPEG\n",
        "- **Naming convention**: `img_XXXXX.jpg` (5-digit zero-padded sequential numbering)\n",
        "- **Image dimensions**: 224 Ã— 224 pixels (exactly)\n",
        "- **Color channels**: 3 (RGB)\n",
        "- **Pixel data type**: uint8\n",
        "- **Pixel value range**: [0, 255]\n",
        "- **JPEG quality**: 95 (high quality, minimal compression artifacts)\n",
        "- **Typical file size**: 40-50 KB per image\n",
        "- **Color space**: RGB (NOT BGR - already converted from ROS)\n",
        "\n",
        "**Important Notes:**\n",
        "- Images are **already in RGB** format (converted from ROS bgr8 encoding)\n",
        "- Images are **resized** from original dimensions to 224Ã—224\n",
        "- Pixel values are **NOT normalized** - still in [0, 255] range\n",
        "- **Normalization happens later** during feature extraction\n",
        "\n",
        "---\n",
        "\n",
        "### metadata.csv Structure\n",
        "\n",
        "**Purpose**: Contains per-image metadata for temporal alignment and traceability.\n",
        "\n",
        "**Format**: CSV with header row\n",
        "\n",
        "**Columns (8 total):**\n",
        "\n",
        "| Column Name | Data Type | Example Value | Description | Required for Fusion? |\n",
        "|-------------|-----------|---------------|-------------|---------------------|\n",
        "| `filename` | string | `img_00042.jpg` | Image filename in processed_images/ | âœ… Yes |\n",
        "| `timestamp` | float | `1634567890.123456` | ROS timestamp in seconds (float with microsecond precision) | âœ… Yes (critical!) |\n",
        "| `timestamp_sec` | integer | `1634567890` | Timestamp seconds part (integer) | âš ï¸ Optional |\n",
        "| `timestamp_nsec` | integer | `123456000` | Timestamp nanoseconds part (0-999999999) | âš ï¸ Optional |\n",
        "| `frame_id` | integer | `42` | Sequential frame number (0-indexed) | âš ï¸ Optional |\n",
        "| `original_width` | integer | `640` | Width of original image from ROS bag (before resize) | âš ï¸ Optional |\n",
        "| `original_height` | integer | `480` | Height of original image from ROS bag (before resize) | âš ï¸ Optional |\n",
        "| `file_size_kb` | float | `45.23` | Size of processed JPEG file in kilobytes | âš ï¸ Optional |\n",
        "\n",
        "**Critical Column Explanations:**\n",
        "\n",
        "**`timestamp` (Most Important!):**\n",
        "- **Format**: Floating point seconds since epoch\n",
        "- **Precision**: Microsecond-level (6 decimal places typical)\n",
        "- **Purpose**: Enables temporal alignment with LiDAR data\n",
        "- **Example**: `1634567890.123456` means October 18, 2021, 12:04:50.123456 UTC\n",
        "- **Usage**: Used to match camera frames with LiDAR scans (Â±50ms tolerance typical)\n",
        "- **Source**: Extracted directly from ROS message header\n",
        "\n",
        "**`timestamp_sec` and `timestamp_nsec`:**\n",
        "- **Relationship**: `timestamp = timestamp_sec + (timestamp_nsec / 1e9)`\n",
        "- **Purpose**: Alternative representation for systems that need integer timestamps\n",
        "- **ROS Standard**: This is how ROS internally stores timestamps\n",
        "- **Example**: `1634567890` sec + `123456000` nsec = `1634567890.123456` sec\n",
        "\n",
        "**Sample CSV Content:**\n",
        "```csv\n",
        "filename,timestamp,timestamp_sec,timestamp_nsec,frame_id,original_width,original_height,file_size_kb\n",
        "img_00000.jpg,1634567890.123456,1634567890,123456000,0,640,480,45.23\n",
        "img_00001.jpg,1634567890.156789,1634567890,156789000,1,640,480,44.87\n",
        "img_00002.jpg,1634567890.190122,1634567890,190122000,2,640,480,46.01\n",
        "img_00003.jpg,1634567890.223455,1634567890,223455000,3,640,480,45.67\n",
        "```\n",
        "\n",
        "**How to Load in Python:**\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "# Access data\n",
        "print(metadata.head())\n",
        "print(f\"Total images: {len(metadata)}\")\n",
        "print(f\"Time span: {metadata['timestamp'].max() - metadata['timestamp'].min():.2f} seconds\")\n",
        "\n",
        "# Get specific image info\n",
        "img_info = metadata.iloc[0]  # First image\n",
        "print(f\"Filename: {img_info['filename']}\")\n",
        "print(f\"Timestamp: {img_info['timestamp']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### dataset_info.json Structure\n",
        "\n",
        "**Purpose**: Contains dataset-level metadata, processing parameters, and statistics.\n",
        "\n",
        "**Format**: JSON with nested structure\n",
        "\n",
        "**Complete Structure with Descriptions:**\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"dataset_metadata\": {\n",
        "    \"creation_date\": \"2024-10-23T15:30:45.123456\",  // ISO 8601 timestamp of processing\n",
        "    \n",
        "    \"ros_bag_info\": {\n",
        "      \"source_file\": \"robot_data_2024_10_23.bag\",    // Original bag filename\n",
        "      \"bag_duration_sec\": 300.5,                       // Total duration in seconds\n",
        "      \"bag_start_time\": 1634567890.0,                  // First image timestamp\n",
        "      \"bag_end_time\": 1634568190.5                     // Last image timestamp\n",
        "    },\n",
        "    \n",
        "    \"camera_info\": {\n",
        "      \"topic\": \"/camera/image_raw\",                   // ROS topic name\n",
        "      \"encoding\": \"bgr8\",                              // Original ROS encoding (bgr8, rgb8, mono8, etc.)\n",
        "      \"frame_rate_hz\": 29.8,                           // Calculated actual frame rate\n",
        "      \"camera_model\": \"unknown\"                        // Camera model (if available in bag)\n",
        "    },\n",
        "    \n",
        "    \"processing_info\": {\n",
        "      \"target_size\": [224, 224],                       // Output image dimensions [width, height]\n",
        "      \"resize_method\": \"LANCZOS\",                      // PIL resize method used\n",
        "      \"jpeg_quality\": 95,                              // JPEG compression quality (0-100)\n",
        "      \"total_images_processed\": 1500,                  // Number of successfully processed images\n",
        "      \"processing_script\": \"rosbag_camera_extractor_v2_rosbags.ipynb\"  // Notebook version\n",
        "    }\n",
        "  },\n",
        "  \n",
        "  \"sensor_fusion_notes\": {\n",
        "    \"time_synchronization\": \"ROS timestamps preserved in CSV\",  // Temporal alignment method\n",
        "    \"coordinate_frame\": \"camera_optical_frame\",                 // ROS coordinate frame\n",
        "    \"notes\": \"Images ready for MobileNet V2 feature extraction. Apply preprocess_input() before inference.\"\n",
        "  },\n",
        "  \n",
        "  \"statistics\": {\n",
        "    \"fps_actual\": 29.8,                                // Calculated frame rate\n",
        "    \"dropped_frames\": 5,                               // Number of failed extractions\n",
        "    \"avg_file_size_kb\": 45.5,                          // Average JPEG file size\n",
        "    \"total_dataset_size_mb\": 68.25                     // Total size of all images\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Key Field Explanations:**\n",
        "\n",
        "- **`bag_duration_sec`**: Duration between first and last image (NOT total bag duration)\n",
        "- **`frame_rate_hz`**: Calculated as: `num_images / bag_duration_sec`\n",
        "- **`encoding`**: Original ROS image encoding (already converted to RGB in output)\n",
        "- **`resize_method`**: LANCZOS provides high-quality downsampling\n",
        "- **`dropped_frames`**: Images that failed to process (corrupted, wrong format, etc.)\n",
        "\n",
        "**How to Load in Python:**\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Load dataset info\n",
        "with open('processed_images/dataset_info.json', 'r') as f:\n",
        "    info = json.load(f)\n",
        "\n",
        "# Access data\n",
        "print(f\"Total images: {info['dataset_metadata']['processing_info']['total_images_processed']}\")\n",
        "print(f\"Frame rate: {info['statistics']['fps_actual']} Hz\")\n",
        "print(f\"Dataset size: {info['statistics']['total_dataset_size_mb']} MB\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Temporal Alignment Guide\n",
        "\n",
        "**Using Timestamps for Sensor Fusion:**\n",
        "\n",
        "The `timestamp` column in `metadata.csv` is critical for aligning camera and LiDAR data.\n",
        "\n",
        "**Example Alignment Scenario:**\n",
        "```\n",
        "Camera timestamps (30 Hz):  [1.000, 1.033, 1.066, 1.100, 1.133, ...]\n",
        "LiDAR timestamps  (10 Hz):  [1.000,        1.100,        1.200, ...]\n",
        "```\n",
        "\n",
        "**Alignment Strategy:**\n",
        "1. **Nearest Neighbor**: Match each LiDAR scan to closest camera frame (Â±50ms typical)\n",
        "2. **Interpolation**: Create virtual frames between camera captures\n",
        "3. **Synchronized Only**: Only use frames where both sensors captured within Â±10ms\n",
        "\n",
        "**Python Example for Nearest Neighbor Matching:**\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load metadata\n",
        "cam_meta = pd.read_csv('processed_images/metadata.csv')\n",
        "lid_meta = pd.read_csv('processed_lidar/metadata.csv')\n",
        "\n",
        "# Find nearest camera frame for each LiDAR scan\n",
        "aligned_pairs = []\n",
        "for _, lidar_row in lid_meta.iterrows():\n",
        "    lidar_time = lidar_row['timestamp']\n",
        "    \n",
        "    # Find nearest camera timestamp\n",
        "    time_diffs = np.abs(cam_meta['timestamp'] - lidar_time)\n",
        "    nearest_idx = time_diffs.argmin()\n",
        "    \n",
        "    # Check if within acceptable threshold (e.g., 50ms)\n",
        "    if time_diffs.iloc[nearest_idx] < 0.050:  # 50ms threshold\n",
        "        aligned_pairs.append({\n",
        "            'camera_file': cam_meta.iloc[nearest_idx]['filename'],\n",
        "            'lidar_file': lidar_row['filename'],\n",
        "            'time_diff_ms': time_diffs.iloc[nearest_idx] * 1000\n",
        "        })\n",
        "\n",
        "print(f\"Aligned {len(aligned_pairs)} camera-LiDAR pairs\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Data Validation Checklist\n",
        "\n",
        "After processing, verify your dataset:\n",
        "\n",
        "**File Structure:**\n",
        "- âœ… `processed_images/` directory exists\n",
        "- âœ… All images named `img_XXXXX.jpg` with sequential numbering\n",
        "- âœ… `metadata.csv` exists and has 8 columns\n",
        "- âœ… `dataset_info.json` exists and is valid JSON\n",
        "\n",
        "**Image Quality:**\n",
        "- âœ… All images are exactly 224Ã—224 pixels\n",
        "- âœ… All images are RGB (3 channels)\n",
        "- âœ… No corrupted images (can open with PIL)\n",
        "- âœ… Pixel values in [0, 255] range\n",
        "\n",
        "**Metadata Integrity:**\n",
        "- âœ… Number of CSV rows matches number of .jpg files\n",
        "- âœ… All filenames in CSV exist as files\n",
        "- âœ… Timestamps are monotonically increasing\n",
        "- âœ… No duplicate timestamps (or very few)\n",
        "- âœ… No NaN or missing values in critical columns\n",
        "\n",
        "**Quick Validation Script:**\n",
        "```python\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Check files\n",
        "images = glob('processed_images/img_*.jpg')\n",
        "metadata = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "print(f\"Images found: {len(images)}\")\n",
        "print(f\"Metadata rows: {len(metadata)}\")\n",
        "assert len(images) == len(metadata), \"Mismatch between images and metadata!\"\n",
        "\n",
        "# Check first image\n",
        "img = Image.open('processed_images/img_00000.jpg')\n",
        "assert img.size == (224, 224), f\"Wrong size: {img.size}\"\n",
        "assert img.mode == 'RGB', f\"Wrong mode: {img.mode}\"\n",
        "\n",
        "# Check timestamps\n",
        "assert metadata['timestamp'].is_monotonic_increasing, \"Timestamps not in order!\"\n",
        "assert not metadata['timestamp'].isna().any(), \"Missing timestamps!\"\n",
        "\n",
        "print(\"âœ“ All validation checks passed!\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009bleZUOAYc"
      },
      "outputs": [],
      "source": [
        "# Display sample processed images\n",
        "from glob import glob\n",
        "\n",
        "sample_processed = []\n",
        "output_images = sorted(glob(os.path.join(OUTPUT_DIR, '*.jpg')))[:6]\n",
        "\n",
        "for img_path in output_images:\n",
        "    img = Image.open(img_path)\n",
        "    sample_processed.append(np.array(img))\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "axes = axes.flatten()\n",
        "for i, img in enumerate(sample_processed):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"Processed - {img.shape}\")\n",
        "    axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIVubTdVOAYc"
      },
      "outputs": [],
      "source": [
        "# Display actual CSV content (first 10 rows)\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"METADATA.CSV CONTENT (First 10 rows)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(df.head(10).to_string(index=False))\n",
        "\n",
        "print(f\"\\n... ({len(df)} total rows)\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jal4tiLxOAYc"
      },
      "outputs": [],
      "source": [
        "# Display actual JSON content (formatted)\n",
        "import json\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET_INFO.JSON CONTENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "print(json.dumps(json_data, indent=2))\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxamCnN8OAYc"
      },
      "outputs": [],
      "source": [
        "# Generate file tree visualization\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATED FILE STRUCTURE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{OUTPUT_DIR}/\")\n",
        "\n",
        "# Get all files\n",
        "all_files = sorted(glob(os.path.join(OUTPUT_DIR, '*')))\n",
        "jpg_files = [f for f in all_files if f.endswith('.jpg')]\n",
        "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
        "json_files = [f for f in all_files if f.endswith('.json')]\n",
        "\n",
        "# Show first few and last few images\n",
        "if len(jpg_files) > 10:\n",
        "    display_jpgs = jpg_files[:5] + ['...'] + jpg_files[-5:]\n",
        "else:\n",
        "    display_jpgs = jpg_files\n",
        "\n",
        "for f in display_jpgs:\n",
        "    if f == '...':\n",
        "        print(f\"â”œâ”€â”€ ...\")\n",
        "        print(f\"â”œâ”€â”€ ... ({len(jpg_files) - 10} more images)\")\n",
        "        print(f\"â”œâ”€â”€ ...\")\n",
        "    else:\n",
        "        fname = os.path.basename(f)\n",
        "        fsize = os.path.getsize(f) / 1024  # KB\n",
        "        print(f\"â”œâ”€â”€ {fname:<20} ({fsize:>6.1f} KB)\")\n",
        "\n",
        "# Show metadata files\n",
        "for f in csv_files:\n",
        "    fname = os.path.basename(f)\n",
        "    fsize = os.path.getsize(f) / 1024\n",
        "    print(f\"â”œâ”€â”€ {fname:<20} ({fsize:>6.1f} KB)\")\n",
        "\n",
        "for f in json_files:\n",
        "    fname = os.path.basename(f)\n",
        "    fsize = os.path.getsize(f) / 1024\n",
        "    print(f\"â””â”€â”€ {fname:<20} ({fsize:>6.1f} KB)\")\n",
        "\n",
        "print(f\"\\nTotal: {len(jpg_files)} images + 2 metadata files\")\n",
        "print(f\"Total size: {sum(os.path.getsize(f) for f in all_files) / (1024*1024):.2f} MB\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpJj_QikOAYc"
      },
      "outputs": [],
      "source": [
        "# Example: Load and inspect metadata.csv\n",
        "import pandas as pd\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "# Display info\n",
        "print(\"Metadata Overview:\")\n",
        "print(metadata.head())\n",
        "print(f\"\\nTotal images: {len(metadata)}\")\n",
        "print(f\"Time span: {metadata['timestamp'].max() - metadata['timestamp'].min():.2f} seconds\")\n",
        "\n",
        "# Get specific image info\n",
        "img_info = metadata.iloc[0]\n",
        "print(f\"\\nFirst image:\")\n",
        "print(f\"  Filename: {img_info['filename']}\")\n",
        "print(f\"  Timestamp: {img_info['timestamp']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thJYG9shOAYc"
      },
      "outputs": [],
      "source": [
        "# Example: Load and inspect dataset_info.json\n",
        "import json\n",
        "\n",
        "# Load dataset info\n",
        "with open('processed_images/dataset_info.json', 'r') as f:\n",
        "    info = json.load(f)\n",
        "\n",
        "# Display key information\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Total images: {info['dataset_metadata']['processing_info']['total_images_processed']}\")\n",
        "print(f\"Frame rate: {info['statistics']['fps_actual']} Hz\")\n",
        "print(f\"Dataset size: {info['statistics']['total_dataset_size_mb']} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qadn7-LUOAYc"
      },
      "outputs": [],
      "source": [
        "# Example: Temporal alignment with LiDAR (nearest neighbor)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# This example assumes you have processed_lidar/ from the LiDAR notebook\n",
        "# Load metadata\n",
        "try:\n",
        "    cam_meta = pd.read_csv('processed_images/metadata.csv')\n",
        "    lid_meta = pd.read_csv('processed_lidar/metadata.csv')\n",
        "\n",
        "    # Find nearest camera frame for each LiDAR scan\n",
        "    aligned_pairs = []\n",
        "    for _, lidar_row in lid_meta.iterrows():\n",
        "        lidar_time = lidar_row['timestamp']\n",
        "\n",
        "        # Find nearest camera timestamp\n",
        "        time_diffs = np.abs(cam_meta['timestamp'] - lidar_time)\n",
        "        nearest_idx = time_diffs.argmin()\n",
        "\n",
        "        # Check if within acceptable threshold (e.g., 50ms)\n",
        "        if time_diffs.iloc[nearest_idx] < 0.050:\n",
        "            aligned_pairs.append({\n",
        "                'camera_file': cam_meta.iloc[nearest_idx]['filename'],\n",
        "                'lidar_file': lidar_row['filename'],\n",
        "                'time_diff_ms': time_diffs.iloc[nearest_idx] * 1000\n",
        "            })\n",
        "\n",
        "    print(f\"Aligned {len(aligned_pairs)} camera-LiDAR pairs\")\n",
        "    if aligned_pairs:\n",
        "        print(f\"Average time difference: {np.mean([p['time_diff_ms'] for p in aligned_pairs]):.2f} ms\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"LiDAR metadata not found. Run LiDAR preprocessing notebook first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGwvioPnOAYd"
      },
      "outputs": [],
      "source": [
        "# Example: Data validation script\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "print(\"Running validation checks...\\n\")\n",
        "\n",
        "# Check files\n",
        "images = glob('processed_images/img_*.jpg')\n",
        "metadata = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "print(f\"âœ“ Images found: {len(images)}\")\n",
        "print(f\"âœ“ Metadata rows: {len(metadata)}\")\n",
        "assert len(images) == len(metadata), \"ERROR: Mismatch between images and metadata!\"\n",
        "\n",
        "# Check first image\n",
        "img = Image.open('processed_images/img_00000.jpg')\n",
        "assert img.size == (224, 224), f\"ERROR: Wrong size: {img.size}\"\n",
        "assert img.mode == 'RGB', f\"ERROR: Wrong mode: {img.mode}\"\n",
        "print(f\"âœ“ Image dimensions correct: {img.size}\")\n",
        "print(f\"âœ“ Image mode correct: {img.mode}\")\n",
        "\n",
        "# Check timestamps\n",
        "assert metadata['timestamp'].is_monotonic_increasing, \"ERROR: Timestamps not in order!\"\n",
        "assert not metadata['timestamp'].isna().any(), \"ERROR: Missing timestamps!\"\n",
        "print(f\"âœ“ Timestamps are monotonically increasing\")\n",
        "print(f\"âœ“ No missing timestamps\")\n",
        "\n",
        "print(\"\\nâœ… All validation checks passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0-2cyTmOAYd"
      },
      "outputs": [],
      "source": [
        "# Clean up temporary extraction directory\n",
        "import shutil\n",
        "\n",
        "print(f\"\\nCleaning up temporary files...\")\n",
        "shutil.rmtree(TEMP_EXTRACTION_DIR)\n",
        "print(f\"âœ“ Removed {TEMP_EXTRACTION_DIR}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAVuHeTsOAYd"
      },
      "source": [
        "## Next Steps: Feature Extraction with MobileNet V2\n",
        "\n",
        "Your images are now ready for MobileNet V2 feature extraction. When you're ready to extract features:\n",
        "\n",
        "```python\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load MobileNet V2 (without top classification layer)\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(\n",
        "    mobilenet.features,\n",
        "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    torch.nn.Flatten()\n",
        ")\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Load metadata\n",
        "df = pd.read_csv('processed_images/metadata.csv')\n",
        "\n",
        "# Preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract features for each image\n",
        "features_list = []\n",
        "for idx, row in df.iterrows():\n",
        "    img_path = f\"processed_images/{row['filename']}\"\n",
        "    \n",
        "    # Load and preprocess\n",
        "    img = Image.open(img_path)\n",
        "    img_tensor = preprocess(img).unsqueeze(0)\n",
        "    \n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(img_tensor)\n",
        "    \n",
        "    features_list.append(features[0].numpy())\n",
        "\n",
        "# features_list now contains 1280D feature vectors for sensor fusion\n",
        "```\n",
        "\n",
        "**Note:** The preprocessing transforms scale pixels to approximately [-1, 1] as required by MobileNet V2."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}