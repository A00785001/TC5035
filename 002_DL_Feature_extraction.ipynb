{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A00785001/TC5035/blob/main/002_DL_Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKNVEsY6g-kk"
      },
      "source": [
        "# Feature Extraction Notebook - Camera & LiDAR\n",
        "\n",
        "## Overview\n",
        "This notebook extracts deep features from preprocessed camera images and LiDAR scans for sensor fusion and loop closure detection. It implements two parallel feature extraction branches:\n",
        "\n",
        "1. **Visual Branch**: MobileNet V2 → 1280D features\n",
        "2. **Geometric Branch**: 1D CNN → 256D features\n",
        "\n",
        "Features are saved in HDF5 format for efficient storage and retrieval.\n",
        "\n",
        "## Prerequisites\n",
        "- Preprocessed images from camera notebook (`processed_images/`)\n",
        "- Preprocessed LiDAR scans from LiDAR notebook (`processed_lidar/`)\n",
        "- Both datasets with aligned timestamps in metadata\n",
        "\n",
        "## Pipeline Architecture\n",
        "```\n",
        "Camera Images (224×224) → MobileNet V2 → 1280D → L2 Norm → Visual Features\n",
        "LiDAR Scans (360,)      → 1D CNN      → 256D  → L2 Norm → Geometric Features\n",
        "                                                ↓\n",
        "                                      HDF5 Feature Database\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpDFF3Fvg-kl"
      },
      "source": [
        "## Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SERlAYCyg-kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0aeb26-39d6-4edb-cb37-fc3d42c51144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet --upgrade torch torchvision\n",
        "!pip install --quiet h5py pandas numpy matplotlib tqdm pillow\n",
        "\n",
        "print(\"✓ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G04R92lxg-km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24948c0-b623-45d5-816c-97978985f914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFy20u_Sg-km"
      },
      "source": [
        "## Section 1: Camera Feature Extraction (Visual Branch)\n",
        "\n",
        "### Purpose\n",
        "Extract high-level visual features from camera images using MobileNet V2, a lightweight CNN pretrained on ImageNet. These features capture semantic and appearance information crucial for place recognition.\n",
        "\n",
        "### Architecture: MobileNet V2\n",
        "- **Input**: 224×224×3 RGB images\n",
        "- **Preprocessing**: Scale pixels from [0,255] to [-1,1]\n",
        "- **Backbone**: MobileNet V2 (pretrained on ImageNet)\n",
        "- **Feature Layer**: Before final classification layer\n",
        "- **Raw Output**: 1280D feature vector\n",
        "- **Post-processing**: L2 normalization\n",
        "- **Final Output**: 1280D normalized feature vector\n",
        "\n",
        "### What This Section Does\n",
        "1. Load preprocessed images from `processed_images/`\n",
        "2. Apply MobileNet V2 preprocessing ([-1,1] scaling)\n",
        "3. Extract 1280D features using pretrained MobileNet V2\n",
        "4. L2 normalize features for similarity comparison\n",
        "5. Save to HDF5 with timestamps and metadata\n",
        "\n",
        "### Why MobileNet V2?\n",
        "- Lightweight: ~3.5M parameters\n",
        "- Fast inference: ~20-30ms on CPU\n",
        "- Robust features proven for place recognition\n",
        "- Pretrained on ImageNet (1000 classes)\n",
        "- Efficient for embedded systems (Jetbot)\n",
        "\n",
        "### Output Format\n",
        "Features saved to HDF5: `features.h5/camera/features` [N, 1280]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yk2BshKY7Nz",
        "outputId": "44d98980-6d82-4e9d-b945-5608fb69831f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = '20251016_133216'"
      ],
      "metadata": {
        "id": "jFd78iPppoYL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcDjH-ksY7Nz",
        "outputId": "1269000f-e8a1-43f3-91ed-bc54fbf1967d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using session: 20251016_133216\n",
            "Changed directory to: /content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot/session_20251016_133216\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using session: {session}\")\n",
        "\n",
        "# Specify the path to your ROS bag file within the shared folder\n",
        "data_path = \"/content/drive/MyDrive/DATA/Artificial_Intelligence/MNA-V/Subjects/TC5035-Proyecto_Integrador/TC5035.data/jetbot\"\n",
        "working_folder = data_path + '/session_' + session\n",
        "bag_name = 'session_data.bag'\n",
        "\n",
        "\n",
        "# Change to the specified subfolder\n",
        "os.chdir(working_folder)\n",
        "print(f\"Changed directory to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xBN_w3RDg-km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74d7ef7-3542-4693-9bbd-912b4f95258d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Camera input directory: processed_images\n",
            "Batch size: 32\n"
          ]
        }
      ],
      "source": [
        "# Configuration for camera feature extraction\n",
        "CAMERA_INPUT_DIR = \"processed_images\"\n",
        "CAMERA_BATCH_SIZE = 32  # Adjust based on GPU memory\n",
        "\n",
        "print(f\"Camera input directory: {CAMERA_INPUT_DIR}\")\n",
        "print(f\"Batch size: {CAMERA_BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hUgEIcs9g-km",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "80bb81b4-d5df-48e6-82d0-5ee1c28388b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 camera images\n",
            "\n",
            "First few entries:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        filename     timestamp  timestamp_sec  timestamp_nsec  frame_id  \\\n",
              "0  img_00000.jpg  1.760650e+09     1760649872       670898199         0   \n",
              "1  img_00001.jpg  1.760650e+09     1760649872       671323537         1   \n",
              "2  img_00002.jpg  1.760650e+09     1760649872       684802532         2   \n",
              "3  img_00003.jpg  1.760650e+09     1760649872       685315847         3   \n",
              "4  img_00004.jpg  1.760650e+09     1760649872       685797214         4   \n",
              "\n",
              "   original_width  original_height  file_size_kb  \n",
              "0             640              480         20.54  \n",
              "1             640              480         20.52  \n",
              "2             640              480         20.54  \n",
              "3             640              480         20.54  \n",
              "4             640              480         20.57  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b71909f5-d3ee-4fc8-9b98-07b0ff8cfeac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>timestamp_sec</th>\n",
              "      <th>timestamp_nsec</th>\n",
              "      <th>frame_id</th>\n",
              "      <th>original_width</th>\n",
              "      <th>original_height</th>\n",
              "      <th>file_size_kb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img_00000.jpg</td>\n",
              "      <td>1.760650e+09</td>\n",
              "      <td>1760649872</td>\n",
              "      <td>670898199</td>\n",
              "      <td>0</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>20.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img_00001.jpg</td>\n",
              "      <td>1.760650e+09</td>\n",
              "      <td>1760649872</td>\n",
              "      <td>671323537</td>\n",
              "      <td>1</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>20.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img_00002.jpg</td>\n",
              "      <td>1.760650e+09</td>\n",
              "      <td>1760649872</td>\n",
              "      <td>684802532</td>\n",
              "      <td>2</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>20.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img_00003.jpg</td>\n",
              "      <td>1.760650e+09</td>\n",
              "      <td>1760649872</td>\n",
              "      <td>685315847</td>\n",
              "      <td>3</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>20.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img_00004.jpg</td>\n",
              "      <td>1.760650e+09</td>\n",
              "      <td>1760649872</td>\n",
              "      <td>685797214</td>\n",
              "      <td>4</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>20.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b71909f5-d3ee-4fc8-9b98-07b0ff8cfeac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b71909f5-d3ee-4fc8-9b98-07b0ff8cfeac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b71909f5-d3ee-4fc8-9b98-07b0ff8cfeac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8f3dca2b-7f07-462c-84d4-e14c6bd5bfe2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f3dca2b-7f07-462c-84d4-e14c6bd5bfe2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8f3dca2b-7f07-462c-84d4-e14c6bd5bfe2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "camera_metadata",
              "summary": "{\n  \"name\": \"camera_metadata\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"img_00083.jpg\",\n          \"img_00053.jpg\",\n          \"img_00070.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 136.75033692999992,\n        \"min\": 1760649872.6708982,\n        \"max\": 1760650316.2304108,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          1760650233.811396,\n          1760650124.3913095,\n          1760650172.175593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 136,\n        \"min\": 1760649872,\n        \"max\": 1760650316,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          1760650065,\n          1760649976,\n          1760650143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp_nsec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 278637331,\n        \"min\": 2012252,\n        \"max\": 991785287,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          811396121,\n          391309738,\n          175592660\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frame_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 640,\n        \"max\": 640,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          640\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 480,\n        \"max\": 480,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          480\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_size_kb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.6325489338070973,\n        \"min\": 12.97,\n        \"max\": 23.24,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          16.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Load camera metadata\n",
        "camera_metadata = pd.read_csv(os.path.join(CAMERA_INPUT_DIR, 'metadata.csv'))\n",
        "\n",
        "print(f\"Found {len(camera_metadata)} camera images\")\n",
        "print(f\"\\nFirst few entries:\")\n",
        "camera_metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tfS_9NeHg-km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66d7f1a-9dbb-48f2-f38f-e8dcbd892bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MobileNet V2...\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 90.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ MobileNet V2 loaded on cpu\n",
            "Feature dimension: 1280D\n"
          ]
        }
      ],
      "source": [
        "# Load MobileNet V2 (pretrained on ImageNet)\n",
        "print(\"Loading MobileNet V2...\")\n",
        "\n",
        "# Load pretrained model\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Remove the final classification layer to get features\n",
        "# MobileNet V2 structure: features → classifier\n",
        "# We want the output of 'features' (before classifier)\n",
        "feature_extractor = nn.Sequential(\n",
        "    mobilenet.features,\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "# Move to GPU if available\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"✓ MobileNet V2 loaded on {device}\")\n",
        "print(f\"Feature dimension: 1280D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m3GJmnlTg-kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a822686a-d701-4250-806d-0292c050fa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNet V2 preprocessing pipeline ready\n"
          ]
        }
      ],
      "source": [
        "# Define MobileNet V2 preprocessing\n",
        "# Input: [0, 255] RGB → Output: [-1, 1] normalized\n",
        "mobilenet_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0, 255] → [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
        "                        std=[0.229, 0.224, 0.225])     # Scale to ~[-1, 1]\n",
        "])\n",
        "\n",
        "print(\"MobileNet V2 preprocessing pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "frw9BTwog-kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929fb00b-64d8-49ce-a7bb-5cfa07db3e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting camera features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Camera batches: 100%|██████████| 4/4 [00:30<00:00,  7.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Extracted camera features: (100, 1280)\n",
            "Feature dimension: 1280D\n",
            "Number of images: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract camera features\n",
        "print(\"Extracting camera features...\")\n",
        "\n",
        "camera_features_list = []\n",
        "camera_timestamps_list = []\n",
        "camera_filenames_list = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for idx in tqdm(range(0, len(camera_metadata), CAMERA_BATCH_SIZE), desc=\"Camera batches\"):\n",
        "        batch_meta = camera_metadata.iloc[idx:idx+CAMERA_BATCH_SIZE]\n",
        "\n",
        "        # Load batch of images\n",
        "        batch_images = []\n",
        "        batch_timestamps = []\n",
        "        batch_filenames = []\n",
        "\n",
        "        for _, row in batch_meta.iterrows():\n",
        "            img_path = os.path.join(CAMERA_INPUT_DIR, row['filename'])\n",
        "\n",
        "            try:\n",
        "                # Load and preprocess image\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_tensor = mobilenet_transform(img)\n",
        "                batch_images.append(img_tensor)\n",
        "                batch_timestamps.append(row['timestamp'])\n",
        "                batch_filenames.append(row['filename'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(batch_images) == 0:\n",
        "            continue\n",
        "\n",
        "        # Stack into batch tensor\n",
        "        batch_tensor = torch.stack(batch_images).to(device)\n",
        "\n",
        "        # Extract features\n",
        "        features = feature_extractor(batch_tensor)\n",
        "\n",
        "        # L2 normalize features\n",
        "        features = F.normalize(features, p=2, dim=1)\n",
        "\n",
        "        # Move to CPU and store\n",
        "        camera_features_list.append(features.cpu().numpy())\n",
        "        camera_timestamps_list.extend(batch_timestamps)\n",
        "        camera_filenames_list.extend(batch_filenames)\n",
        "\n",
        "# Concatenate all batches\n",
        "camera_features = np.vstack(camera_features_list)\n",
        "camera_timestamps = np.array(camera_timestamps_list)\n",
        "camera_filenames = np.array(camera_filenames_list)\n",
        "\n",
        "print(f\"\\n✓ Extracted camera features: {camera_features.shape}\")\n",
        "print(f\"Feature dimension: {camera_features.shape[1]}D\")\n",
        "print(f\"Number of images: {camera_features.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rTJxr9EEg-kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf417fc-e435-4c5c-f7b0-dd45a9ee8311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Camera Feature Statistics:\n",
            "  Shape: (100, 1280)\n",
            "  Mean: 0.0196\n",
            "  Std: 0.0199\n",
            "  Min: 0.0000\n",
            "  Max: 0.1544\n",
            "  L2 norms: mean=1.0000, std=0.000000\n",
            "  (should be ~1.0 after L2 normalization)\n"
          ]
        }
      ],
      "source": [
        "# Verify feature properties\n",
        "print(\"Camera Feature Statistics:\")\n",
        "print(f\"  Shape: {camera_features.shape}\")\n",
        "print(f\"  Mean: {camera_features.mean():.4f}\")\n",
        "print(f\"  Std: {camera_features.std():.4f}\")\n",
        "print(f\"  Min: {camera_features.min():.4f}\")\n",
        "print(f\"  Max: {camera_features.max():.4f}\")\n",
        "\n",
        "# Check L2 normalization (should be ~1.0)\n",
        "norms = np.linalg.norm(camera_features, axis=1)\n",
        "print(f\"  L2 norms: mean={norms.mean():.4f}, std={norms.std():.6f}\")\n",
        "print(f\"  (should be ~1.0 after L2 normalization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-LBtt8g-kn"
      },
      "source": [
        "## Section 2: LiDAR Feature Extraction (Geometric Branch)\n",
        "\n",
        "### Purpose\n",
        "Extract geometric features from LiDAR scans using a custom 1D CNN. These features capture spatial structure and geometry information complementary to visual features.\n",
        "\n",
        "### Architecture: 1D CNN\n",
        "- **Input**: 360 normalized distance values [0, 1]\n",
        "- **Architecture**:\n",
        "  - Conv1D(1→64, kernel=5) + ReLU + BatchNorm\n",
        "  - Conv1D(64→128, kernel=5) + ReLU + BatchNorm\n",
        "  - Conv1D(128→256, kernel=3) + ReLU + BatchNorm\n",
        "  - Conv1D(256→256, kernel=3) + ReLU + BatchNorm\n",
        "  - Global Average Pooling\n",
        "- **Parameters**: ~350K\n",
        "- **Raw Output**: 256D feature vector\n",
        "- **Post-processing**: L2 normalization\n",
        "- **Final Output**: 256D normalized feature vector\n",
        "\n",
        "### What This Section Does\n",
        "1. Define 1D CNN architecture matching pipeline specs\n",
        "2. Initialize with random weights (no pretraining available)\n",
        "3. Load preprocessed LiDAR scans from `processed_lidar/`\n",
        "4. Extract 256D geometric descriptors\n",
        "5. L2 normalize features\n",
        "6. Save to HDF5 with timestamps and metadata\n",
        "\n",
        "### Important Note\n",
        "⚠️ **This network uses random initialization** (no pretrained weights available). For production use:\n",
        "- Train on loop closure detection task\n",
        "- Or use triplet loss with place recognition labels\n",
        "- Current features serve as geometric descriptors but may not be optimal\n",
        "\n",
        "### Why 1D CNN?\n",
        "- Captures local geometric patterns in 360° scans\n",
        "- Translation-invariant along angular dimension\n",
        "- Lightweight: ~350K params (10× smaller than MobileNet V2)\n",
        "- Fast: ~20-30ms inference on embedded systems\n",
        "- Proven effective for LiDAR-based place recognition\n",
        "\n",
        "### Output Format\n",
        "Features saved to HDF5: `features.h5/lidar/features` [N, 256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOu8S0VJg-kn"
      },
      "outputs": [],
      "source": [
        "# Configuration for LiDAR feature extraction\n",
        "LIDAR_INPUT_DIR = \"processed_lidar\"\n",
        "LIDAR_BATCH_SIZE = 64  # LiDAR is lighter than images\n",
        "\n",
        "print(f\"LiDAR input directory: {LIDAR_INPUT_DIR}\")\n",
        "print(f\"Batch size: {LIDAR_BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgb680p6g-kn"
      },
      "outputs": [],
      "source": [
        "# Load LiDAR metadata\n",
        "lidar_metadata = pd.read_csv(os.path.join(LIDAR_INPUT_DIR, 'metadata.csv'))\n",
        "\n",
        "print(f\"Found {len(lidar_metadata)} LiDAR scans\")\n",
        "print(f\"\\nFirst few entries:\")\n",
        "lidar_metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvhSRByQg-kn"
      },
      "outputs": [],
      "source": [
        "# Define 1D CNN for Geometric Feature Extraction\n",
        "class GeometricCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    1D CNN for LiDAR geometric feature extraction.\n",
        "\n",
        "    Architecture:\n",
        "    - 4 Conv1D layers with increasing channels\n",
        "    - BatchNorm + ReLU after each conv\n",
        "    - Global Average Pooling\n",
        "    - Output: 256D feature vector\n",
        "\n",
        "    Parameters: ~350K (matching pipeline specification)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=360, output_dim=256):\n",
        "        super(GeometricCNN, self).__init__()\n",
        "\n",
        "        # Layer 1: 1 → 64 channels\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        # Layer 2: 64 → 128 channels\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Layer 3: 128 → 256 channels\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Layer 4: 256 → 256 channels\n",
        "        self.conv4 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 1, 360)\n",
        "\n",
        "        # Conv block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Conv block 2\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        # Conv block 3\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Conv block 4\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)  # (batch, 256, 1)\n",
        "        x = x.squeeze(-1)  # (batch, 256)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"GeometricCNN class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bueq-fiKg-kn"
      },
      "outputs": [],
      "source": [
        "# Initialize the 1D CNN\n",
        "print(\"Initializing Geometric CNN...\")\n",
        "\n",
        "geometric_cnn = GeometricCNN(input_dim=360, output_dim=256)\n",
        "geometric_cnn = geometric_cnn.to(device)\n",
        "geometric_cnn.eval()  # Evaluation mode\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in geometric_cnn.parameters())\n",
        "print(f\"✓ Geometric CNN initialized on {device}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Feature dimension: 256D\")\n",
        "print(f\"\\n⚠️  Using random initialization (no pretrained weights)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ron8xH6g-ko"
      },
      "outputs": [],
      "source": [
        "# Test the network with a sample scan\n",
        "print(\"Testing network with sample scan...\")\n",
        "\n",
        "# Load first scan\n",
        "sample_scan = pd.read_csv(\n",
        "    os.path.join(LIDAR_INPUT_DIR, lidar_metadata.iloc[0]['filename']),\n",
        "    header=None\n",
        ").values[0]\n",
        "\n",
        "print(f\"Sample scan shape: {sample_scan.shape}\")\n",
        "print(f\"Value range: [{sample_scan.min():.4f}, {sample_scan.max():.4f}]\")\n",
        "\n",
        "# Convert to tensor and add batch/channel dimensions\n",
        "sample_tensor = torch.tensor(sample_scan, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "print(f\"Tensor shape: {sample_tensor.shape}\")\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    sample_features = geometric_cnn(sample_tensor)\n",
        "    sample_features_norm = F.normalize(sample_features, p=2, dim=1)\n",
        "\n",
        "print(f\"\\nOutput features shape: {sample_features.shape}\")\n",
        "print(f\"Feature norm (before L2): {torch.norm(sample_features, dim=1).item():.4f}\")\n",
        "print(f\"Feature norm (after L2): {torch.norm(sample_features_norm, dim=1).item():.4f}\")\n",
        "print(\"✓ Network test successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsNsUUjFg-ko"
      },
      "outputs": [],
      "source": [
        "# Extract LiDAR features\n",
        "print(\"Extracting LiDAR features...\")\n",
        "\n",
        "lidar_features_list = []\n",
        "lidar_timestamps_list = []\n",
        "lidar_filenames_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in tqdm(range(0, len(lidar_metadata), LIDAR_BATCH_SIZE), desc=\"LiDAR batches\"):\n",
        "        batch_meta = lidar_metadata.iloc[idx:idx+LIDAR_BATCH_SIZE]\n",
        "\n",
        "        # Load batch of scans\n",
        "        batch_scans = []\n",
        "        batch_timestamps = []\n",
        "        batch_filenames = []\n",
        "\n",
        "        for _, row in batch_meta.iterrows():\n",
        "            scan_path = os.path.join(LIDAR_INPUT_DIR, row['filename'])\n",
        "\n",
        "            try:\n",
        "                # Load scan (single row CSV)\n",
        "                scan = pd.read_csv(scan_path, header=None).values[0]\n",
        "                batch_scans.append(torch.tensor(scan, dtype=torch.float32))\n",
        "                batch_timestamps.append(row['timestamp'])\n",
        "                batch_filenames.append(row['filename'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {scan_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(batch_scans) == 0:\n",
        "            continue\n",
        "\n",
        "        # Stack into batch tensor (batch, 1, 360)\n",
        "        batch_tensor = torch.stack(batch_scans).unsqueeze(1).to(device)\n",
        "\n",
        "        # Extract features\n",
        "        features = geometric_cnn(batch_tensor)\n",
        "\n",
        "        # L2 normalize features\n",
        "        features = F.normalize(features, p=2, dim=1)\n",
        "\n",
        "        # Move to CPU and store\n",
        "        lidar_features_list.append(features.cpu().numpy())\n",
        "        lidar_timestamps_list.extend(batch_timestamps)\n",
        "        lidar_filenames_list.extend(batch_filenames)\n",
        "\n",
        "# Concatenate all batches\n",
        "lidar_features = np.vstack(lidar_features_list)\n",
        "lidar_timestamps = np.array(lidar_timestamps_list)\n",
        "lidar_filenames = np.array(lidar_filenames_list)\n",
        "\n",
        "print(f\"\\n✓ Extracted LiDAR features: {lidar_features.shape}\")\n",
        "print(f\"Feature dimension: {lidar_features.shape[1]}D\")\n",
        "print(f\"Number of scans: {lidar_features.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKi0TIDPg-ko"
      },
      "outputs": [],
      "source": [
        "# Verify feature properties\n",
        "print(\"LiDAR Feature Statistics:\")\n",
        "print(f\"  Shape: {lidar_features.shape}\")\n",
        "print(f\"  Mean: {lidar_features.mean():.4f}\")\n",
        "print(f\"  Std: {lidar_features.std():.4f}\")\n",
        "print(f\"  Min: {lidar_features.min():.4f}\")\n",
        "print(f\"  Max: {lidar_features.max():.4f}\")\n",
        "\n",
        "# Check L2 normalization\n",
        "norms = np.linalg.norm(lidar_features, axis=1)\n",
        "print(f\"  L2 norms: mean={norms.mean():.4f}, std={norms.std():.6f}\")\n",
        "print(f\"  (should be ~1.0 after L2 normalization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EixCyqtng-ko"
      },
      "source": [
        "## Section 3: Save Features to HDF5\n",
        "\n",
        "### Purpose\n",
        "Store extracted features in HDF5 format for efficient access and sensor fusion. HDF5 provides fast random access, compression, and hierarchical organization.\n",
        "\n",
        "### Output Structure\n",
        "```\n",
        "features.h5\n",
        "├── camera/\n",
        "│   ├── features [N_cam, 1280]     # Camera feature vectors\n",
        "│   ├── timestamps [N_cam]         # ROS timestamps (float)\n",
        "│   └── filenames [N_cam]          # Source image filenames\n",
        "├── lidar/\n",
        "│   ├── features [N_lid, 256]      # LiDAR feature vectors\n",
        "│   ├── timestamps [N_lid]         # ROS timestamps (float)\n",
        "│   └── filenames [N_lid]          # Source scan filenames\n",
        "└── metadata (attributes)\n",
        "    ├── creation_date\n",
        "    ├── camera_model\n",
        "    ├── lidar_model\n",
        "    ├── camera_feature_dim\n",
        "    ├── lidar_feature_dim\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "### What This Section Does\n",
        "1. Create HDF5 file with hierarchical structure\n",
        "2. Save camera features (1280D × N_cam)\n",
        "3. Save LiDAR features (256D × N_lid)\n",
        "4. Store timestamps for temporal alignment\n",
        "5. Store filenames for traceability\n",
        "6. Add comprehensive metadata as attributes\n",
        "7. Generate summary JSON file\n",
        "\n",
        "### Usage Example (Next Stage)\n",
        "```python\n",
        "import h5py\n",
        "\n",
        "# Load features\n",
        "with h5py.File('features.h5', 'r') as f:\n",
        "    cam_features = f['camera/features'][:]  # (N, 1280)\n",
        "    cam_timestamps = f['camera/timestamps'][:]\n",
        "    \n",
        "    lid_features = f['lidar/features'][:]   # (N, 256)\n",
        "    lid_timestamps = f['lidar/timestamps'][:]\n",
        "    \n",
        "    # Access metadata\n",
        "    camera_dim = f['camera'].attrs['feature_dim']\n",
        "```\n",
        "\n",
        "### Important Notes\n",
        "- **No temporal alignment yet**: Camera and LiDAR features saved separately\n",
        "- **Timestamps preserved**: Use for alignment in fusion stage\n",
        "- **L2 normalized**: Features ready for cosine similarity\n",
        "- **Traceability**: Filenames link back to original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75AvHpsrg-ko"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "OUTPUT_FILE = \"features.h5\"\n",
        "COMPRESSION = \"gzip\"  # Use gzip compression\n",
        "\n",
        "print(f\"Output file: {OUTPUT_FILE}\")\n",
        "print(f\"Compression: {COMPRESSION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08oc-ZdNg-ko"
      },
      "outputs": [],
      "source": [
        "# Create HDF5 file and save features\n",
        "print(\"Creating HDF5 file...\")\n",
        "\n",
        "with h5py.File(OUTPUT_FILE, 'w') as f:\n",
        "    # Create camera group\n",
        "    camera_group = f.create_group('camera')\n",
        "    camera_group.create_dataset('features', data=camera_features, compression=COMPRESSION)\n",
        "    camera_group.create_dataset('timestamps', data=camera_timestamps, compression=COMPRESSION)\n",
        "    camera_group.create_dataset('filenames', data=camera_filenames.astype('S'))\n",
        "\n",
        "    # Add camera metadata\n",
        "    camera_group.attrs['feature_dim'] = camera_features.shape[1]\n",
        "    camera_group.attrs['num_samples'] = camera_features.shape[0]\n",
        "    camera_group.attrs['model'] = 'MobileNet V2'\n",
        "    camera_group.attrs['pretrained'] = 'ImageNet'\n",
        "    camera_group.attrs['normalization'] = 'L2'\n",
        "    camera_group.attrs['input_size'] = '224x224x3'\n",
        "\n",
        "    # Create lidar group\n",
        "    lidar_group = f.create_group('lidar')\n",
        "    lidar_group.create_dataset('features', data=lidar_features, compression=COMPRESSION)\n",
        "    lidar_group.create_dataset('timestamps', data=lidar_timestamps, compression=COMPRESSION)\n",
        "    lidar_group.create_dataset('filenames', data=lidar_filenames.astype('S'))\n",
        "\n",
        "    # Add lidar metadata\n",
        "    lidar_group.attrs['feature_dim'] = lidar_features.shape[1]\n",
        "    lidar_group.attrs['num_samples'] = lidar_features.shape[0]\n",
        "    lidar_group.attrs['model'] = '1D CNN (4 Conv1D + GAP)'\n",
        "    lidar_group.attrs['pretrained'] = 'None (random init)'\n",
        "    lidar_group.attrs['normalization'] = 'L2'\n",
        "    lidar_group.attrs['input_size'] = '360'\n",
        "    lidar_group.attrs['parameters'] = f'{total_params:,}'\n",
        "\n",
        "    # Add global metadata\n",
        "    f.attrs['creation_date'] = datetime.now().isoformat()\n",
        "    f.attrs['camera_input_dir'] = CAMERA_INPUT_DIR\n",
        "    f.attrs['lidar_input_dir'] = LIDAR_INPUT_DIR\n",
        "    f.attrs['device'] = str(device)\n",
        "    f.attrs['camera_batch_size'] = CAMERA_BATCH_SIZE\n",
        "    f.attrs['lidar_batch_size'] = LIDAR_BATCH_SIZE\n",
        "    f.attrs['temporal_alignment'] = 'Not performed - features extracted independently'\n",
        "\n",
        "print(f\"✓ Features saved to {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3U_DHibg-ko"
      },
      "outputs": [],
      "source": [
        "# Verify HDF5 file\n",
        "print(\"\\nVerifying HDF5 file...\")\n",
        "\n",
        "with h5py.File(OUTPUT_FILE, 'r') as f:\n",
        "    print(f\"\\nGroups: {list(f.keys())}\")\n",
        "\n",
        "    print(f\"\\nCamera:\")\n",
        "    print(f\"  features: {f['camera/features'].shape}\")\n",
        "    print(f\"  timestamps: {f['camera/timestamps'].shape}\")\n",
        "    print(f\"  filenames: {f['camera/filenames'].shape}\")\n",
        "    print(f\"  feature_dim: {f['camera'].attrs['feature_dim']}\")\n",
        "\n",
        "    print(f\"\\nLiDAR:\")\n",
        "    print(f\"  features: {f['lidar/features'].shape}\")\n",
        "    print(f\"  timestamps: {f['lidar/timestamps'].shape}\")\n",
        "    print(f\"  filenames: {f['lidar/filenames'].shape}\")\n",
        "    print(f\"  feature_dim: {f['lidar'].attrs['feature_dim']}\")\n",
        "\n",
        "    print(f\"\\nGlobal attributes:\")\n",
        "    for key, value in f.attrs.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Get file size\n",
        "file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
        "print(f\"\\nFile size: {file_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6xOVau8g-ko"
      },
      "outputs": [],
      "source": [
        "# Create summary JSON\n",
        "summary = {\n",
        "    \"feature_extraction_summary\": {\n",
        "        \"creation_date\": datetime.now().isoformat(),\n",
        "        \"output_file\": OUTPUT_FILE,\n",
        "        \"file_size_mb\": round(file_size_mb, 2)\n",
        "    },\n",
        "    \"camera\": {\n",
        "        \"model\": \"MobileNet V2\",\n",
        "        \"pretrained\": \"ImageNet\",\n",
        "        \"feature_dim\": int(camera_features.shape[1]),\n",
        "        \"num_samples\": int(camera_features.shape[0]),\n",
        "        \"input_size\": \"224x224x3\",\n",
        "        \"preprocessing\": \"Scale to [-1,1]\",\n",
        "        \"normalization\": \"L2\",\n",
        "        \"batch_size\": CAMERA_BATCH_SIZE\n",
        "    },\n",
        "    \"lidar\": {\n",
        "        \"model\": \"1D CNN (4 Conv1D + GAP)\",\n",
        "        \"pretrained\": \"None (random initialization)\",\n",
        "        \"feature_dim\": int(lidar_features.shape[1]),\n",
        "        \"num_samples\": int(lidar_features.shape[0]),\n",
        "        \"parameters\": total_params,\n",
        "        \"input_size\": \"360\",\n",
        "        \"preprocessing\": \"Normalized to [0,1]\",\n",
        "        \"normalization\": \"L2\",\n",
        "        \"batch_size\": LIDAR_BATCH_SIZE\n",
        "    },\n",
        "    \"notes\": {\n",
        "        \"temporal_alignment\": \"Not performed - features extracted independently\",\n",
        "        \"next_steps\": [\n",
        "            \"Temporal alignment using timestamps\",\n",
        "            \"Sensor fusion (concatenate or attention-based)\",\n",
        "            \"Loop closure detection training\",\n",
        "            \"Fine-tune networks on place recognition task\"\n",
        "        ],\n",
        "        \"warnings\": [\n",
        "            \"LiDAR CNN uses random initialization - train before production use\",\n",
        "            \"Features from different modalities have different timestamps\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "summary_file = \"feature_extraction_summary.json\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"✓ Summary saved to {summary_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKcFNkRfg-ko"
      },
      "outputs": [],
      "source": [
        "# Display final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE EXTRACTION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nOutput file: {OUTPUT_FILE}\")\n",
        "print(f\"File size: {file_size_mb:.2f} MB\")\n",
        "print(f\"\\nCamera Features:\")\n",
        "print(f\"  Model: MobileNet V2 (pretrained on ImageNet)\")\n",
        "print(f\"  Dimension: {camera_features.shape[1]}D\")\n",
        "print(f\"  Samples: {camera_features.shape[0]:,}\")\n",
        "print(f\"  Normalization: L2\")\n",
        "print(f\"\\nLiDAR Features:\")\n",
        "print(f\"  Model: 1D CNN ({total_params:,} parameters)\")\n",
        "print(f\"  Dimension: {lidar_features.shape[1]}D\")\n",
        "print(f\"  Samples: {lidar_features.shape[0]:,}\")\n",
        "print(f\"  Normalization: L2\")\n",
        "print(f\"  ⚠️  Random initialization (train before production use)\")\n",
        "print(f\"\\nNext Steps:\")\n",
        "print(f\"  1. Temporal alignment using timestamps\")\n",
        "print(f\"  2. Sensor fusion pipeline\")\n",
        "print(f\"  3. Loop closure detection\")\n",
        "print(f\"  4. Network fine-tuning\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdTEYpe6g-ko"
      },
      "source": [
        "## Visualization & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3q_1zeJg-ko"
      },
      "outputs": [],
      "source": [
        "# Visualize feature distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Camera feature distribution\n",
        "axes[0, 0].hist(camera_features.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Feature Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Camera Feature Distribution (1280D × N)')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# LiDAR feature distribution\n",
        "axes[0, 1].hist(lidar_features.flatten(), bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
        "axes[0, 1].set_xlabel('Feature Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('LiDAR Feature Distribution (256D × N)')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Camera L2 norms\n",
        "cam_norms = np.linalg.norm(camera_features, axis=1)\n",
        "axes[1, 0].hist(cam_norms, bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].axvline(1.0, color='r', linestyle='--', label='Expected (1.0)')\n",
        "axes[1, 0].set_xlabel('L2 Norm')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_title('Camera Feature L2 Norms')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# LiDAR L2 norms\n",
        "lid_norms = np.linalg.norm(lidar_features, axis=1)\n",
        "axes[1, 1].hist(lid_norms, bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
        "axes[1, 1].axvline(1.0, color='r', linestyle='--', label='Expected (1.0)')\n",
        "axes[1, 1].set_xlabel('L2 Norm')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].set_title('LiDAR Feature L2 Norms')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization saved as 'feature_distributions.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnHZ3OOgg-kp"
      },
      "outputs": [],
      "source": [
        "# Analyze temporal coverage\n",
        "print(\"\\nTemporal Coverage Analysis:\")\n",
        "print(f\"\\nCamera:\")\n",
        "print(f\"  Time range: {camera_timestamps.min():.2f} - {camera_timestamps.max():.2f} sec\")\n",
        "print(f\"  Duration: {camera_timestamps.max() - camera_timestamps.min():.2f} sec\")\n",
        "print(f\"  Average interval: {np.mean(np.diff(camera_timestamps)):.4f} sec\")\n",
        "\n",
        "print(f\"\\nLiDAR:\")\n",
        "print(f\"  Time range: {lidar_timestamps.min():.2f} - {lidar_timestamps.max():.2f} sec\")\n",
        "print(f\"  Duration: {lidar_timestamps.max() - lidar_timestamps.min():.2f} sec\")\n",
        "print(f\"  Average interval: {np.mean(np.diff(lidar_timestamps)):.4f} sec\")\n",
        "\n",
        "# Plot timeline\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 4))\n",
        "ax.plot(camera_timestamps, np.ones_like(camera_timestamps), '|', markersize=10, label='Camera')\n",
        "ax.plot(lidar_timestamps, np.ones_like(lidar_timestamps) * 1.1, '|', markersize=10, label='LiDAR', color='orange')\n",
        "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
        "ax.set_yticks([])\n",
        "ax.set_title('Temporal Distribution of Features', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('temporal_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Timeline saved as 'temporal_distribution.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbKxOk6fg-kp"
      },
      "source": [
        "### Output Files\n",
        "- `features.h5` - Main feature database (HDF5)\n",
        "- `feature_extraction_summary.json` - Processing metadata\n",
        "- `feature_distributions.png` - Feature statistics visualization\n",
        "- `temporal_distribution.png` - Timeline visualization\n",
        "\n",
        "### Next Steps\n",
        "1. **Temporal Alignment**: Match camera and LiDAR features by timestamp\n",
        "2. **Sensor Fusion**: Combine modalities (concatenation, attention, etc.)\n",
        "3. **Loop Closure Detection**: Train classifier on fused features\n",
        "4. **Network Fine-tuning**: Train 1D CNN and fine-tune MobileNet V2\n",
        "\n",
        "### Important Reminders\n",
        "⚠️ LiDAR CNN uses random initialization - train on loop closure task before production  \n",
        "⚠️ Features extracted independently - temporal alignment needed for fusion  \n",
        "⚠️ All features are L2 normalized - use cosine similarity for comparison  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}